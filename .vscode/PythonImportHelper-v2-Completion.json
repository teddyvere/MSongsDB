[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "unicodedata",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "unicodedata",
        "description": "unicodedata",
        "detail": "unicodedata",
        "documentation": {}
    },
    {
        "label": "itertools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itertools",
        "description": "itertools",
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "Levenshtein",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "Levenshtein",
        "description": "Levenshtein",
        "detail": "Levenshtein",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "Queue",
        "importPath": "Queue",
        "description": "Queue",
        "isExtraImport": true,
        "detail": "Queue",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "urllib2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib2",
        "description": "urllib2",
        "detail": "urllib2",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "numpy.random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy.random",
        "description": "numpy.random",
        "detail": "numpy.random",
        "documentation": {}
    },
    {
        "label": "pyechonest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyechonest",
        "description": "pyechonest",
        "detail": "pyechonest",
        "documentation": {}
    },
    {
        "label": "artist",
        "importPath": "pyechonest",
        "description": "pyechonest",
        "isExtraImport": true,
        "detail": "pyechonest",
        "documentation": {}
    },
    {
        "label": "song",
        "importPath": "pyechonest",
        "description": "pyechonest",
        "isExtraImport": true,
        "detail": "pyechonest",
        "documentation": {}
    },
    {
        "label": "track",
        "importPath": "pyechonest",
        "description": "pyechonest",
        "isExtraImport": true,
        "detail": "pyechonest",
        "documentation": {}
    },
    {
        "label": "artist",
        "importPath": "pyechonest",
        "description": "pyechonest",
        "isExtraImport": true,
        "detail": "pyechonest",
        "documentation": {}
    },
    {
        "label": "song",
        "importPath": "pyechonest",
        "description": "pyechonest",
        "isExtraImport": true,
        "detail": "pyechonest",
        "documentation": {}
    },
    {
        "label": "track",
        "importPath": "pyechonest",
        "description": "pyechonest",
        "isExtraImport": true,
        "detail": "pyechonest",
        "documentation": {}
    },
    {
        "label": "config",
        "importPath": "pyechonest",
        "description": "pyechonest",
        "isExtraImport": true,
        "detail": "pyechonest",
        "documentation": {}
    },
    {
        "label": "pyechonest.config",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyechonest.config",
        "description": "pyechonest.config",
        "detail": "pyechonest.config",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pg",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pg",
        "description": "pg",
        "detail": "pg",
        "documentation": {}
    },
    {
        "label": "hdf5_utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hdf5_utils",
        "description": "hdf5_utils",
        "detail": "hdf5_utils",
        "documentation": {}
    },
    {
        "label": "utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "utils",
        "description": "utils",
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "hdf5_getters",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hdf5_getters",
        "description": "hdf5_getters",
        "detail": "hdf5_getters",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "hdf5_getters",
        "description": "hdf5_getters",
        "isExtraImport": true,
        "detail": "hdf5_getters",
        "documentation": {}
    },
    {
        "label": "tables",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tables",
        "description": "tables",
        "detail": "tables",
        "documentation": {}
    },
    {
        "label": "hdf5_descriptors",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hdf5_descriptors",
        "description": "hdf5_descriptors",
        "detail": "hdf5_descriptors",
        "documentation": {}
    },
    {
        "label": "sqlite3",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sqlite3",
        "description": "sqlite3",
        "detail": "sqlite3",
        "documentation": {}
    },
    {
        "label": "itemgetter",
        "importPath": "operator",
        "description": "operator",
        "isExtraImport": true,
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "itemgetter",
        "importPath": "operator",
        "description": "operator",
        "isExtraImport": true,
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "itemgetter",
        "importPath": "operator",
        "description": "operator",
        "isExtraImport": true,
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "itemgetter",
        "importPath": "operator",
        "description": "operator",
        "isExtraImport": true,
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "itemgetter",
        "importPath": "operator",
        "description": "operator",
        "isExtraImport": true,
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "process_train_set",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "process_train_set",
        "description": "process_train_set",
        "detail": "process_train_set",
        "documentation": {}
    },
    {
        "label": "fingerprint_hash",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fingerprint_hash",
        "description": "fingerprint_hash",
        "detail": "fingerprint_hash",
        "documentation": {}
    },
    {
        "label": "cover_hash_table",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cover_hash_table",
        "description": "cover_hash_table",
        "detail": "cover_hash_table",
        "documentation": {}
    },
    {
        "label": "minidom",
        "importPath": "xml.dom",
        "description": "xml.dom",
        "isExtraImport": true,
        "detail": "xml.dom",
        "documentation": {}
    },
    {
        "label": "urllib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib",
        "description": "urllib",
        "detail": "urllib",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "get_preview_url",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "get_preview_url",
        "description": "get_preview_url",
        "detail": "get_preview_url",
        "documentation": {}
    },
    {
        "label": "string",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "string",
        "description": "string",
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "measure_vw_res",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "measure_vw_res",
        "description": "measure_vw_res",
        "detail": "measure_vw_res",
        "documentation": {}
    },
    {
        "label": "randproj",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "randproj",
        "description": "randproj",
        "detail": "randproj",
        "documentation": {}
    },
    {
        "label": "year_pred_benchmark",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "year_pred_benchmark",
        "description": "year_pred_benchmark",
        "detail": "year_pred_benchmark",
        "documentation": {}
    },
    {
        "label": "compress_feat",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "compress_feat",
        "description": "compress_feat",
        "detail": "compress_feat",
        "documentation": {}
    },
    {
        "label": "get_bttimbre",
        "importPath": "beat_aligned_feats",
        "description": "beat_aligned_feats",
        "isExtraImport": true,
        "detail": "beat_aligned_feats",
        "documentation": {}
    },
    {
        "label": "get_bttimbre",
        "importPath": "beat_aligned_feats",
        "description": "beat_aligned_feats",
        "isExtraImport": true,
        "detail": "beat_aligned_feats",
        "documentation": {}
    },
    {
        "label": "char_is_ascii",
        "kind": 2,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "def char_is_ascii(c):\n    \"\"\"\n    Check if a unicode character, e.g. u'A', u'1' or u'\\u0301' is ASCII\n    \"\"\"\n    #return ord(c) < 128\n    # the following should be faster, according to:\n    #http://stackoverflow.com/questions/196345/how-to-check-if-a-string-in-python-is-in-ascii\n    return c < u\"\\x7F\"\ndef remove_non_ascii(s):\n    \"\"\"",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "remove_non_ascii",
        "kind": 2,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "def remove_non_ascii(s):\n    \"\"\"\n    Normalize characters in unicode string 's' that are not ASCII,\n    try to transform accented characters to non accented version.\n    Otherwise, remove non-ascii chars\n    \"\"\"\n    decomposition = unicodedata.normalize('NFKD', s)\n    return filter(lambda x: char_is_ascii(x), decomposition)\ndef to_lower_case(s):\n    \"\"\"",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "to_lower_case",
        "kind": 2,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "def to_lower_case(s):\n    \"\"\"\n    transform a unicode string 's' to lowercase\n    ok, this one is trivial, I know\n    \"\"\"\n    return s.lower()\ndef remove_spaces(s):\n    \"\"\"\n    Remove all possible spaces in the unicode string s\n    \"\"\"",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "remove_spaces",
        "kind": 2,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "def remove_spaces(s):\n    \"\"\"\n    Remove all possible spaces in the unicode string s\n    \"\"\"\n    return re_space.sub('', s)\ndef replace_rotation_symbols(s):\n    \"\"\"\n    Mostly, replace '&' by 'and'\n    \"\"\"\n    return re_rotsymbols.sub(' and ', s)",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "replace_rotation_symbols",
        "kind": 2,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "def replace_rotation_symbols(s):\n    \"\"\"\n    Mostly, replace '&' by 'and'\n    \"\"\"\n    return re_rotsymbols.sub(' and ', s)\ndef remove_stub(s):\n    \"\"\"\n    Remove a questionable beginning, e.g. dj\n    otherwise return string at is\n    \"\"\"",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "remove_stub",
        "kind": 2,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "def remove_stub(s):\n    \"\"\"\n    Remove a questionable beginning, e.g. dj\n    otherwise return string at is\n    \"\"\"\n    m = re_remstub.match(s)\n    if not m:\n        return s\n    return m.groups()[1]\ndef remove_endings(s):",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "remove_endings",
        "kind": 2,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "def remove_endings(s):\n    \"\"\"\n    Remove questionable endings, e.g. 'band'\n    \"\"\"\n    m = re_remending1.match(s)\n    if m:\n       s = m.groups()[0]\n    m = re_remending2.match(s)\n    if m:\n        s = m.groups()[0]",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "remove_quotes",
        "kind": 2,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "def remove_quotes(s):\n    \"\"\"\n    Remove the quote, like Thierry \"The Awesomest\" BM\n    \"\"\"\n    m = re_remquotes.match(s)\n    if not m:\n        return s\n    parts = m.groups()\n    assert len(parts) == 3\n    return parts[0] + ' ' + parts[2]",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "remove_parenthesis",
        "kind": 2,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "def remove_parenthesis(s):\n    \"\"\"\n    Remove parenthesis, like Thierry (Coolest guy)\n    \"\"\"\n    m = re_remparenthesis.match(s)\n    if not m:\n        return s\n    parts = m.groups()\n    assert len(parts) >= 2\n    if len(parts) == 2:",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "remove_brackets",
        "kind": 2,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "def remove_brackets(s):\n    \"\"\"\n    Remove brackets, like Thierry [Coolest guy]\n    \"\"\"\n    m = re_rembrackets.match(s)\n    if not m:\n        return s\n    parts = m.groups()\n    assert len(parts) >= 2\n    if len(parts) == 2:",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "normalize_no_rotation",
        "kind": 2,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "def normalize_no_rotation(s):\n    \"\"\"\n    We normalize a name that is supposed to contain no\n    rotation term ('and', 'y', ...)\n    \"\"\"\n    # remove beginning\n    s = remove_stub(s)\n    # remove ends\n    s = remove_endings(s)    \n    # remove ()",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "split_rotation_words",
        "kind": 2,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "def split_rotation_words(s):\n    \"\"\"\n    Split a name using the rotation words: 'and', 'vs', 'y', 'et', ...\n    then create all possible permutations\n    \"\"\"\n    parts = re_rotwords.split(s)\n    parts = filter(lambda p: not p in rotation_words, parts)[:5]\n    results = set()\n    # keep only the individual elems (risky?)\n    for p in parts:",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "remove_nonalphanumeric",
        "kind": 2,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "def remove_nonalphanumeric(s):\n    \"\"\"\n    Remove usual punctuation signs:  ! , ? : ; . '   etc\n    Also, we transform long spaces into normal ones\n    \"\"\"\n    # split around non-alphanum chars\n    parts = re_nonalphanum.split(s)\n    # remove empty spots\n    parts = filter(lambda p: p, parts)\n    # rejoin with regular space ' '",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "normalize_artist",
        "kind": 2,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "def normalize_artist(s):\n    \"\"\"\n    Return a set of normalized versions of that artist name\n    \"\"\"\n    # normalized versions\n    results = set()\n    # lower case\n    s = to_lower_case(s)\n    results.add(s)\n    # remove non-ascii chars (try to replace them)",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "normalize_title",
        "kind": 2,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "def normalize_title(s):\n    \"\"\"\n    Return a set of normalized versions of that title\n    \"\"\"\n    # normalized versions\n    results = set()\n    # lower case\n    s = to_lower_case(s)\n    results.add(s)\n    # remove non-ascii chars (try to replace them)",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "same_artist",
        "kind": 2,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "def same_artist(name1, name2):\n    \"\"\"\n    Compare two artists:\n    - edit distance\n    - if one name is contained in the other\n    - by normalizing the names\n    Return True if it's the same artist, False otherwise\n    \"\"\"\n    # trivial\n    n1 = to_lower_case(name1)",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "same_title",
        "kind": 2,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "def same_title(title1, title2):\n    \"\"\"\n    Compare two titles:\n    - edit distance\n    - if one name is contained in the other\n    - by normalizing the title\n    Return True if it's the same title, False otherwise\n    \"\"\"\n    # trivial\n    t1 = to_lower_case(title1)",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "rotation_symbols",
        "kind": 5,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "rotation_symbols = ['\\|', '/', '&', ',', '\\+', ';', '_']#, '\\-']\nrotation_words = ['and', 'y', 'et', 'vs', 'vs.', 'v', 'with', 'feat',\n                  'feat.', 'featuring', 'presents', 'ft.', 'pres.']\n# SYMBOLS TO REMOVE AT THE BEGINNING\nstub_to_remove = ['dj', 'dj.', 'mc', 'm.c.', 'mc.', 'the', 'los', 'les']\n# SYMBOLS TO REMOVE AT THE END\nend_to_remove1 = ['big band', 'trio', 'quartet', 'ensemble', 'orchestra']\nend_to_remove2 = ['band']\n# COMPILED REGULAR EXPRESSION\n# white spaces",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "rotation_words",
        "kind": 5,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "rotation_words = ['and', 'y', 'et', 'vs', 'vs.', 'v', 'with', 'feat',\n                  'feat.', 'featuring', 'presents', 'ft.', 'pres.']\n# SYMBOLS TO REMOVE AT THE BEGINNING\nstub_to_remove = ['dj', 'dj.', 'mc', 'm.c.', 'mc.', 'the', 'los', 'les']\n# SYMBOLS TO REMOVE AT THE END\nend_to_remove1 = ['big band', 'trio', 'quartet', 'ensemble', 'orchestra']\nend_to_remove2 = ['band']\n# COMPILED REGULAR EXPRESSION\n# white spaces\nre_space = re.compile(r'\\s')",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "stub_to_remove",
        "kind": 5,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "stub_to_remove = ['dj', 'dj.', 'mc', 'm.c.', 'mc.', 'the', 'los', 'les']\n# SYMBOLS TO REMOVE AT THE END\nend_to_remove1 = ['big band', 'trio', 'quartet', 'ensemble', 'orchestra']\nend_to_remove2 = ['band']\n# COMPILED REGULAR EXPRESSION\n# white spaces\nre_space = re.compile(r'\\s')\n# non alphanumeric\nre_nonalphanum = re.compile(r'\\W')\n# rotation symbols",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "end_to_remove1",
        "kind": 5,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "end_to_remove1 = ['big band', 'trio', 'quartet', 'ensemble', 'orchestra']\nend_to_remove2 = ['band']\n# COMPILED REGULAR EXPRESSION\n# white spaces\nre_space = re.compile(r'\\s')\n# non alphanumeric\nre_nonalphanum = re.compile(r'\\W')\n# rotation symbols\nre_rotsymbols = re.compile('\\s*?' + '|'.join(rotation_symbols) + '\\s*?')\n# rotation words",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "end_to_remove2",
        "kind": 5,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "end_to_remove2 = ['band']\n# COMPILED REGULAR EXPRESSION\n# white spaces\nre_space = re.compile(r'\\s')\n# non alphanumeric\nre_nonalphanum = re.compile(r'\\W')\n# rotation symbols\nre_rotsymbols = re.compile('\\s*?' + '|'.join(rotation_symbols) + '\\s*?')\n# rotation words\nre_rotwords = re.compile(r'\\s(' + '|'.join(rotation_words) + ')\\s')",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "re_space",
        "kind": 5,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "re_space = re.compile(r'\\s')\n# non alphanumeric\nre_nonalphanum = re.compile(r'\\W')\n# rotation symbols\nre_rotsymbols = re.compile('\\s*?' + '|'.join(rotation_symbols) + '\\s*?')\n# rotation words\nre_rotwords = re.compile(r'\\s(' + '|'.join(rotation_words) + ')\\s')\n# stub to remove\nre_remstub = re.compile('(' + '|'.join(stub_to_remove) + ')\\s(.*)')\n# ending to remove",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "re_nonalphanum",
        "kind": 5,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "re_nonalphanum = re.compile(r'\\W')\n# rotation symbols\nre_rotsymbols = re.compile('\\s*?' + '|'.join(rotation_symbols) + '\\s*?')\n# rotation words\nre_rotwords = re.compile(r'\\s(' + '|'.join(rotation_words) + ')\\s')\n# stub to remove\nre_remstub = re.compile('(' + '|'.join(stub_to_remove) + ')\\s(.*)')\n# ending to remove\nre_remending1 = re.compile('(.*)\\s(' + '|'.join(end_to_remove1) + ')')\nre_remending2 = re.compile('(.*)\\s(' + '|'.join(end_to_remove2) + ')')",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "re_rotsymbols",
        "kind": 5,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "re_rotsymbols = re.compile('\\s*?' + '|'.join(rotation_symbols) + '\\s*?')\n# rotation words\nre_rotwords = re.compile(r'\\s(' + '|'.join(rotation_words) + ')\\s')\n# stub to remove\nre_remstub = re.compile('(' + '|'.join(stub_to_remove) + ')\\s(.*)')\n# ending to remove\nre_remending1 = re.compile('(.*)\\s(' + '|'.join(end_to_remove1) + ')')\nre_remending2 = re.compile('(.*)\\s(' + '|'.join(end_to_remove2) + ')')\n# quotes to remove\nre_remquotes = re.compile('(.+)\\s(\".+?\")\\s(.+)')",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "re_rotwords",
        "kind": 5,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "re_rotwords = re.compile(r'\\s(' + '|'.join(rotation_words) + ')\\s')\n# stub to remove\nre_remstub = re.compile('(' + '|'.join(stub_to_remove) + ')\\s(.*)')\n# ending to remove\nre_remending1 = re.compile('(.*)\\s(' + '|'.join(end_to_remove1) + ')')\nre_remending2 = re.compile('(.*)\\s(' + '|'.join(end_to_remove2) + ')')\n# quotes to remove\nre_remquotes = re.compile('(.+)\\s(\".+?\")\\s(.+)')\n# parenthesis to remove\nre_remparenthesis = re.compile('(.+)\\s(\\(.+?\\))\\s*(.*)')",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "re_remstub",
        "kind": 5,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "re_remstub = re.compile('(' + '|'.join(stub_to_remove) + ')\\s(.*)')\n# ending to remove\nre_remending1 = re.compile('(.*)\\s(' + '|'.join(end_to_remove1) + ')')\nre_remending2 = re.compile('(.*)\\s(' + '|'.join(end_to_remove2) + ')')\n# quotes to remove\nre_remquotes = re.compile('(.+)\\s(\".+?\")\\s(.+)')\n# parenthesis to remove\nre_remparenthesis = re.compile('(.+)\\s(\\(.+?\\))\\s*(.*)')\n# brackets to remove\nre_rembrackets = re.compile('(.+)\\s(\\[.+?\\])\\s*(.*)')",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "re_remending1",
        "kind": 5,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "re_remending1 = re.compile('(.*)\\s(' + '|'.join(end_to_remove1) + ')')\nre_remending2 = re.compile('(.*)\\s(' + '|'.join(end_to_remove2) + ')')\n# quotes to remove\nre_remquotes = re.compile('(.+)\\s(\".+?\")\\s(.+)')\n# parenthesis to remove\nre_remparenthesis = re.compile('(.+)\\s(\\(.+?\\))\\s*(.*)')\n# brackets to remove\nre_rembrackets = re.compile('(.+)\\s(\\[.+?\\])\\s*(.*)')\ndef char_is_ascii(c):\n    \"\"\"",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "re_remending2",
        "kind": 5,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "re_remending2 = re.compile('(.*)\\s(' + '|'.join(end_to_remove2) + ')')\n# quotes to remove\nre_remquotes = re.compile('(.+)\\s(\".+?\")\\s(.+)')\n# parenthesis to remove\nre_remparenthesis = re.compile('(.+)\\s(\\(.+?\\))\\s*(.*)')\n# brackets to remove\nre_rembrackets = re.compile('(.+)\\s(\\[.+?\\])\\s*(.*)')\ndef char_is_ascii(c):\n    \"\"\"\n    Check if a unicode character, e.g. u'A', u'1' or u'\\u0301' is ASCII",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "re_remquotes",
        "kind": 5,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "re_remquotes = re.compile('(.+)\\s(\".+?\")\\s(.+)')\n# parenthesis to remove\nre_remparenthesis = re.compile('(.+)\\s(\\(.+?\\))\\s*(.*)')\n# brackets to remove\nre_rembrackets = re.compile('(.+)\\s(\\[.+?\\])\\s*(.*)')\ndef char_is_ascii(c):\n    \"\"\"\n    Check if a unicode character, e.g. u'A', u'1' or u'\\u0301' is ASCII\n    \"\"\"\n    #return ord(c) < 128",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "re_remparenthesis",
        "kind": 5,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "re_remparenthesis = re.compile('(.+)\\s(\\(.+?\\))\\s*(.*)')\n# brackets to remove\nre_rembrackets = re.compile('(.+)\\s(\\[.+?\\])\\s*(.*)')\ndef char_is_ascii(c):\n    \"\"\"\n    Check if a unicode character, e.g. u'A', u'1' or u'\\u0301' is ASCII\n    \"\"\"\n    #return ord(c) < 128\n    # the following should be faster, according to:\n    #http://stackoverflow.com/questions/196345/how-to-check-if-a-string-in-python-is-in-ascii",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "re_rembrackets",
        "kind": 5,
        "importPath": "NameNormalizer.normalizer",
        "description": "NameNormalizer.normalizer",
        "peekOfCode": "re_rembrackets = re.compile('(.+)\\s(\\[.+?\\])\\s*(.*)')\ndef char_is_ascii(c):\n    \"\"\"\n    Check if a unicode character, e.g. u'A', u'1' or u'\\u0301' is ASCII\n    \"\"\"\n    #return ord(c) < 128\n    # the following should be faster, according to:\n    #http://stackoverflow.com/questions/196345/how-to-check-if-a-string-in-python-is-in-ascii\n    return c < u\"\\x7F\"\ndef remove_non_ascii(s):",
        "detail": "NameNormalizer.normalizer",
        "documentation": {}
    },
    {
        "label": "my_trackset",
        "kind": 6,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "class my_trackset():\n    \"\"\"\n    class works with multiprocessing\n    should look like a set from outside\n    \"\"\"\n    def __init__(self):\n        array_length = 10\n        self.ar = multiprocessing.Array('l',array_length) # l for long, i not enough\n        for k in range(len(self.ar)):\n            self.ar[k] = 0",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "KeyboardInterruptError",
        "kind": 6,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "class KeyboardInterruptError(Exception):pass\n# for multiprocessing\ndef run_steps_wrapper(args):\n    \"\"\" wrapper function for multiprocessor, calls run_steps \"\"\"\n    run_steps(**args)\ndef run_steps(maindir,nomb=False,nfilesbuffer=0,startstep=0,onlystep=-1,idxthread=0):\n    \"\"\"\n    Main function to run the different steps of the dataset creation.\n    Each thread should be initialized by calling this function.\n    INPUT",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "close_creation",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def close_creation():\n    \"\"\"\n    Function to use to help stop all processes in a clean way.\n    It is usefull because of multithreading: only one thread\n    will get the keyboard interrupt, this function tries to\n    propagate the info\n    \"\"\"\n    close_trackset()\n    global CREATION_CLOSED\n    CREATION_CLOSED = True",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "close_trackset",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def close_trackset():\n    \"\"\"\n    When terminating the thread, nothing can add anything\n    to TRACKSET anymore\n    \"\"\"\n    global TRACKSET_CLOSED\n    TRACKSET_CLOSED = True\ndef get_lock_track(trackid):\n    \"\"\"\n    Get the lock for the creation of one particular file",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "get_lock_track",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def get_lock_track(trackid):\n    \"\"\"\n    Get the lock for the creation of one particular file\n    Returns True if you got, False otherwise (meaning\n    someone else just got it\n    This is a blocking call.\n    \"\"\"\n    got_lock = TRACKSET_LOCK.acquire() # blocking by default\n    if not got_lock:\n        print 'ERROR: could not get TRACKSET_LOCK locked?'",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "release_lock_track",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def release_lock_track(trackid):\n    \"\"\"\n    Release the lock for the creation of one particular file.\n    Should always return True, unless there is a problem\n    Releasing a song that you don't have the lock on is dangerous.\n    \"\"\"\n    got_lock = TRACKSET_LOCK.acquire() # blocking by default\n    if not got_lock:\n        print 'ERROR: could not get TRACKSET_LOCK lock?'\n        return False",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "path_from_trackid",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def path_from_trackid(trackid):\n    \"\"\"\n    Returns the typical path, with the letters[2-3-4]\n    of the trackid (starting at 0), hence a song with\n    trackid: TRABC1839DQL4H... will have path:\n    A/B/C/TRABC1839DQL4H....h5\n    \"\"\"\n    p = os.path.join(trackid[2],trackid[3])\n    p = os.path.join(p,trackid[4])\n    p = os.path.join(p,trackid+'.h5')",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "count_h5_files",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def count_h5_files(basedir):\n    \"\"\"\n    Return the number of hdf5 files contained in all\n    subdirectories of base\n    \"\"\"\n    cnt = 0\n    try:\n        for root, dirs, files in os.walk(basedir):\n            files = glob.glob(os.path.join(root,'*.h5'))\n            cnt += len(files)",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "create_track_file",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def create_track_file(maindir,trackid,track,song,artist,mbconnect=None):\n    \"\"\"\n    Main function to create an HDF5 song file.\n    You got to have the track, song and artist already.\n    If you pass an open connection to the musicbrainz database, we also use it.\n    Returns True if song was created, False otherwise.\n    False can mean another thread is already doing that song.\n    We also check whether the path exists.\n    INPUT\n       maindir      - main directory of the Million Song Dataset",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "create_track_file_from_trackid",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def create_track_file_from_trackid(maindir,trackid,song,artist,mbconnect=None):\n    \"\"\"\n    Get a track from a track id and calls for its creation.\n    We assume we already have song and artist.\n    We can have a connection to musicbrainz as an option.\n    This function should create only one file!\n    GOAL: mostly, it checks if we have the track already created before\n          calling EchoNest API. It saves some calls/time\n          Also, handles some errors.\n    INPUT",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "create_track_file_from_song",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def create_track_file_from_song(maindir,song,artist,mbconnect=None):\n    \"\"\"\n    Get tracks from a song, choose the first one and calls for its creation.\n    We assume we already have song and artist.\n    We can have a connection to musicbrainz as an option.\n    This function should create only one file!\n    GOAL: handles some errors.\n    INPUT\n        maindir    - MillionSongDataset root directory\n        song       - pyechonest song object for that track",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "create_track_file_from_song_noartist",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def create_track_file_from_song_noartist(maindir,song,mbconnect=None):\n    \"\"\"\n    After getting the artist, get tracks from a song, choose the first one and calls for its creation.\n    We assume we already have a song.\n    We can have a connection to musicbrainz as an option.\n    This function should create only one file!\n    GOAL: handles some errors.\n    INPUT\n        maindir    - MillionSongDataset root directory\n        song       - pyechonest song object for that track",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "create_track_files_from_artist",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def create_track_files_from_artist(maindir,artist,mbconnect=None,maxsongs=100):\n    \"\"\"\n    Get all songs from an artist, for each song call for its creation\n    We assume we already have artist.\n    We can have a connection to musicbrainz as an option.\n    This function should create only one file!\n    GOAL: handles some errors.\n    INPUT\n        maindir    - MillionSongDataset root directory\n        artist     - pyechonest artist object for that song/track",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "create_track_files_from_artistid",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def create_track_files_from_artistid(maindir,artistid,mbconnect=None,maxsongs=100):\n    \"\"\"\n    Get an artist from its ID, get all his songs, for each song call for its creation\n    We assume we already have artist ID.\n    We can have a connection to musicbrainz as an option.\n    This function should create only one file!\n    GOAL: handles some errors.\n    INPUT\n        maindir    - MillionSongDataset root directory\n        artistid   - echonest artist id",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "get_top_terms",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def get_top_terms(nresults=1000):\n    \"\"\"\n    Get the top terms from the Echo Nest, up to 1000\n    \"\"\"\n    assert nresults <= 1000,'cannot ask for more than 1000 top terms'\n    url = \"http://developer.echonest.com/api/v4/artist/top_terms?api_key=\"\n    url += _api_dev_key + \"&format=json&results=\" + str(nresults)\n    # get terms\n    while True:\n        try:",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "get_most_familiar_artists",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def get_most_familiar_artists(nresults=100):\n    \"\"\"\n    Get the most familiar artists according to the Echo Nest\n    \"\"\"\n    assert nresults <= 100,'we cant ask for more than 100 artists at the moment'\n    locked = FAMILIARARTISTS_LOCK.acquire()\n    assert locked,'FAMILIARARTISTS_LOCK could not lock?'\n    # get top artists\n    while True:\n        try:",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "search_songs",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def search_songs(**args):\n    \"\"\"\n    Use the Search song API, wrapped in our usual error handling\n    try/except. All the args are passed toward songEN.search,\n    if there is an error... good luck!\n    Note that this implies we only look once, so not good for the step param if\n    implemented (or you must call that function again)\n    RETURN list of songs, can be empty\n    \"\"\"\n    while True:",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "get_artists_from_description",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def get_artists_from_description(description,nresults=100):\n    \"\"\"\n    Return artists given a string description,\n    for instance a tag.\n    \"\"\"\n    assert nresults <= 100,'we cant do more than 100 artists for the moment...'\n    # get the artists for that description\n    while True:\n        try:\n            artists = artistEN.search(description=description,sort='familiarity-desc',results=nresults,",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "get_similar_artists",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def get_similar_artists(artist):\n    \"\"\"\n    Get the list of similar artists from a target one, might be empty\n    \"\"\"\n    while True:\n        try:\n            similars = artist.get_similar()\n            break\n        except (KeyboardInterrupt,NameError):\n            close_creation()",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "get_artist_song_from_names",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def get_artist_song_from_names(artistname,songtitle):\n    \"\"\"\n    Get an artist and a song from their artist name and title,\n    return the two: artist,song or None,None if problem\n    \"\"\"\n    while True:\n        try:\n            songs = songEN.search(artist=artistname,\n                                  title=songtitle,results=1,\n                                  buckets=['artist_familiarity',",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "create_step10",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def create_step10(maindir,mbconnect=None,maxsongs=500,nfilesbuffer=0,verbose=0):\n    \"\"\"\n    Most likely the first step to the databse creation.\n    Get artists from the EchoNest based on familiarity\n    INPUT\n       maindir       - MillionSongDataset main directory\n       mbconnect     - open musicbrainz pg connection\n       maxsongs      - max number of songs per artist\n       nfilesbuffer  - number of files to leave when we reach the M songs,\n                       e.g. we stop adding new ones if there are more",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "create_step20",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def create_step20(maindir,mbconnect=None,maxsongs=500,nfilesbuffer=0,verbose=0):\n    \"\"\"\n    Get artists based on most used Echo Nest terms. Encode all their\n    songs (up to maxsongs)\n    INPUT\n       maindir       - MillionSongDataset main directory\n       mbconnect     - open musicbrainz pg connection\n       maxsongs      - max number of songs per artist\n       nfilesbuffer  - number of files to leave when we reach the M songs,\n                       e.g. we stop adding new ones if there are more",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "create_step30",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def create_step30(maindir,mbconnect=None,maxsongs=500,nfilesbuffer=0):\n    \"\"\"\n    Get artists and songs from the CAL500 dataset.\n    First search for a particular pair artist/song, then from\n    that artist, get all possible songs (up to maxsongs)\n    We assume the file 'cal500_artist_song.txt' is in CAL500 param\n    which is an online URL (to github probably)\n    INPUT\n       maindir       - MillionSongDataset main directory\n       mbconnect     - open musicbrainz pg connection",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "create_step40",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def create_step40(maindir,mbconnect=None,maxsongs=100,nfilesbuffer=0):\n    \"\"\"\n    Search for songs with different attributes, danceability, energy, ...\n    INPUT\n       maindir       - root directory of the Million Song dataset\n       mbconnect     - open pg connection to Musicbrainz\n       maxsongs      - max number of song per search (max=100)\n       nfilesbuffer  - number of files we leave unfilled in the dataset\n    RETURN\n       the number of songs actually created",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "create_step60",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def create_step60(maindir,mbconnect=None,maxsongs=100,nfilesbuffer=0):\n    \"\"\"\n    Makes sure we have the similar artists to the top 100 most familiar\n    artists, and then go on with more similar artists.\n    INPUT\n       maindir       - root directory of the Million Song dataset\n       mbconnect     - open pg connection to Musicbrainz\n       maxsongs      - max number of song per search (max=100)\n       nfilesbuffer  - number of files we leave unfilled in the dataset\n    RETURN",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "run_steps_wrapper",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def run_steps_wrapper(args):\n    \"\"\" wrapper function for multiprocessor, calls run_steps \"\"\"\n    run_steps(**args)\ndef run_steps(maindir,nomb=False,nfilesbuffer=0,startstep=0,onlystep=-1,idxthread=0):\n    \"\"\"\n    Main function to run the different steps of the dataset creation.\n    Each thread should be initialized by calling this function.\n    INPUT\n       maindir       - main directory of the Million Song Dataset\n       nomb          - if True, don't use musicbrainz",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "run_steps",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def run_steps(maindir,nomb=False,nfilesbuffer=0,startstep=0,onlystep=-1,idxthread=0):\n    \"\"\"\n    Main function to run the different steps of the dataset creation.\n    Each thread should be initialized by calling this function.\n    INPUT\n       maindir       - main directory of the Million Song Dataset\n       nomb          - if True, don't use musicbrainz\n       nfilesbuffer  -\n    \"\"\"\n    print 'run_steps is launched on dir:',maindir",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'dataset_creator.py'\n    print '    by T. Bertin-Mahieux (2010) Columbia University'\n    print '       tb2332@columbia.edu'\n    print 'Download data from the EchoNest to create the MillionSongDataset'\n    print 'usage:'\n    print '   python dataset_creator.py [FLAGS] <maindir>'\n    print 'FLAGS'\n    print '  -nthreads n      - number of threads to use'",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "TRACKSET_LOCK",
        "kind": 5,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "TRACKSET_LOCK = multiprocessing.Lock()\nTRACKSET_CLOSED = multiprocessing.Value('b')\nTRACKSET_CLOSED = False # use to end the process, nothing can get a\n                        # track lock if this is turn to True\nCREATION_CLOSED = multiprocessing.Value('b')\nCREATION_CLOSED = False # use to end all threads at a higher level\n                        # than trackset closed, it is more a matter\n                        # of printing and returning than the risk of\n                        # creating a corrupted file\nclass my_trackset():",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "TRACKSET_CLOSED",
        "kind": 5,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "TRACKSET_CLOSED = multiprocessing.Value('b')\nTRACKSET_CLOSED = False # use to end the process, nothing can get a\n                        # track lock if this is turn to True\nCREATION_CLOSED = multiprocessing.Value('b')\nCREATION_CLOSED = False # use to end all threads at a higher level\n                        # than trackset closed, it is more a matter\n                        # of printing and returning than the risk of\n                        # creating a corrupted file\nclass my_trackset():\n    \"\"\"",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "TRACKSET_CLOSED",
        "kind": 5,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "TRACKSET_CLOSED = False # use to end the process, nothing can get a\n                        # track lock if this is turn to True\nCREATION_CLOSED = multiprocessing.Value('b')\nCREATION_CLOSED = False # use to end all threads at a higher level\n                        # than trackset closed, it is more a matter\n                        # of printing and returning than the risk of\n                        # creating a corrupted file\nclass my_trackset():\n    \"\"\"\n    class works with multiprocessing",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "CREATION_CLOSED",
        "kind": 5,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "CREATION_CLOSED = multiprocessing.Value('b')\nCREATION_CLOSED = False # use to end all threads at a higher level\n                        # than trackset closed, it is more a matter\n                        # of printing and returning than the risk of\n                        # creating a corrupted file\nclass my_trackset():\n    \"\"\"\n    class works with multiprocessing\n    should look like a set from outside\n    \"\"\"",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "CREATION_CLOSED",
        "kind": 5,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "CREATION_CLOSED = False # use to end all threads at a higher level\n                        # than trackset closed, it is more a matter\n                        # of printing and returning than the risk of\n                        # creating a corrupted file\nclass my_trackset():\n    \"\"\"\n    class works with multiprocessing\n    should look like a set from outside\n    \"\"\"\n    def __init__(self):",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "FAMILIARARTISTS_LOCK",
        "kind": 5,
        "importPath": "PythonSrc.DatasetCreation.dataset_creator",
        "description": "PythonSrc.DatasetCreation.dataset_creator",
        "peekOfCode": "FAMILIARARTISTS_LOCK = multiprocessing.Lock()\ndef get_most_familiar_artists(nresults=100):\n    \"\"\"\n    Get the most familiar artists according to the Echo Nest\n    \"\"\"\n    assert nresults <= 100,'we cant ask for more than 100 artists at the moment'\n    locked = FAMILIARARTISTS_LOCK.acquire()\n    assert locked,'FAMILIARARTISTS_LOCK could not lock?'\n    # get top artists\n    while True:",
        "detail": "PythonSrc.DatasetCreation.dataset_creator",
        "documentation": {}
    },
    {
        "label": "get_all_files",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_filestats",
        "description": "PythonSrc.DatasetCreation.dataset_filestats",
        "peekOfCode": "def get_all_files(basedir,ext='.h5') :\n    \"\"\"\n    From a root directory, go through all subdirectories\n    and find all files with the given extension.\n    Return all absolute paths in a list.\n    \"\"\"\n    allfiles = []\n    for root, dirs, files in os.walk(basedir):\n        files = glob.glob(os.path.join(root,'*'+ext))\n        for f in files :",
        "detail": "PythonSrc.DatasetCreation.dataset_filestats",
        "documentation": {}
    },
    {
        "label": "count_normal_leaves",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_filestats",
        "description": "PythonSrc.DatasetCreation.dataset_filestats",
        "peekOfCode": "def count_normal_leaves(basedir,revindex=True):\n    \"\"\"\n    Count how many directories are of the form\n    basedir/A/B/C\n    If revindex, we fill up MAP_NFILES_DIR where\n    the keys are number of files, and the value is\n    a set of directory filenames\n    \"\"\"\n    cnt = 0\n    for root, dirs, files in os.walk(basedir):",
        "detail": "PythonSrc.DatasetCreation.dataset_filestats",
        "documentation": {}
    },
    {
        "label": "get_all_files_modif_date",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_filestats",
        "description": "PythonSrc.DatasetCreation.dataset_filestats",
        "peekOfCode": "def get_all_files_modif_date(basedir,ext='.h5'):\n    \"\"\"\n    From a root directory, look at all the file,\n    get their last modification date, put in in priority\n    queue so the most recent file pop up first\n    \"\"\"\n    for root, dirs, files in os.walk(basedir):\n        files = glob.glob(os.path.join(root,'*'+ext))\n        for f in files :\n            mdate = file_modif_date(f)",
        "detail": "PythonSrc.DatasetCreation.dataset_filestats",
        "documentation": {}
    },
    {
        "label": "file_modif_date",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_filestats",
        "description": "PythonSrc.DatasetCreation.dataset_filestats",
        "peekOfCode": "def file_modif_date(f):\n    \"\"\" return modif date in seconds (as in time.time()) \"\"\"\n    return os.stat(f).st_mtime\ndef die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'dataset_filestats.py'\n    print '   by T. Bertin-Mahieux (2010) Columbia University'\n    print '      tb2332@columbia.edu'\n    print 'Simple util to check the file repartition and the most'\n    print 'recent file in the Million Song dataset directory'",
        "detail": "PythonSrc.DatasetCreation.dataset_filestats",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_filestats",
        "description": "PythonSrc.DatasetCreation.dataset_filestats",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'dataset_filestats.py'\n    print '   by T. Bertin-Mahieux (2010) Columbia University'\n    print '      tb2332@columbia.edu'\n    print 'Simple util to check the file repartition and the most'\n    print 'recent file in the Million Song dataset directory'\n    print 'usage:'\n    print '   python dataset_filestats.py <maindir>'\n    sys.exit(0)",
        "detail": "PythonSrc.DatasetCreation.dataset_filestats",
        "documentation": {}
    },
    {
        "label": "MODIFQUEUE",
        "kind": 5,
        "importPath": "PythonSrc.DatasetCreation.dataset_filestats",
        "description": "PythonSrc.DatasetCreation.dataset_filestats",
        "peekOfCode": "MODIFQUEUE = PriorityQueue()\n# list of leaves ordered by number of files\n# get filled up when we count leaves (by default)\nMAP_NFILES_DIR = {}\ndef get_all_files(basedir,ext='.h5') :\n    \"\"\"\n    From a root directory, go through all subdirectories\n    and find all files with the given extension.\n    Return all absolute paths in a list.\n    \"\"\"",
        "detail": "PythonSrc.DatasetCreation.dataset_filestats",
        "documentation": {}
    },
    {
        "label": "MAP_NFILES_DIR",
        "kind": 5,
        "importPath": "PythonSrc.DatasetCreation.dataset_filestats",
        "description": "PythonSrc.DatasetCreation.dataset_filestats",
        "peekOfCode": "MAP_NFILES_DIR = {}\ndef get_all_files(basedir,ext='.h5') :\n    \"\"\"\n    From a root directory, go through all subdirectories\n    and find all files with the given extension.\n    Return all absolute paths in a list.\n    \"\"\"\n    allfiles = []\n    for root, dirs, files in os.walk(basedir):\n        files = glob.glob(os.path.join(root,'*'+ext))",
        "detail": "PythonSrc.DatasetCreation.dataset_filestats",
        "documentation": {}
    },
    {
        "label": "KeyboardInterruptError",
        "kind": 6,
        "importPath": "PythonSrc.DatasetCreation.dataset_sanity_check",
        "description": "PythonSrc.DatasetCreation.dataset_sanity_check",
        "peekOfCode": "class KeyboardInterruptError(Exception):pass\n# wrapper\ndef sanity_check_1thread_wrapper(args):\n    \"\"\" wrapper for multiprocessing to call the real function \"\"\"\n    sanity_check_1thread(**args)\n# actual function\ndef sanity_check_1thread(maindir=None,threadid=-1,nthreads=-1,allfiles=[]):\n    \"\"\"\n    Main function, check a bunch of files by opening every field in\n    getter.",
        "detail": "PythonSrc.DatasetCreation.dataset_sanity_check",
        "documentation": {}
    },
    {
        "label": "get_all_files",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_sanity_check",
        "description": "PythonSrc.DatasetCreation.dataset_sanity_check",
        "peekOfCode": "def get_all_files(basedir,ext='.h5') :\n    \"\"\"\n    From a root directory, go through all subdirectories\n    and find all files with the given extension.\n    Return all absolute paths in a list.\n    \"\"\"\n    allfiles = []\n    for root, dirs, files in os.walk(basedir):\n        files = glob.glob(os.path.join(root,'*'+ext))\n        for f in files :",
        "detail": "PythonSrc.DatasetCreation.dataset_sanity_check",
        "documentation": {}
    },
    {
        "label": "sanity_check_1thread_wrapper",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_sanity_check",
        "description": "PythonSrc.DatasetCreation.dataset_sanity_check",
        "peekOfCode": "def sanity_check_1thread_wrapper(args):\n    \"\"\" wrapper for multiprocessing to call the real function \"\"\"\n    sanity_check_1thread(**args)\n# actual function\ndef sanity_check_1thread(maindir=None,threadid=-1,nthreads=-1,allfiles=[]):\n    \"\"\"\n    Main function, check a bunch of files by opening every field in\n    getter.\n    \"\"\"\n    assert not maindir is None,'wrong param maindir'",
        "detail": "PythonSrc.DatasetCreation.dataset_sanity_check",
        "documentation": {}
    },
    {
        "label": "sanity_check_1thread",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_sanity_check",
        "description": "PythonSrc.DatasetCreation.dataset_sanity_check",
        "peekOfCode": "def sanity_check_1thread(maindir=None,threadid=-1,nthreads=-1,allfiles=[]):\n    \"\"\"\n    Main function, check a bunch of files by opening every field in\n    getter.\n    \"\"\"\n    assert not maindir is None,'wrong param maindir'\n    assert threadid>-1,'wrong param threadid'\n    assert nthreads>0,'wrong param nthreads'\n    assert len(allfiles)>0,'wrong param allfiles, or no files'\n    # get getters",
        "detail": "PythonSrc.DatasetCreation.dataset_sanity_check",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "PythonSrc.DatasetCreation.dataset_sanity_check",
        "description": "PythonSrc.DatasetCreation.dataset_sanity_check",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'dataset_sanity_check.py'\n    print '  by T. Bertin-Mahieux (2010) Columbia University'\n    print '     tb2332@columbia.edu'\n    print 'do a simple but full sanity check on the dataset'\n    print 'GOAL: read every field of every file to make sure nothing'\n    print '      is corrupted'\n    print 'usage:'\n    print '  python dataset_sanity_check.py -nthreads N <main dir>'",
        "detail": "PythonSrc.DatasetCreation.dataset_sanity_check",
        "documentation": {}
    },
    {
        "label": "encode_string",
        "kind": 2,
        "importPath": "PythonSrc.MBrainzDB.query",
        "description": "PythonSrc.MBrainzDB.query",
        "peekOfCode": "def encode_string(s):\n    \"\"\"\n    Simple utility function to make sure a string is proper\n    to be used in a SQL query\n    EXAMPLE:\n      That's my boy! -> N'That''s my boy!'\n    \"\"\"\n    res = \"N'\"+s.replace(\"'\",\"''\")+\"'\"\n    res = res.replace(\"\\\\''\",\"''\")\n    res = res.replace(\"\\''\",\"''\")",
        "detail": "PythonSrc.MBrainzDB.query",
        "documentation": {}
    },
    {
        "label": "connect_mbdb",
        "kind": 2,
        "importPath": "PythonSrc.MBrainzDB.query",
        "description": "PythonSrc.MBrainzDB.query",
        "peekOfCode": "def connect_mbdb():\n    \"\"\"\n    Simple connection to the musicbrainz database, returns a pgobject\n    Return None if there is a problem\n    \"\"\"\n    try:\n        connect = pg.connect('musicbrainz_db','localhost',-1,None,None,\n                             USER,PASSWD)\n    except TypeError as e:\n        print('CONNECT_MBDB: type error, should not happen:', e)",
        "detail": "PythonSrc.MBrainzDB.query",
        "documentation": {}
    },
    {
        "label": "find_year_safemode",
        "kind": 2,
        "importPath": "PythonSrc.MBrainzDB.query",
        "description": "PythonSrc.MBrainzDB.query",
        "peekOfCode": "def find_year_safemode(connect,artist_mbid,title,release,artist):\n    \"\"\"\n    This is the main function for the creation of the MillionSongDataset\n    We get a year value only if we have a recognized musicbrainz id\n    and an exact match on either the title or the release (lowercase).\n    Other possibility, exact match on title and artist_name or\n    release_and artist_name\n    INPUT\n        artist_mbid   string or None          artist musicbrainz id\n        title         string                  track name",
        "detail": "PythonSrc.MBrainzDB.query",
        "documentation": {}
    },
    {
        "label": "find_year_safemode_nombid",
        "kind": 2,
        "importPath": "PythonSrc.MBrainzDB.query",
        "description": "PythonSrc.MBrainzDB.query",
        "peekOfCode": "def find_year_safemode_nombid(connect,title,release,artist):\n    \"\"\"\n    We try to get a year for a particular track without musicbrainz id\n    for the artist.\n    We get only if we have a perfect match either for (artist_name / title)\n    or (artist_name / release)\n    RETURN 0 if not found, or year as int\n    \"\"\"\n    # find all albums based on tracks found by exact track title match\n    # return the earliest release year of one of these albums",
        "detail": "PythonSrc.MBrainzDB.query",
        "documentation": {}
    },
    {
        "label": "get_artist_tags",
        "kind": 2,
        "importPath": "PythonSrc.MBrainzDB.query",
        "description": "PythonSrc.MBrainzDB.query",
        "peekOfCode": "def get_artist_tags(connect, artist_mbid, maxtags=20):\n    \"\"\"\n    Get the musicbrainz tags and tag count given a musicbrainz\n    artist. Returns two list of length max 'maxtags'\n    Always return two lists, eventually empty\n    \"\"\"\n    if artist_mbid is None or artist_mbid == '':\n        return [],[]\n    # find all tags\n    q = \"SELECT tag.name,artist_tag.count FROM artist\"",
        "detail": "PythonSrc.MBrainzDB.query",
        "documentation": {}
    },
    {
        "label": "debug_from_song_file",
        "kind": 2,
        "importPath": "PythonSrc.MBrainzDB.query",
        "description": "PythonSrc.MBrainzDB.query",
        "peekOfCode": "def debug_from_song_file(connect,h5path,verbose=0):\n    \"\"\"\n    Slow debugging function that takes a h5 file, reads the info,\n    check the match with musicbrainz db, prints out the result.\n    Only prints when we dont get exact match!\n    RETURN counts of how many files we filled for years, tags\n    \"\"\"\n    import hdf5_utils as HDF5\n    import hdf5_getters as GETTERS\n    h5 = HDF5.open_h5_file_read(h5path)",
        "detail": "PythonSrc.MBrainzDB.query",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "PythonSrc.MBrainzDB.query",
        "description": "PythonSrc.MBrainzDB.query",
        "peekOfCode": "def die_with_usage():\n    print('This contains library functions to query the musicbrainz database')\n    print('For debugging:')\n    print('    python query.py -hdf5 <list of songs>')\n    print('    e.g. python query.py -hdf5 MillionSong/A/A/*/*.h5')\n    sys.exit(0)\nif __name__ == '__main__':\n    # help menu\n    if len(sys.argv) < 2:\n        die_with_usage()",
        "detail": "PythonSrc.MBrainzDB.query",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "PythonSrc.create_aggregate_file",
        "description": "PythonSrc.create_aggregate_file",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'create_aggregate_file.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print '   tb2332@columbia.edu'\n    print ''\n    print 'Creates an aggregate file from all song file (h5 files)'\n    print 'in a given directory.'\n    print 'Aggregate files contains many songs. and none of the arrays,'\n    print ''",
        "detail": "PythonSrc.create_aggregate_file",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "PythonSrc.create_summary_file",
        "description": "PythonSrc.create_summary_file",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'create_summary_file.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print '   tb2332@columbia.edu'\n    print ''\n    print 'Creates a summary file from all song file (h5 files)'\n    print 'in a given directory.'\n    print 'Summary files contains many songs and none of the arrays,'\n    print 'i.e. no beat/segment data, artist similarity, tags, ...'",
        "detail": "PythonSrc.create_summary_file",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "PythonSrc.display_song",
        "description": "PythonSrc.display_song",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'display_song.py'\n    print 'T. Bertin-Mahieux (2010) tb2332@columbia.edu'\n    print 'to quickly display all we know about a song'\n    print 'usage:'\n    print '   python display_song.py [FLAGS] <HDF5 file> <OPT: song idx> <OPT: getter>'\n    print 'example:'\n    print '   python display_song.py mysong.h5 0 danceability'\n    print 'INPUTS'",
        "detail": "PythonSrc.display_song",
        "documentation": {}
    },
    {
        "label": "KeyboardInterruptError",
        "kind": 6,
        "importPath": "PythonSrc.enpyapi_to_hdf5",
        "description": "PythonSrc.enpyapi_to_hdf5",
        "peekOfCode": "class KeyboardInterruptError(Exception):pass\ndef convert_one_song(audiofile,output,mbconnect=None,verbose=0,DESTROYAUDIO=False):\n    \"\"\"\n    PRINCIPAL FUNCTION\n    Converts one given audio file to hdf5 format (saved in 'output')\n    by uploading it to The Echo Nest API\n    INPUT\n         audiofile   - path to a typical audio file (wav, mp3, ...)\n            output   - nonexisting hdf5 path\n         mbconnect   - if not None, open connection to musicbrainz server",
        "detail": "PythonSrc.enpyapi_to_hdf5",
        "documentation": {}
    },
    {
        "label": "convert_one_song",
        "kind": 2,
        "importPath": "PythonSrc.enpyapi_to_hdf5",
        "description": "PythonSrc.enpyapi_to_hdf5",
        "peekOfCode": "def convert_one_song(audiofile,output,mbconnect=None,verbose=0,DESTROYAUDIO=False):\n    \"\"\"\n    PRINCIPAL FUNCTION\n    Converts one given audio file to hdf5 format (saved in 'output')\n    by uploading it to The Echo Nest API\n    INPUT\n         audiofile   - path to a typical audio file (wav, mp3, ...)\n            output   - nonexisting hdf5 path\n         mbconnect   - if not None, open connection to musicbrainz server\n           verbose   - if >0 display more information",
        "detail": "PythonSrc.enpyapi_to_hdf5",
        "documentation": {}
    },
    {
        "label": "convert_one_song_wrapper",
        "kind": 2,
        "importPath": "PythonSrc.enpyapi_to_hdf5",
        "description": "PythonSrc.enpyapi_to_hdf5",
        "peekOfCode": "def convert_one_song_wrapper(args):\n    \"\"\" for multiprocessing \"\"\"\n    mbconnect = None\n    if args['usemb']:\n        if verbose>0: print 'fill HDF5 file using musicbrainz'\n        mbconnect = pg.connect('musicbrainz_db','localhost',-1,None,None,\n                               args['mbuser'],args['mbpasswd'])\n    try:\n        convert_one_song(args['audiofile'],args['output'],\n                         mbconnect=mbconnect,verbose=args['verbose'],",
        "detail": "PythonSrc.enpyapi_to_hdf5",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "PythonSrc.enpyapi_to_hdf5",
        "description": "PythonSrc.enpyapi_to_hdf5",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'enpyapi_to_hdf5.py'\n    print 'by T. Bertin-Mahieux (2010) Columbia University'\n    print ''\n    print 'Upload a song to get its analysis, writes it to a HDF5 file'\n    print 'using the Million Song Database format'\n    print 'NO GUARANTEE THAT THE FILE IS KNOWN! => no artist or song name'\n    print 'Note that we do not catch errors like timeouts, etc.'\n    print ''",
        "detail": "PythonSrc.enpyapi_to_hdf5",
        "documentation": {}
    },
    {
        "label": "DEF_MB_USER",
        "kind": 5,
        "importPath": "PythonSrc.enpyapi_to_hdf5",
        "description": "PythonSrc.enpyapi_to_hdf5",
        "peekOfCode": "DEF_MB_USER = 'gordon'\nDEF_MB_PASSWD = 'gordon'\n# for multiprocessing\nclass KeyboardInterruptError(Exception):pass\ndef convert_one_song(audiofile,output,mbconnect=None,verbose=0,DESTROYAUDIO=False):\n    \"\"\"\n    PRINCIPAL FUNCTION\n    Converts one given audio file to hdf5 format (saved in 'output')\n    by uploading it to The Echo Nest API\n    INPUT",
        "detail": "PythonSrc.enpyapi_to_hdf5",
        "documentation": {}
    },
    {
        "label": "DEF_MB_PASSWD",
        "kind": 5,
        "importPath": "PythonSrc.enpyapi_to_hdf5",
        "description": "PythonSrc.enpyapi_to_hdf5",
        "peekOfCode": "DEF_MB_PASSWD = 'gordon'\n# for multiprocessing\nclass KeyboardInterruptError(Exception):pass\ndef convert_one_song(audiofile,output,mbconnect=None,verbose=0,DESTROYAUDIO=False):\n    \"\"\"\n    PRINCIPAL FUNCTION\n    Converts one given audio file to hdf5 format (saved in 'output')\n    by uploading it to The Echo Nest API\n    INPUT\n         audiofile   - path to a typical audio file (wav, mp3, ...)",
        "detail": "PythonSrc.enpyapi_to_hdf5",
        "documentation": {}
    },
    {
        "label": "SongMetaData",
        "kind": 6,
        "importPath": "PythonSrc.hdf5_descriptors",
        "description": "PythonSrc.hdf5_descriptors",
        "peekOfCode": "class SongMetaData(tables.IsDescription):\n    \"\"\"\n    Class to hold the metadata of one song\n    \"\"\"\n    artist_name = tables.StringCol(MAXSTRLEN)\n    artist_id = tables.StringCol(32)\n    artist_mbid = tables.StringCol(40)\n    artist_playmeid = tables.IntCol()\n    artist_7digitalid = tables.IntCol()\n    analyzer_version = tables.StringCol(32)",
        "detail": "PythonSrc.hdf5_descriptors",
        "documentation": {}
    },
    {
        "label": "SongAnalysis",
        "kind": 6,
        "importPath": "PythonSrc.hdf5_descriptors",
        "description": "PythonSrc.hdf5_descriptors",
        "peekOfCode": "class SongAnalysis(tables.IsDescription):\n    \"\"\"\n    Class to hold the analysis of one song\n    \"\"\"\n    analysis_sample_rate = tables.IntCol()\n    audio_md5 = tables.StringCol(32)\n    danceability = tables.Float64Col()\n    duration = tables.Float64Col()\n    end_of_fade_in = tables.Float64Col()\n    energy = tables.Float64Col()",
        "detail": "PythonSrc.hdf5_descriptors",
        "documentation": {}
    },
    {
        "label": "SongMusicBrainz",
        "kind": 6,
        "importPath": "PythonSrc.hdf5_descriptors",
        "description": "PythonSrc.hdf5_descriptors",
        "peekOfCode": "class SongMusicBrainz(tables.IsDescription):\n    \"\"\"\n    Class to hold information coming from\n    MusicBrainz for one song\n    \"\"\"\n    year = tables.IntCol()\n    # ARRAY INDEX\n    idx_artist_mbtags = tables.IntCol()",
        "detail": "PythonSrc.hdf5_descriptors",
        "documentation": {}
    },
    {
        "label": "MAXSTRLEN",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_descriptors",
        "description": "PythonSrc.hdf5_descriptors",
        "peekOfCode": "MAXSTRLEN = 1024\nclass SongMetaData(tables.IsDescription):\n    \"\"\"\n    Class to hold the metadata of one song\n    \"\"\"\n    artist_name = tables.StringCol(MAXSTRLEN)\n    artist_id = tables.StringCol(32)\n    artist_mbid = tables.StringCol(40)\n    artist_playmeid = tables.IntCol()\n    artist_7digitalid = tables.IntCol()",
        "detail": "PythonSrc.hdf5_descriptors",
        "documentation": {}
    },
    {
        "label": "open_h5_file_read",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def open_h5_file_read(h5filename):\n    \"\"\"\n    Open an existing H5 in read mode.\n    Same function as in hdf5_utils, here so we avoid one import\n    \"\"\"\n    return tables.openFile(h5filename, mode='r')\ndef get_num_songs(h5):\n    \"\"\"\n    Return the number of songs contained in this h5 file, i.e. the number of rows\n    for all basic informations like name, artist, ...",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_num_songs",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_num_songs(h5):\n    \"\"\"\n    Return the number of songs contained in this h5 file, i.e. the number of rows\n    for all basic informations like name, artist, ...\n    \"\"\"\n    return h5.root.metadata.songs.nrows\ndef get_artist_familiarity(h5,songidx=0):\n    \"\"\"\n    Get artist familiarity from a HDF5 song file, by default the first song in it\n    \"\"\"",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_artist_familiarity",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_artist_familiarity(h5,songidx=0):\n    \"\"\"\n    Get artist familiarity from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.artist_familiarity[songidx]\ndef get_artist_hotttnesss(h5,songidx=0):\n    \"\"\"\n    Get artist hotttnesss from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.artist_hotttnesss[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_artist_hotttnesss",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_artist_hotttnesss(h5,songidx=0):\n    \"\"\"\n    Get artist hotttnesss from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.artist_hotttnesss[songidx]\ndef get_artist_id(h5,songidx=0):\n    \"\"\"\n    Get artist id from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.artist_id[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_artist_id",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_artist_id(h5,songidx=0):\n    \"\"\"\n    Get artist id from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.artist_id[songidx]\ndef get_artist_mbid(h5,songidx=0):\n    \"\"\"\n    Get artist musibrainz id from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.artist_mbid[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_artist_mbid",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_artist_mbid(h5,songidx=0):\n    \"\"\"\n    Get artist musibrainz id from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.artist_mbid[songidx]\ndef get_artist_playmeid(h5,songidx=0):\n    \"\"\"\n    Get artist playme id from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.artist_playmeid[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_artist_playmeid",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_artist_playmeid(h5,songidx=0):\n    \"\"\"\n    Get artist playme id from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.artist_playmeid[songidx]\ndef get_artist_7digitalid(h5,songidx=0):\n    \"\"\"\n    Get artist 7digital id from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.artist_7digitalid[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_artist_7digitalid",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_artist_7digitalid(h5,songidx=0):\n    \"\"\"\n    Get artist 7digital id from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.artist_7digitalid[songidx]\ndef get_artist_latitude(h5,songidx=0):\n    \"\"\"\n    Get artist latitude from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.artist_latitude[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_artist_latitude",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_artist_latitude(h5,songidx=0):\n    \"\"\"\n    Get artist latitude from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.artist_latitude[songidx]\ndef get_artist_longitude(h5,songidx=0):\n    \"\"\"\n    Get artist longitude from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.artist_longitude[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_artist_longitude",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_artist_longitude(h5,songidx=0):\n    \"\"\"\n    Get artist longitude from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.artist_longitude[songidx]\ndef get_artist_location(h5,songidx=0):\n    \"\"\"\n    Get artist location from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.artist_location[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_artist_location",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_artist_location(h5,songidx=0):\n    \"\"\"\n    Get artist location from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.artist_location[songidx]\ndef get_artist_name(h5,songidx=0):\n    \"\"\"\n    Get artist name from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.artist_name[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_artist_name",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_artist_name(h5,songidx=0):\n    \"\"\"\n    Get artist name from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.artist_name[songidx]\ndef get_release(h5,songidx=0):\n    \"\"\"\n    Get release from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.release[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_release",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_release(h5,songidx=0):\n    \"\"\"\n    Get release from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.release[songidx]\ndef get_release_7digitalid(h5,songidx=0):\n    \"\"\"\n    Get release 7digital id from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.release_7digitalid[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_release_7digitalid",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_release_7digitalid(h5,songidx=0):\n    \"\"\"\n    Get release 7digital id from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.release_7digitalid[songidx]\ndef get_song_id(h5,songidx=0):\n    \"\"\"\n    Get song id from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.song_id[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_song_id",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_song_id(h5,songidx=0):\n    \"\"\"\n    Get song id from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.song_id[songidx]\ndef get_song_hotttnesss(h5,songidx=0):\n    \"\"\"\n    Get song hotttnesss from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.song_hotttnesss[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_song_hotttnesss",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_song_hotttnesss(h5,songidx=0):\n    \"\"\"\n    Get song hotttnesss from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.song_hotttnesss[songidx]\ndef get_title(h5,songidx=0):\n    \"\"\"\n    Get title from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.title[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_title",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_title(h5,songidx=0):\n    \"\"\"\n    Get title from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.title[songidx]\ndef get_track_7digitalid(h5,songidx=0):\n    \"\"\"\n    Get track 7digital id from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.track_7digitalid[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_track_7digitalid",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_track_7digitalid(h5,songidx=0):\n    \"\"\"\n    Get track 7digital id from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.metadata.songs.cols.track_7digitalid[songidx]\ndef get_similar_artists(h5,songidx=0):\n    \"\"\"\n    Get similar artists array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_similar_artists",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_similar_artists(h5,songidx=0):\n    \"\"\"\n    Get similar artists array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.metadata.songs.nrows == songidx + 1:\n        return h5.root.metadata.similar_artists[h5.root.metadata.songs.cols.idx_similar_artists[songidx]:]\n    return h5.root.metadata.similar_artists[h5.root.metadata.songs.cols.idx_similar_artists[songidx]:\n                                            h5.root.metadata.songs.cols.idx_similar_artists[songidx+1]]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_artist_terms",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_artist_terms(h5,songidx=0):\n    \"\"\"\n    Get artist terms array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.metadata.songs.nrows == songidx + 1:\n        return h5.root.metadata.artist_terms[h5.root.metadata.songs.cols.idx_artist_terms[songidx]:]\n    return h5.root.metadata.artist_terms[h5.root.metadata.songs.cols.idx_artist_terms[songidx]:\n                                            h5.root.metadata.songs.cols.idx_artist_terms[songidx+1]]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_artist_terms_freq",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_artist_terms_freq(h5,songidx=0):\n    \"\"\"\n    Get artist terms array frequencies. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.metadata.songs.nrows == songidx + 1:\n        return h5.root.metadata.artist_terms_freq[h5.root.metadata.songs.cols.idx_artist_terms[songidx]:]\n    return h5.root.metadata.artist_terms_freq[h5.root.metadata.songs.cols.idx_artist_terms[songidx]:\n                                              h5.root.metadata.songs.cols.idx_artist_terms[songidx+1]]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_artist_terms_weight",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_artist_terms_weight(h5,songidx=0):\n    \"\"\"\n    Get artist terms array frequencies. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.metadata.songs.nrows == songidx + 1:\n        return h5.root.metadata.artist_terms_weight[h5.root.metadata.songs.cols.idx_artist_terms[songidx]:]\n    return h5.root.metadata.artist_terms_weight[h5.root.metadata.songs.cols.idx_artist_terms[songidx]:\n                                                h5.root.metadata.songs.cols.idx_artist_terms[songidx+1]]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_analysis_sample_rate",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_analysis_sample_rate(h5,songidx=0):\n    \"\"\"\n    Get analysis sample rate from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.analysis_sample_rate[songidx]\ndef get_audio_md5(h5,songidx=0):\n    \"\"\"\n    Get audio MD5 from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.audio_md5[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_audio_md5",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_audio_md5(h5,songidx=0):\n    \"\"\"\n    Get audio MD5 from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.audio_md5[songidx]\ndef get_danceability(h5,songidx=0):\n    \"\"\"\n    Get danceability from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.danceability[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_danceability",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_danceability(h5,songidx=0):\n    \"\"\"\n    Get danceability from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.danceability[songidx]\ndef get_duration(h5,songidx=0):\n    \"\"\"\n    Get duration from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.duration[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_duration",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_duration(h5,songidx=0):\n    \"\"\"\n    Get duration from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.duration[songidx]\ndef get_end_of_fade_in(h5,songidx=0):\n    \"\"\"\n    Get end of fade in from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.end_of_fade_in[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_end_of_fade_in",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_end_of_fade_in(h5,songidx=0):\n    \"\"\"\n    Get end of fade in from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.end_of_fade_in[songidx]\ndef get_energy(h5,songidx=0):\n    \"\"\"\n    Get energy from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.energy[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_energy",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_energy(h5,songidx=0):\n    \"\"\"\n    Get energy from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.energy[songidx]\ndef get_key(h5,songidx=0):\n    \"\"\"\n    Get key from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.key[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_key",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_key(h5,songidx=0):\n    \"\"\"\n    Get key from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.key[songidx]\ndef get_key_confidence(h5,songidx=0):\n    \"\"\"\n    Get key confidence from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.key_confidence[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_key_confidence",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_key_confidence(h5,songidx=0):\n    \"\"\"\n    Get key confidence from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.key_confidence[songidx]\ndef get_loudness(h5,songidx=0):\n    \"\"\"\n    Get loudness from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.loudness[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_loudness",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_loudness(h5,songidx=0):\n    \"\"\"\n    Get loudness from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.loudness[songidx]\ndef get_mode(h5,songidx=0):\n    \"\"\"\n    Get mode from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.mode[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_mode",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_mode(h5,songidx=0):\n    \"\"\"\n    Get mode from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.mode[songidx]\ndef get_mode_confidence(h5,songidx=0):\n    \"\"\"\n    Get mode confidence from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.mode_confidence[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_mode_confidence",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_mode_confidence(h5,songidx=0):\n    \"\"\"\n    Get mode confidence from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.mode_confidence[songidx]\ndef get_start_of_fade_out(h5,songidx=0):\n    \"\"\"\n    Get start of fade out from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.start_of_fade_out[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_start_of_fade_out",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_start_of_fade_out(h5,songidx=0):\n    \"\"\"\n    Get start of fade out from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.start_of_fade_out[songidx]\ndef get_tempo(h5,songidx=0):\n    \"\"\"\n    Get tempo from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.tempo[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_tempo",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_tempo(h5,songidx=0):\n    \"\"\"\n    Get tempo from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.tempo[songidx]\ndef get_time_signature(h5,songidx=0):\n    \"\"\"\n    Get signature from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.time_signature[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_time_signature",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_time_signature(h5,songidx=0):\n    \"\"\"\n    Get signature from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.time_signature[songidx]\ndef get_time_signature_confidence(h5,songidx=0):\n    \"\"\"\n    Get signature confidence from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.time_signature_confidence[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_time_signature_confidence",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_time_signature_confidence(h5,songidx=0):\n    \"\"\"\n    Get signature confidence from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.time_signature_confidence[songidx]\ndef get_track_id(h5,songidx=0):\n    \"\"\"\n    Get track id from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.track_id[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_track_id",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_track_id(h5,songidx=0):\n    \"\"\"\n    Get track id from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.analysis.songs.cols.track_id[songidx]\ndef get_segments_start(h5,songidx=0):\n    \"\"\"\n    Get segments start array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_segments_start",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_segments_start(h5,songidx=0):\n    \"\"\"\n    Get segments start array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.analysis.songs.nrows == songidx + 1:\n        return h5.root.analysis.segments_start[h5.root.analysis.songs.cols.idx_segments_start[songidx]:]\n    return h5.root.analysis.segments_start[h5.root.analysis.songs.cols.idx_segments_start[songidx]:\n                                           h5.root.analysis.songs.cols.idx_segments_start[songidx+1]]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_segments_confidence",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_segments_confidence(h5,songidx=0):\n    \"\"\"\n    Get segments confidence array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.analysis.songs.nrows == songidx + 1:\n        return h5.root.analysis.segments_confidence[h5.root.analysis.songs.cols.idx_segments_confidence[songidx]:]\n    return h5.root.analysis.segments_confidence[h5.root.analysis.songs.cols.idx_segments_confidence[songidx]:\n                                                h5.root.analysis.songs.cols.idx_segments_confidence[songidx+1]]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_segments_pitches",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_segments_pitches(h5,songidx=0):\n    \"\"\"\n    Get segments pitches array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.analysis.songs.nrows == songidx + 1:\n        return h5.root.analysis.segments_pitches[h5.root.analysis.songs.cols.idx_segments_pitches[songidx]:,:]\n    return h5.root.analysis.segments_pitches[h5.root.analysis.songs.cols.idx_segments_pitches[songidx]:\n                                             h5.root.analysis.songs.cols.idx_segments_pitches[songidx+1],:]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_segments_timbre",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_segments_timbre(h5,songidx=0):\n    \"\"\"\n    Get segments timbre array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.analysis.songs.nrows == songidx + 1:\n        return h5.root.analysis.segments_timbre[h5.root.analysis.songs.cols.idx_segments_timbre[songidx]:,:]\n    return h5.root.analysis.segments_timbre[h5.root.analysis.songs.cols.idx_segments_timbre[songidx]:\n                                            h5.root.analysis.songs.cols.idx_segments_timbre[songidx+1],:]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_segments_loudness_max",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_segments_loudness_max(h5,songidx=0):\n    \"\"\"\n    Get segments loudness max array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.analysis.songs.nrows == songidx + 1:\n        return h5.root.analysis.segments_loudness_max[h5.root.analysis.songs.cols.idx_segments_loudness_max[songidx]:]\n    return h5.root.analysis.segments_loudness_max[h5.root.analysis.songs.cols.idx_segments_loudness_max[songidx]:\n                                                  h5.root.analysis.songs.cols.idx_segments_loudness_max[songidx+1]]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_segments_loudness_max_time",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_segments_loudness_max_time(h5,songidx=0):\n    \"\"\"\n    Get segments loudness max time array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.analysis.songs.nrows == songidx + 1:\n        return h5.root.analysis.segments_loudness_max_time[h5.root.analysis.songs.cols.idx_segments_loudness_max_time[songidx]:]\n    return h5.root.analysis.segments_loudness_max_time[h5.root.analysis.songs.cols.idx_segments_loudness_max_time[songidx]:\n                                                       h5.root.analysis.songs.cols.idx_segments_loudness_max_time[songidx+1]]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_segments_loudness_start",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_segments_loudness_start(h5,songidx=0):\n    \"\"\"\n    Get segments loudness start array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.analysis.songs.nrows == songidx + 1:\n        return h5.root.analysis.segments_loudness_start[h5.root.analysis.songs.cols.idx_segments_loudness_start[songidx]:]\n    return h5.root.analysis.segments_loudness_start[h5.root.analysis.songs.cols.idx_segments_loudness_start[songidx]:\n                                                    h5.root.analysis.songs.cols.idx_segments_loudness_start[songidx+1]]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_sections_start",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_sections_start(h5,songidx=0):\n    \"\"\"\n    Get sections start array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.analysis.songs.nrows == songidx + 1:\n        return h5.root.analysis.sections_start[h5.root.analysis.songs.cols.idx_sections_start[songidx]:]\n    return h5.root.analysis.sections_start[h5.root.analysis.songs.cols.idx_sections_start[songidx]:\n                                           h5.root.analysis.songs.cols.idx_sections_start[songidx+1]]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_sections_confidence",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_sections_confidence(h5,songidx=0):\n    \"\"\"\n    Get sections confidence array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.analysis.songs.nrows == songidx + 1:\n        return h5.root.analysis.sections_confidence[h5.root.analysis.songs.cols.idx_sections_confidence[songidx]:]\n    return h5.root.analysis.sections_confidence[h5.root.analysis.songs.cols.idx_sections_confidence[songidx]:\n                                                h5.root.analysis.songs.cols.idx_sections_confidence[songidx+1]]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_beats_start",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_beats_start(h5,songidx=0):\n    \"\"\"\n    Get beats start array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.analysis.songs.nrows == songidx + 1:\n        return h5.root.analysis.beats_start[h5.root.analysis.songs.cols.idx_beats_start[songidx]:]\n    return h5.root.analysis.beats_start[h5.root.analysis.songs.cols.idx_beats_start[songidx]:\n                                        h5.root.analysis.songs.cols.idx_beats_start[songidx+1]]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_beats_confidence",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_beats_confidence(h5,songidx=0):\n    \"\"\"\n    Get beats confidence array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.analysis.songs.nrows == songidx + 1:\n        return h5.root.analysis.beats_confidence[h5.root.analysis.songs.cols.idx_beats_confidence[songidx]:]\n    return h5.root.analysis.beats_confidence[h5.root.analysis.songs.cols.idx_beats_confidence[songidx]:\n                                             h5.root.analysis.songs.cols.idx_beats_confidence[songidx+1]]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_bars_start",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_bars_start(h5,songidx=0):\n    \"\"\"\n    Get bars start array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.analysis.songs.nrows == songidx + 1:\n        return h5.root.analysis.bars_start[h5.root.analysis.songs.cols.idx_bars_start[songidx]:]\n    return h5.root.analysis.bars_start[h5.root.analysis.songs.cols.idx_bars_start[songidx]:\n                                       h5.root.analysis.songs.cols.idx_bars_start[songidx+1]]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_bars_confidence",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_bars_confidence(h5,songidx=0):\n    \"\"\"\n    Get bars start array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.analysis.songs.nrows == songidx + 1:\n        return h5.root.analysis.bars_confidence[h5.root.analysis.songs.cols.idx_bars_confidence[songidx]:]\n    return h5.root.analysis.bars_confidence[h5.root.analysis.songs.cols.idx_bars_confidence[songidx]:\n                                            h5.root.analysis.songs.cols.idx_bars_confidence[songidx+1]]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_tatums_start",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_tatums_start(h5,songidx=0):\n    \"\"\"\n    Get tatums start array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.analysis.songs.nrows == songidx + 1:\n        return h5.root.analysis.tatums_start[h5.root.analysis.songs.cols.idx_tatums_start[songidx]:]\n    return h5.root.analysis.tatums_start[h5.root.analysis.songs.cols.idx_tatums_start[songidx]:\n                                         h5.root.analysis.songs.cols.idx_tatums_start[songidx+1]]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_tatums_confidence",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_tatums_confidence(h5,songidx=0):\n    \"\"\"\n    Get tatums confidence array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.analysis.songs.nrows == songidx + 1:\n        return h5.root.analysis.tatums_confidence[h5.root.analysis.songs.cols.idx_tatums_confidence[songidx]:]\n    return h5.root.analysis.tatums_confidence[h5.root.analysis.songs.cols.idx_tatums_confidence[songidx]:\n                                              h5.root.analysis.songs.cols.idx_tatums_confidence[songidx+1]]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_artist_mbtags",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_artist_mbtags(h5,songidx=0):\n    \"\"\"\n    Get artist musicbrainz tag array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.musicbrainz.songs.nrows == songidx + 1:\n        return h5.root.musicbrainz.artist_mbtags[h5.root.musicbrainz.songs.cols.idx_artist_mbtags[songidx]:]\n    return h5.root.musicbrainz.artist_mbtags[h5.root.metadata.songs.cols.idx_artist_mbtags[songidx]:\n                                             h5.root.metadata.songs.cols.idx_artist_mbtags[songidx+1]]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_artist_mbtags_count",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_artist_mbtags_count(h5,songidx=0):\n    \"\"\"\n    Get artist musicbrainz tag count array. Takes care of the proper indexing if we are in aggregate\n    file. By default, return the array for the first song in the h5 file.\n    To get a regular numpy ndarray, cast the result to: numpy.array( )\n    \"\"\"\n    if h5.root.musicbrainz.songs.nrows == songidx + 1:\n        return h5.root.musicbrainz.artist_mbtags_count[h5.root.musicbrainz.songs.cols.idx_artist_mbtags[songidx]:]\n    return h5.root.musicbrainz.artist_mbtags_count[h5.root.metadata.songs.cols.idx_artist_mbtags[songidx]:\n                                                   h5.root.metadata.songs.cols.idx_artist_mbtags[songidx+1]]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "get_year",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_getters",
        "description": "PythonSrc.hdf5_getters",
        "peekOfCode": "def get_year(h5,songidx=0):\n    \"\"\"\n    Get release year from a HDF5 song file, by default the first song in it\n    \"\"\"\n    return h5.root.musicbrainz.songs.cols.year[songidx]",
        "detail": "PythonSrc.hdf5_getters",
        "documentation": {}
    },
    {
        "label": "transfer",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_to_matfile",
        "description": "PythonSrc.hdf5_to_matfile",
        "peekOfCode": "def transfer(h5path,matpath=None,force=False):\n    \"\"\"\n    Transfer an HDF5 song file (.h5) to a matfile (.mat)\n    If there are more than one song in the HDF5 file, each\n    field name gets a number happened: 1, 2, 3, ...., numfiles\n    PARAM\n        h5path  - path to the HDF5 song file\n        matpath - path to the new matfile, same as HDF5 path\n                  with a different extension by default\n        force   - if True and matfile exists, overwrite",
        "detail": "PythonSrc.hdf5_to_matfile",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_to_matfile",
        "description": "PythonSrc.hdf5_to_matfile",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'hdf5_to_matfile.py'\n    print 'Transform a song file in HDF5 format to a matfile'\n    print 'with the same information.'\n    print ' '\n    print 'usage:'\n    print '   python hdf5_to_matfile.py <DIR/FILE>'\n    print 'PARAM'\n    print '   <DIR/FILE>   if a file TR123.h5, creates TR123.mat in the same dir'",
        "detail": "PythonSrc.hdf5_to_matfile",
        "documentation": {}
    },
    {
        "label": "fill_hdf5_from_artist",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "def fill_hdf5_from_artist(h5,artist):\n    \"\"\"\n    Fill an open hdf5 using all content in a artist object\n    from the Echo Nest python API\n    There could be overlap with fill_from_song and fill_from_track,\n    we assume the data is consistent!\n    \"\"\"\n    # get the metadata table, fill it\n    metadata = h5.root.metadata.songs\n    metadata.cols.artist_id[0] = artist.id",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "fill_hdf5_from_song",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "def fill_hdf5_from_song(h5,song):\n    \"\"\"\n    Fill an open hdf5 using all the content in a song object\n    from the Echo Nest python API.\n    Usually, fill_hdf5_from_track() will have been called first.\n    \"\"\"\n    # get the metadata table, fill it\n    metadata = h5.root.metadata.songs\n    metadata.cols.artist_familiarity[0] = song.get_artist_familiarity()\n    metadata.cols.artist_hotttnesss[0] = song.get_artist_hotttnesss()",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "fill_hdf5_from_track",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "def fill_hdf5_from_track(h5,track):\n    \"\"\"\n    Fill an open hdf5 using all the content in a track object\n    from the Echo Nest python API\n    \"\"\"\n    # get the metadata table, fill it\n    metadata = h5.root.metadata.songs\n    #metadata.cols.analyzer_version[0] = track.analyzer_version\n    metadata.cols.artist_name[0] = getattr(track, 'artist', u'').encode('utf-8')\n    metadata.cols.release[0] = getattr(track, 'release', u'').encode('utf-8')",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "fill_hdf5_from_musicbrainz",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "def fill_hdf5_from_musicbrainz(h5,connect):\n    \"\"\"\n    Fill an open hdf5 using the musicbrainz server and data.\n    We assume this code is run after fill_hdf5_from_artist/song\n    because we need artist_mbid, artist_name, release and title\n    INPUT\n       h5        - open song file (append mode)\n       connect   - open pg connection to musicbrainz_db\n    \"\"\"\n    # get info from h5 song file",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "fill_hdf5_aggregate_file",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "def fill_hdf5_aggregate_file(h5,h5_filenames,summaryfile=False):\n    \"\"\"\n    Fill an open hdf5 aggregate file using all the content from all the HDF5 files\n    listed as filenames. These HDF5 files are supposed to be filled already.\n    Usefull to create one big HDF5 file from many, thus improving IO speed.\n    For most of the info, we simply use one row per song.\n    For the arrays (e.g. segment_start) we need the indecies (e.g. idx_segment_start)\n    to know which part of the array belongs to one particular song.\n    If summaryfile=True, we skip arrays (indices all 0)\n    \"\"\"",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "create_song_file",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "def create_song_file(h5filename,title='H5 Song File',force=False,complevel=1):\n    \"\"\"\n    Create a new HDF5 file for a new song.\n    If force=False, refuse to overwrite an existing file\n    Raise a ValueError if it's the case.\n    Other optional param is the H5 file.\n    Setups the groups, each containing a table 'songs' with one row:\n    - metadata\n    - analysis\n    DETAIL",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "create_aggregate_file",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "def create_aggregate_file(h5filename,title='H5 Aggregate File',force=False,expectedrows=1000,complevel=1,\n                          summaryfile=False):\n    \"\"\"\n    Create a new HDF5 file for all songs.\n    It will contains everything that are in regular song files.\n    Tables created empty.\n    If force=False, refuse to overwrite an existing file\n    Raise a ValueError if it's the case.\n    If summaryfile=True, creates a sumary file, i.e. no arrays\n    Other optional param is the H5 file.",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "create_all_arrays",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "def create_all_arrays(h5,expectedrows=1000):\n    \"\"\"\n    Utility functions used by both create_song_file and create_aggregate_files,\n    creates all the EArrays (empty).\n    INPUT\n       h5   - hdf5 file, open with write or append permissions\n              metadata and analysis groups already exist!\n    \"\"\"\n    # group metadata arrays\n    group = h5.root.metadata",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "open_h5_file_read",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "def open_h5_file_read(h5filename):\n    \"\"\"\n    Open an existing H5 in read mode.\n    \"\"\"\n    return tables.openFile(h5filename, mode='r')\ndef open_h5_file_append(h5filename):\n    \"\"\"\n    Open an existing H5 in append mode.\n    \"\"\"\n    return tables.openFile(h5filename, mode='a')",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "open_h5_file_append",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "def open_h5_file_append(h5filename):\n    \"\"\"\n    Open an existing H5 in append mode.\n    \"\"\"\n    return tables.openFile(h5filename, mode='a')\n################################################ MAIN #####################################\ndef die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'hdf5_utils.py'\n    print 'by T. Bertin-Mahieux (2010) Columbia University'",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'hdf5_utils.py'\n    print 'by T. Bertin-Mahieux (2010) Columbia University'\n    print ''\n    print 'should be used as a library, contains functions to create'\n    print 'HDF5 files for the Million Song Dataset project'\n    sys.exit(0)\nif __name__ == '__main__':\n    # help menu",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_SIMILAR_ARTISTS",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_SIMILAR_ARTISTS = 'array of similar artists Echo Nest id'\nARRAY_DESC_ARTIST_TERMS = 'array of terms (Echo Nest tags) for an artist'\nARRAY_DESC_ARTIST_TERMS_FREQ = 'array of term (Echo Nest tags) frequencies for an artist'\nARRAY_DESC_ARTIST_TERMS_WEIGHT = 'array of term (Echo Nest tags) weights for an artist'\nARRAY_DESC_SEGMENTS_START = 'array of start times of segments'\nARRAY_DESC_SEGMENTS_CONFIDENCE = 'array of confidence of segments'\nARRAY_DESC_SEGMENTS_PITCHES = 'array of pitches of segments (chromas)'\nARRAY_DESC_SEGMENTS_TIMBRE = 'array of timbre of segments (MFCC-like)'\nARRAY_DESC_SEGMENTS_LOUDNESS_MAX = 'array of max loudness of segments'\nARRAY_DESC_SEGMENTS_LOUDNESS_MAX_TIME = 'array of max loudness time of segments'",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_ARTIST_TERMS",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_ARTIST_TERMS = 'array of terms (Echo Nest tags) for an artist'\nARRAY_DESC_ARTIST_TERMS_FREQ = 'array of term (Echo Nest tags) frequencies for an artist'\nARRAY_DESC_ARTIST_TERMS_WEIGHT = 'array of term (Echo Nest tags) weights for an artist'\nARRAY_DESC_SEGMENTS_START = 'array of start times of segments'\nARRAY_DESC_SEGMENTS_CONFIDENCE = 'array of confidence of segments'\nARRAY_DESC_SEGMENTS_PITCHES = 'array of pitches of segments (chromas)'\nARRAY_DESC_SEGMENTS_TIMBRE = 'array of timbre of segments (MFCC-like)'\nARRAY_DESC_SEGMENTS_LOUDNESS_MAX = 'array of max loudness of segments'\nARRAY_DESC_SEGMENTS_LOUDNESS_MAX_TIME = 'array of max loudness time of segments'\nARRAY_DESC_SEGMENTS_LOUDNESS_START = 'array of loudness of segments at start time'",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_ARTIST_TERMS_FREQ",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_ARTIST_TERMS_FREQ = 'array of term (Echo Nest tags) frequencies for an artist'\nARRAY_DESC_ARTIST_TERMS_WEIGHT = 'array of term (Echo Nest tags) weights for an artist'\nARRAY_DESC_SEGMENTS_START = 'array of start times of segments'\nARRAY_DESC_SEGMENTS_CONFIDENCE = 'array of confidence of segments'\nARRAY_DESC_SEGMENTS_PITCHES = 'array of pitches of segments (chromas)'\nARRAY_DESC_SEGMENTS_TIMBRE = 'array of timbre of segments (MFCC-like)'\nARRAY_DESC_SEGMENTS_LOUDNESS_MAX = 'array of max loudness of segments'\nARRAY_DESC_SEGMENTS_LOUDNESS_MAX_TIME = 'array of max loudness time of segments'\nARRAY_DESC_SEGMENTS_LOUDNESS_START = 'array of loudness of segments at start time'\nARRAY_DESC_SECTIONS_START = 'array of start times of sections'",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_ARTIST_TERMS_WEIGHT",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_ARTIST_TERMS_WEIGHT = 'array of term (Echo Nest tags) weights for an artist'\nARRAY_DESC_SEGMENTS_START = 'array of start times of segments'\nARRAY_DESC_SEGMENTS_CONFIDENCE = 'array of confidence of segments'\nARRAY_DESC_SEGMENTS_PITCHES = 'array of pitches of segments (chromas)'\nARRAY_DESC_SEGMENTS_TIMBRE = 'array of timbre of segments (MFCC-like)'\nARRAY_DESC_SEGMENTS_LOUDNESS_MAX = 'array of max loudness of segments'\nARRAY_DESC_SEGMENTS_LOUDNESS_MAX_TIME = 'array of max loudness time of segments'\nARRAY_DESC_SEGMENTS_LOUDNESS_START = 'array of loudness of segments at start time'\nARRAY_DESC_SECTIONS_START = 'array of start times of sections'\nARRAY_DESC_SECTIONS_CONFIDENCE = 'array of confidence of sections'",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_SEGMENTS_START",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_SEGMENTS_START = 'array of start times of segments'\nARRAY_DESC_SEGMENTS_CONFIDENCE = 'array of confidence of segments'\nARRAY_DESC_SEGMENTS_PITCHES = 'array of pitches of segments (chromas)'\nARRAY_DESC_SEGMENTS_TIMBRE = 'array of timbre of segments (MFCC-like)'\nARRAY_DESC_SEGMENTS_LOUDNESS_MAX = 'array of max loudness of segments'\nARRAY_DESC_SEGMENTS_LOUDNESS_MAX_TIME = 'array of max loudness time of segments'\nARRAY_DESC_SEGMENTS_LOUDNESS_START = 'array of loudness of segments at start time'\nARRAY_DESC_SECTIONS_START = 'array of start times of sections'\nARRAY_DESC_SECTIONS_CONFIDENCE = 'array of confidence of sections'\nARRAY_DESC_BEATS_START = 'array of start times of beats'",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_SEGMENTS_CONFIDENCE",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_SEGMENTS_CONFIDENCE = 'array of confidence of segments'\nARRAY_DESC_SEGMENTS_PITCHES = 'array of pitches of segments (chromas)'\nARRAY_DESC_SEGMENTS_TIMBRE = 'array of timbre of segments (MFCC-like)'\nARRAY_DESC_SEGMENTS_LOUDNESS_MAX = 'array of max loudness of segments'\nARRAY_DESC_SEGMENTS_LOUDNESS_MAX_TIME = 'array of max loudness time of segments'\nARRAY_DESC_SEGMENTS_LOUDNESS_START = 'array of loudness of segments at start time'\nARRAY_DESC_SECTIONS_START = 'array of start times of sections'\nARRAY_DESC_SECTIONS_CONFIDENCE = 'array of confidence of sections'\nARRAY_DESC_BEATS_START = 'array of start times of beats'\nARRAY_DESC_BEATS_CONFIDENCE = 'array of confidence of sections'",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_SEGMENTS_PITCHES",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_SEGMENTS_PITCHES = 'array of pitches of segments (chromas)'\nARRAY_DESC_SEGMENTS_TIMBRE = 'array of timbre of segments (MFCC-like)'\nARRAY_DESC_SEGMENTS_LOUDNESS_MAX = 'array of max loudness of segments'\nARRAY_DESC_SEGMENTS_LOUDNESS_MAX_TIME = 'array of max loudness time of segments'\nARRAY_DESC_SEGMENTS_LOUDNESS_START = 'array of loudness of segments at start time'\nARRAY_DESC_SECTIONS_START = 'array of start times of sections'\nARRAY_DESC_SECTIONS_CONFIDENCE = 'array of confidence of sections'\nARRAY_DESC_BEATS_START = 'array of start times of beats'\nARRAY_DESC_BEATS_CONFIDENCE = 'array of confidence of sections'\nARRAY_DESC_BARS_START = 'array of start times of bars'",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_SEGMENTS_TIMBRE",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_SEGMENTS_TIMBRE = 'array of timbre of segments (MFCC-like)'\nARRAY_DESC_SEGMENTS_LOUDNESS_MAX = 'array of max loudness of segments'\nARRAY_DESC_SEGMENTS_LOUDNESS_MAX_TIME = 'array of max loudness time of segments'\nARRAY_DESC_SEGMENTS_LOUDNESS_START = 'array of loudness of segments at start time'\nARRAY_DESC_SECTIONS_START = 'array of start times of sections'\nARRAY_DESC_SECTIONS_CONFIDENCE = 'array of confidence of sections'\nARRAY_DESC_BEATS_START = 'array of start times of beats'\nARRAY_DESC_BEATS_CONFIDENCE = 'array of confidence of sections'\nARRAY_DESC_BARS_START = 'array of start times of bars'\nARRAY_DESC_BARS_CONFIDENCE = 'array of confidence of bars'",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_SEGMENTS_LOUDNESS_MAX",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_SEGMENTS_LOUDNESS_MAX = 'array of max loudness of segments'\nARRAY_DESC_SEGMENTS_LOUDNESS_MAX_TIME = 'array of max loudness time of segments'\nARRAY_DESC_SEGMENTS_LOUDNESS_START = 'array of loudness of segments at start time'\nARRAY_DESC_SECTIONS_START = 'array of start times of sections'\nARRAY_DESC_SECTIONS_CONFIDENCE = 'array of confidence of sections'\nARRAY_DESC_BEATS_START = 'array of start times of beats'\nARRAY_DESC_BEATS_CONFIDENCE = 'array of confidence of sections'\nARRAY_DESC_BARS_START = 'array of start times of bars'\nARRAY_DESC_BARS_CONFIDENCE = 'array of confidence of bars'\nARRAY_DESC_TATUMS_START = 'array of start times of tatums'",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_SEGMENTS_LOUDNESS_MAX_TIME",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_SEGMENTS_LOUDNESS_MAX_TIME = 'array of max loudness time of segments'\nARRAY_DESC_SEGMENTS_LOUDNESS_START = 'array of loudness of segments at start time'\nARRAY_DESC_SECTIONS_START = 'array of start times of sections'\nARRAY_DESC_SECTIONS_CONFIDENCE = 'array of confidence of sections'\nARRAY_DESC_BEATS_START = 'array of start times of beats'\nARRAY_DESC_BEATS_CONFIDENCE = 'array of confidence of sections'\nARRAY_DESC_BARS_START = 'array of start times of bars'\nARRAY_DESC_BARS_CONFIDENCE = 'array of confidence of bars'\nARRAY_DESC_TATUMS_START = 'array of start times of tatums'\nARRAY_DESC_TATUMS_CONFIDENCE = 'array of confidence of tatums'",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_SEGMENTS_LOUDNESS_START",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_SEGMENTS_LOUDNESS_START = 'array of loudness of segments at start time'\nARRAY_DESC_SECTIONS_START = 'array of start times of sections'\nARRAY_DESC_SECTIONS_CONFIDENCE = 'array of confidence of sections'\nARRAY_DESC_BEATS_START = 'array of start times of beats'\nARRAY_DESC_BEATS_CONFIDENCE = 'array of confidence of sections'\nARRAY_DESC_BARS_START = 'array of start times of bars'\nARRAY_DESC_BARS_CONFIDENCE = 'array of confidence of bars'\nARRAY_DESC_TATUMS_START = 'array of start times of tatums'\nARRAY_DESC_TATUMS_CONFIDENCE = 'array of confidence of tatums'\nARRAY_DESC_ARTIST_MBTAGS = 'array of tags from MusicBrainz for an artist'",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_SECTIONS_START",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_SECTIONS_START = 'array of start times of sections'\nARRAY_DESC_SECTIONS_CONFIDENCE = 'array of confidence of sections'\nARRAY_DESC_BEATS_START = 'array of start times of beats'\nARRAY_DESC_BEATS_CONFIDENCE = 'array of confidence of sections'\nARRAY_DESC_BARS_START = 'array of start times of bars'\nARRAY_DESC_BARS_CONFIDENCE = 'array of confidence of bars'\nARRAY_DESC_TATUMS_START = 'array of start times of tatums'\nARRAY_DESC_TATUMS_CONFIDENCE = 'array of confidence of tatums'\nARRAY_DESC_ARTIST_MBTAGS = 'array of tags from MusicBrainz for an artist'\nARRAY_DESC_ARTIST_MBTAGS_COUNT = 'array of tag counts from MusicBrainz for an artist'",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_SECTIONS_CONFIDENCE",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_SECTIONS_CONFIDENCE = 'array of confidence of sections'\nARRAY_DESC_BEATS_START = 'array of start times of beats'\nARRAY_DESC_BEATS_CONFIDENCE = 'array of confidence of sections'\nARRAY_DESC_BARS_START = 'array of start times of bars'\nARRAY_DESC_BARS_CONFIDENCE = 'array of confidence of bars'\nARRAY_DESC_TATUMS_START = 'array of start times of tatums'\nARRAY_DESC_TATUMS_CONFIDENCE = 'array of confidence of tatums'\nARRAY_DESC_ARTIST_MBTAGS = 'array of tags from MusicBrainz for an artist'\nARRAY_DESC_ARTIST_MBTAGS_COUNT = 'array of tag counts from MusicBrainz for an artist'\ndef fill_hdf5_from_artist(h5,artist):",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_BEATS_START",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_BEATS_START = 'array of start times of beats'\nARRAY_DESC_BEATS_CONFIDENCE = 'array of confidence of sections'\nARRAY_DESC_BARS_START = 'array of start times of bars'\nARRAY_DESC_BARS_CONFIDENCE = 'array of confidence of bars'\nARRAY_DESC_TATUMS_START = 'array of start times of tatums'\nARRAY_DESC_TATUMS_CONFIDENCE = 'array of confidence of tatums'\nARRAY_DESC_ARTIST_MBTAGS = 'array of tags from MusicBrainz for an artist'\nARRAY_DESC_ARTIST_MBTAGS_COUNT = 'array of tag counts from MusicBrainz for an artist'\ndef fill_hdf5_from_artist(h5,artist):\n    \"\"\"",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_BEATS_CONFIDENCE",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_BEATS_CONFIDENCE = 'array of confidence of sections'\nARRAY_DESC_BARS_START = 'array of start times of bars'\nARRAY_DESC_BARS_CONFIDENCE = 'array of confidence of bars'\nARRAY_DESC_TATUMS_START = 'array of start times of tatums'\nARRAY_DESC_TATUMS_CONFIDENCE = 'array of confidence of tatums'\nARRAY_DESC_ARTIST_MBTAGS = 'array of tags from MusicBrainz for an artist'\nARRAY_DESC_ARTIST_MBTAGS_COUNT = 'array of tag counts from MusicBrainz for an artist'\ndef fill_hdf5_from_artist(h5,artist):\n    \"\"\"\n    Fill an open hdf5 using all content in a artist object",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_BARS_START",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_BARS_START = 'array of start times of bars'\nARRAY_DESC_BARS_CONFIDENCE = 'array of confidence of bars'\nARRAY_DESC_TATUMS_START = 'array of start times of tatums'\nARRAY_DESC_TATUMS_CONFIDENCE = 'array of confidence of tatums'\nARRAY_DESC_ARTIST_MBTAGS = 'array of tags from MusicBrainz for an artist'\nARRAY_DESC_ARTIST_MBTAGS_COUNT = 'array of tag counts from MusicBrainz for an artist'\ndef fill_hdf5_from_artist(h5,artist):\n    \"\"\"\n    Fill an open hdf5 using all content in a artist object\n    from the Echo Nest python API",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_BARS_CONFIDENCE",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_BARS_CONFIDENCE = 'array of confidence of bars'\nARRAY_DESC_TATUMS_START = 'array of start times of tatums'\nARRAY_DESC_TATUMS_CONFIDENCE = 'array of confidence of tatums'\nARRAY_DESC_ARTIST_MBTAGS = 'array of tags from MusicBrainz for an artist'\nARRAY_DESC_ARTIST_MBTAGS_COUNT = 'array of tag counts from MusicBrainz for an artist'\ndef fill_hdf5_from_artist(h5,artist):\n    \"\"\"\n    Fill an open hdf5 using all content in a artist object\n    from the Echo Nest python API\n    There could be overlap with fill_from_song and fill_from_track,",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_TATUMS_START",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_TATUMS_START = 'array of start times of tatums'\nARRAY_DESC_TATUMS_CONFIDENCE = 'array of confidence of tatums'\nARRAY_DESC_ARTIST_MBTAGS = 'array of tags from MusicBrainz for an artist'\nARRAY_DESC_ARTIST_MBTAGS_COUNT = 'array of tag counts from MusicBrainz for an artist'\ndef fill_hdf5_from_artist(h5,artist):\n    \"\"\"\n    Fill an open hdf5 using all content in a artist object\n    from the Echo Nest python API\n    There could be overlap with fill_from_song and fill_from_track,\n    we assume the data is consistent!",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_TATUMS_CONFIDENCE",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_TATUMS_CONFIDENCE = 'array of confidence of tatums'\nARRAY_DESC_ARTIST_MBTAGS = 'array of tags from MusicBrainz for an artist'\nARRAY_DESC_ARTIST_MBTAGS_COUNT = 'array of tag counts from MusicBrainz for an artist'\ndef fill_hdf5_from_artist(h5,artist):\n    \"\"\"\n    Fill an open hdf5 using all content in a artist object\n    from the Echo Nest python API\n    There could be overlap with fill_from_song and fill_from_track,\n    we assume the data is consistent!\n    \"\"\"",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_ARTIST_MBTAGS",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_ARTIST_MBTAGS = 'array of tags from MusicBrainz for an artist'\nARRAY_DESC_ARTIST_MBTAGS_COUNT = 'array of tag counts from MusicBrainz for an artist'\ndef fill_hdf5_from_artist(h5,artist):\n    \"\"\"\n    Fill an open hdf5 using all content in a artist object\n    from the Echo Nest python API\n    There could be overlap with fill_from_song and fill_from_track,\n    we assume the data is consistent!\n    \"\"\"\n    # get the metadata table, fill it",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "ARRAY_DESC_ARTIST_MBTAGS_COUNT",
        "kind": 5,
        "importPath": "PythonSrc.hdf5_utils",
        "description": "PythonSrc.hdf5_utils",
        "peekOfCode": "ARRAY_DESC_ARTIST_MBTAGS_COUNT = 'array of tag counts from MusicBrainz for an artist'\ndef fill_hdf5_from_artist(h5,artist):\n    \"\"\"\n    Fill an open hdf5 using all content in a artist object\n    from the Echo Nest python API\n    There could be overlap with fill_from_song and fill_from_track,\n    we assume the data is consistent!\n    \"\"\"\n    # get the metadata table, fill it\n    metadata = h5.root.metadata.songs",
        "detail": "PythonSrc.hdf5_utils",
        "documentation": {}
    },
    {
        "label": "get_all_files",
        "kind": 2,
        "importPath": "PythonSrc.utils",
        "description": "PythonSrc.utils",
        "peekOfCode": "def get_all_files(basedir,ext='.h5') :\n    \"\"\"\n    From a root directory, go through all subdirectories\n    and find all files with the given extension.\n    Return all absolute paths in a list.\n    \"\"\"\n    allfiles = []\n    for root, dirs, files in os.walk(basedir):\n        files = glob.glob(os.path.join(root,'*'+ext))\n        for f in files :",
        "detail": "PythonSrc.utils",
        "documentation": {}
    },
    {
        "label": "KeyboardInterruptError",
        "kind": 6,
        "importPath": "Tasks_Demos.ArtistRecognition.process_test_set",
        "description": "Tasks_Demos.ArtistRecognition.process_test_set",
        "peekOfCode": "class KeyboardInterruptError(Exception):pass\ndef fullpath_from_trackid(maindir,trackid):\n    \"\"\" Creates proper file paths for song files \"\"\"\n    p = os.path.join(maindir,trackid[2])\n    p = os.path.join(p,trackid[3])\n    p = os.path.join(p,trackid[4])\n    p = os.path.join(p,trackid+'.h5')\n    return str(p)\ndef get_all_files(basedir,ext='.h5'):\n    \"\"\"",
        "detail": "Tasks_Demos.ArtistRecognition.process_test_set",
        "documentation": {}
    },
    {
        "label": "fullpath_from_trackid",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.process_test_set",
        "description": "Tasks_Demos.ArtistRecognition.process_test_set",
        "peekOfCode": "def fullpath_from_trackid(maindir,trackid):\n    \"\"\" Creates proper file paths for song files \"\"\"\n    p = os.path.join(maindir,trackid[2])\n    p = os.path.join(p,trackid[3])\n    p = os.path.join(p,trackid[4])\n    p = os.path.join(p,trackid+'.h5')\n    return str(p)\ndef get_all_files(basedir,ext='.h5'):\n    \"\"\"\n    From a root directory, go through all subdirectories",
        "detail": "Tasks_Demos.ArtistRecognition.process_test_set",
        "documentation": {}
    },
    {
        "label": "get_all_files",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.process_test_set",
        "description": "Tasks_Demos.ArtistRecognition.process_test_set",
        "peekOfCode": "def get_all_files(basedir,ext='.h5'):\n    \"\"\"\n    From a root directory, go through all subdirectories\n    and find all files with the given extension.\n    Return all absolute paths in a list.\n    \"\"\"\n    allfiles = []\n    apply_to_all_files(basedir,func=lambda x: allfiles.append(x),ext=ext)\n    return allfiles\ndef apply_to_all_files(basedir,func=lambda x: x,ext='.h5'):",
        "detail": "Tasks_Demos.ArtistRecognition.process_test_set",
        "documentation": {}
    },
    {
        "label": "apply_to_all_files",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.process_test_set",
        "description": "Tasks_Demos.ArtistRecognition.process_test_set",
        "peekOfCode": "def apply_to_all_files(basedir,func=lambda x: x,ext='.h5'):\n    \"\"\"\n    From a root directory, go through all subdirectories\n    and find all files with the given extension.\n    Apply the given function func\n    If no function passed, does nothing and counts file\n    Return number of files\n    \"\"\"\n    cnt = 0\n    for root, dirs, files in os.walk(basedir):",
        "detail": "Tasks_Demos.ArtistRecognition.process_test_set",
        "documentation": {}
    },
    {
        "label": "compute_features",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.process_test_set",
        "description": "Tasks_Demos.ArtistRecognition.process_test_set",
        "peekOfCode": "def compute_features(h5):\n    \"\"\"\n    Get the same features than during training\n    \"\"\"\n    return TRAIN.compute_features(h5)\ndef do_prediction(processed_feats,kd,h5model,K=1):\n    \"\"\"\n    Receive processed features from test set, apply KNN,\n    return an actual predicted year (float)\n    INPUT",
        "detail": "Tasks_Demos.ArtistRecognition.process_test_set",
        "documentation": {}
    },
    {
        "label": "do_prediction",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.process_test_set",
        "description": "Tasks_Demos.ArtistRecognition.process_test_set",
        "peekOfCode": "def do_prediction(processed_feats,kd,h5model,K=1):\n    \"\"\"\n    Receive processed features from test set, apply KNN,\n    return an actual predicted year (float)\n    INPUT\n       processed_feats - extracted from a test song\n                    kd - ANN kdtree on top of model\n               h5model - open h5 file with data.feats and data.year\n                     K - K-nn parameter\n    \"\"\"",
        "detail": "Tasks_Demos.ArtistRecognition.process_test_set",
        "documentation": {}
    },
    {
        "label": "process_filelist_test",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.process_test_set",
        "description": "Tasks_Demos.ArtistRecognition.process_test_set",
        "peekOfCode": "def process_filelist_test(filelist=None,model=None,tmpfilename=None,K=1):\n    \"\"\"\n    Main function, process all files in the list (as long as their track_id\n    is not in testsongs)\n    INPUT\n       filelist     - a list of song files\n       model        - h5 file containing feats and artist_id for all train songs\n       tmpfilename  - where to save our processed features\n       K            - K-nn parameter (default=1)\n    \"\"\"",
        "detail": "Tasks_Demos.ArtistRecognition.process_test_set",
        "documentation": {}
    },
    {
        "label": "process_filelist_test_wrapper",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.process_test_set",
        "description": "Tasks_Demos.ArtistRecognition.process_test_set",
        "peekOfCode": "def process_filelist_test_wrapper(args):\n    \"\"\" wrapper function for multiprocessor, calls process_filelist_test \"\"\"\n    try:\n        process_filelist_test(**args)\n    except KeyboardInterrupt:\n        raise KeyboardInterruptError()\ndef process_filelist_test_main_pass(nthreads,model,testsongs,K):\n    \"\"\"\n    Do the main walk through the data, deals with the threads,\n    creates the tmpfiles.",
        "detail": "Tasks_Demos.ArtistRecognition.process_test_set",
        "documentation": {}
    },
    {
        "label": "process_filelist_test_main_pass",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.process_test_set",
        "description": "Tasks_Demos.ArtistRecognition.process_test_set",
        "peekOfCode": "def process_filelist_test_main_pass(nthreads,model,testsongs,K):\n    \"\"\"\n    Do the main walk through the data, deals with the threads,\n    creates the tmpfiles.\n    INPUT\n      - nthreads     - number of threads to use\n      - model        - h5 files containing feats and artist_id for all train songs\n      - testsongs    - set of songs to ignore\n      - K            - K-nn parameter\n    RETURN",
        "detail": "Tasks_Demos.ArtistRecognition.process_test_set",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.process_test_set",
        "description": "Tasks_Demos.ArtistRecognition.process_test_set",
        "peekOfCode": "def test(nthreads,model,testsongs,K):\n    \"\"\"\n    Main function to do the training\n    Do the main pass with the number of given threads.\n    Then, reads the tmp files, creates the main output, delete the tmpfiles.\n    INPUT\n      - nthreads     - number of threads to use\n      - model        - h5 files containing feats and artist_id for all train songs\n      - testsongs    - set of songs to ignore\n      - K            - K-nn parameter",
        "detail": "Tasks_Demos.ArtistRecognition.process_test_set",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.process_test_set",
        "description": "Tasks_Demos.ArtistRecognition.process_test_set",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'process_test_set.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print '      tb2332@columbia.edu'\n    print 'Code to perform artist recognition on the Million Song Dataset.'\n    print 'This performs the evaluation of a trained KNN model.'\n    print 'REQUIRES ANN LIBRARY and its python wrapper.'\n    print 'USAGE:'\n    print '  python process_test_set.py [FLAGS] <MSD_DIR> <model> <testsongs> <tmdb>'",
        "detail": "Tasks_Demos.ArtistRecognition.process_test_set",
        "documentation": {}
    },
    {
        "label": "KeyboardInterruptError",
        "kind": 6,
        "importPath": "Tasks_Demos.ArtistRecognition.process_train_set",
        "description": "Tasks_Demos.ArtistRecognition.process_train_set",
        "peekOfCode": "class KeyboardInterruptError(Exception):pass\ndef fullpath_from_trackid(maindir,trackid):\n    \"\"\" Creates proper file paths for song files \"\"\"\n    p = os.path.join(maindir,trackid[2])\n    p = os.path.join(p,trackid[3])\n    p = os.path.join(p,trackid[4])\n    p = os.path.join(p,trackid+'.h5')\n    return str(p)\ndef get_all_files(basedir,ext='.h5'):\n    \"\"\"",
        "detail": "Tasks_Demos.ArtistRecognition.process_train_set",
        "documentation": {}
    },
    {
        "label": "fullpath_from_trackid",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.process_train_set",
        "description": "Tasks_Demos.ArtistRecognition.process_train_set",
        "peekOfCode": "def fullpath_from_trackid(maindir,trackid):\n    \"\"\" Creates proper file paths for song files \"\"\"\n    p = os.path.join(maindir,trackid[2])\n    p = os.path.join(p,trackid[3])\n    p = os.path.join(p,trackid[4])\n    p = os.path.join(p,trackid+'.h5')\n    return str(p)\ndef get_all_files(basedir,ext='.h5'):\n    \"\"\"\n    From a root directory, go through all subdirectories",
        "detail": "Tasks_Demos.ArtistRecognition.process_train_set",
        "documentation": {}
    },
    {
        "label": "get_all_files",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.process_train_set",
        "description": "Tasks_Demos.ArtistRecognition.process_train_set",
        "peekOfCode": "def get_all_files(basedir,ext='.h5'):\n    \"\"\"\n    From a root directory, go through all subdirectories\n    and find all files with the given extension.\n    Return all absolute paths in a list.\n    \"\"\"\n    allfiles = []\n    apply_to_all_files(basedir,func=lambda x: allfiles.append(x),ext=ext)\n    return allfiles\ndef apply_to_all_files(basedir,func=lambda x: x,ext='.h5'):",
        "detail": "Tasks_Demos.ArtistRecognition.process_train_set",
        "documentation": {}
    },
    {
        "label": "apply_to_all_files",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.process_train_set",
        "description": "Tasks_Demos.ArtistRecognition.process_train_set",
        "peekOfCode": "def apply_to_all_files(basedir,func=lambda x: x,ext='.h5'):\n    \"\"\"\n    From a root directory, go through all subdirectories\n    and find all files with the given extension.\n    Apply the given function func\n    If no function passed, does nothing and counts file\n    Return number of files\n    \"\"\"\n    cnt = 0\n    for root, dirs, files in os.walk(basedir):",
        "detail": "Tasks_Demos.ArtistRecognition.process_train_set",
        "documentation": {}
    },
    {
        "label": "compute_features",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.process_train_set",
        "description": "Tasks_Demos.ArtistRecognition.process_train_set",
        "peekOfCode": "def compute_features(h5):\n    \"\"\"\n    From an open HDF5 song file, extract average and covariance of the\n    timbre vectors.\n    RETURN 1x90 vector or None if there is a problem\n    \"\"\"\n    feats = GETTERS.get_segments_timbre(h5).T\n    # features length\n    ftlen = feats.shape[1]\n    ndim = feats.shape[0]",
        "detail": "Tasks_Demos.ArtistRecognition.process_train_set",
        "documentation": {}
    },
    {
        "label": "process_filelist_train",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.process_train_set",
        "description": "Tasks_Demos.ArtistRecognition.process_train_set",
        "peekOfCode": "def process_filelist_train(filelist=None,testsongs=None,tmpfilename=None):\n    \"\"\"\n    Main function, process all files in the list (as long as their track_id\n    is not in testsongs)\n    INPUT\n       filelist     - a list of song files\n       testsongs    - set of track ID that we should not use\n       tmpfilename  - where to save our processed features\n    \"\"\"\n    # sanity check",
        "detail": "Tasks_Demos.ArtistRecognition.process_train_set",
        "documentation": {}
    },
    {
        "label": "process_filelist_train_wrapper",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.process_train_set",
        "description": "Tasks_Demos.ArtistRecognition.process_train_set",
        "peekOfCode": "def process_filelist_train_wrapper(args):\n    \"\"\" wrapper function for multiprocessor, calls process_filelist_train \"\"\"\n    try:\n        process_filelist_train(**args)\n    except KeyboardInterrupt:\n        raise KeyboardInterruptError()\ndef process_filelist_train_main_pass(nthreads,maindir,testsongs,trainsongs=None):\n    \"\"\"\n    Do the main walk through the data, deals with the threads,\n    creates the tmpfiles.",
        "detail": "Tasks_Demos.ArtistRecognition.process_train_set",
        "documentation": {}
    },
    {
        "label": "process_filelist_train_main_pass",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.process_train_set",
        "description": "Tasks_Demos.ArtistRecognition.process_train_set",
        "peekOfCode": "def process_filelist_train_main_pass(nthreads,maindir,testsongs,trainsongs=None):\n    \"\"\"\n    Do the main walk through the data, deals with the threads,\n    creates the tmpfiles.\n    INPUT\n      - nthreads     - number of threads to use\n      - maindir      - dir of the MSD, wehre to find song files\n      - testsongs    - set of songs to ignore\n      - trainsongs   - list of files to use for training (faster!)\n    RETURN",
        "detail": "Tasks_Demos.ArtistRecognition.process_train_set",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.process_train_set",
        "description": "Tasks_Demos.ArtistRecognition.process_train_set",
        "peekOfCode": "def train(nthreads,maindir,output,testsongs,trainsongs=None):\n    \"\"\"\n    Main function to do the training\n    Do the main pass with the number of given threads.\n    Then, reads the tmp files, creates the main output, delete the tmpfiles.\n    INPUT\n      - nthreads     - number of threads to use\n      - maindir      - dir of the MSD, wehre to find song files\n      - output       - main model, contains everything to perform KNN\n      - testsongs    - set of songs to ignore",
        "detail": "Tasks_Demos.ArtistRecognition.process_train_set",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.process_train_set",
        "description": "Tasks_Demos.ArtistRecognition.process_train_set",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'process_train_set.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print '      tb2332@columbia.edu'\n    print 'Code to perform artist recognition on the Million Song Dataset.'\n    print 'This performs the training of the KNN model.'\n    print 'USAGE:'\n    print '  python process_train_set.py [FLAGS] <MSD_DIR> <testsongs> <tmdb> <output>'\n    print 'PARAMS:'",
        "detail": "Tasks_Demos.ArtistRecognition.process_train_set",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.split_train_test",
        "description": "Tasks_Demos.ArtistRecognition.split_train_test",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'split_train_test.py'\n    print '  by T. Bertin-Mahieux (2010) Columbia University'\n    print '     tb2332@columbia.edu'\n    print 'GOAL'\n    print '  Split the list of songs into train and test for artist recognition.'\n    print '  We only consider artists with at least 20 songs.'\n    print '  The training set consists of 15 songs from each of these artists.'\n    print 'USAGE'",
        "detail": "Tasks_Demos.ArtistRecognition.split_train_test",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.ArtistRecognition.split_train_test_unbalanced",
        "description": "Tasks_Demos.ArtistRecognition.split_train_test_unbalanced",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'split_train_test_unbalanced.py'\n    print '  by T. Bertin-Mahieux (2010) Columbia University'\n    print '     tb2332@columbia.edu'\n    print 'GOAL'\n    print '  Split the list of songs into train and test for artist recognition.'\n    print '  We only consider artists with at least 20 songs.'\n    print '  The training set consists of 2/3 of all songs from each of these artists.'\n    print 'USAGE'",
        "detail": "Tasks_Demos.ArtistRecognition.split_train_test_unbalanced",
        "documentation": {}
    },
    {
        "label": "get_btchromas",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "description": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "peekOfCode": "def get_btchromas(h5):\n    \"\"\"\n    Get beat-aligned chroma from a song file of the Million Song Dataset\n    INPUT:\n       h5          - filename or open h5 file\n    RETURN:\n       btchromas   - beat-aligned chromas, one beat per column\n                     or None if something went wrong (e.g. no beats)\n    \"\"\"\n    # if string, open and get chromas, if h5, get chromas",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "documentation": {}
    },
    {
        "label": "get_btloudnessmax",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "description": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "peekOfCode": "def get_btloudnessmax(h5):\n    \"\"\"\n    Get beat-aligned loudness max from a song file of the Million Song Dataset\n    INPUT:\n       h5             - filename or open h5 file\n    RETURN:\n       btloudnessmax  - beat-aligned loudness max, one beat per column\n                        or None if something went wrong (e.g. no beats)\n    \"\"\"\n    # if string, open and get max loudness, if h5, get max loudness",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "documentation": {}
    },
    {
        "label": "align_feats",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "description": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "peekOfCode": "def align_feats(feats, segstarts, btstarts, duration):\n    \"\"\"\n    MAIN FUNCTION: aligned whatever matrix of features is passed,\n    one column per segment, and interpolate them to get features\n    per beat.\n    Note that btstarts could be anything, e.g. bar starts\n    INPUT\n       feats      - matrix of features, one column per segment\n       segstarts  - segments starts in seconds,\n                    dim must match feats # cols (flatten ndarray)",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "documentation": {}
    },
    {
        "label": "get_time_warp_matrix",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "description": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "peekOfCode": "def get_time_warp_matrix(segstart, btstart, duration):\n    \"\"\"\n    Used by create_beat_synchro_chromagram\n    Returns a matrix (#beats,#segs)\n    #segs should be larger than #beats, i.e. many events or segs\n    happen in one beat.\n    THIS FUNCTION WAS ORIGINALLY CREATED BY RON J. WEISS (Columbia/NYU/Google)\n    \"\"\"\n    # length of beats and segments in seconds\n    # result for track: 'TR0002Q11C3FA8332D'",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "documentation": {}
    },
    {
        "label": "idB",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "description": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "peekOfCode": "def idB(loudness_array):\n    \"\"\"\n    Reverse the Echo Nest loudness dB features.\n    'loudness_array' can be pretty any numpy object:\n    one value or an array\n    Inspired by D. Ellis MATLAB code\n    \"\"\"\n    return np.power(10., loudness_array / 20.)\ndef dB(inv_loudness_array):\n    \"\"\"",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "documentation": {}
    },
    {
        "label": "dB",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "description": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "peekOfCode": "def dB(inv_loudness_array):\n    \"\"\"\n    Put loudness back in dB\n    \"\"\"\n    return np.log10(inv_loudness_array) * 20.\ndef die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'beat_aligned_feats.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print '      tb2332@columbia.edu'",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "description": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'beat_aligned_feats.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print '      tb2332@columbia.edu'\n    print ''\n    print 'This code is intended to be used as a library.'\n    print 'For debugging purposes, you can launch:'\n    print '   python beat_aligned_feats.py <SONG FILENAME>'\n    sys.exit(0)",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.beat_aligned_feats",
        "documentation": {}
    },
    {
        "label": "KeyboardInterruptError",
        "kind": 6,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "description": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "peekOfCode": "class KeyboardInterruptError(Exception):\n    pass\n# for multiprocessing\ndef create_fill_one_partial_db_wrapper(args):\n    \"\"\" wrapper function for multiprocessor, calls run_steps \"\"\"\n    try:\n        create_fill_one_partial_db(**args)\n    except KeyboardInterrupt:\n        raise KeyboardInterruptError()\ndef die_with_usage():",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "documentation": {}
    },
    {
        "label": "get_all_files",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "description": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "peekOfCode": "def get_all_files(basedir,ext='.h5') :\n    \"\"\"\n    From a root directory, go through all subdirectories\n    and find all files with the given extension.\n    Return all absolute paths in a list.\n    \"\"\"\n    allfiles = set()\n    for root, dirs, files in os.walk(basedir):\n        files = glob.glob(os.path.join(root,'*'+ext))\n        for f in files :",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "documentation": {}
    },
    {
        "label": "aggregate_dbs",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "description": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "peekOfCode": "def aggregate_dbs(conn, tmpdbpath):\n    \"\"\"\n    Aggregate the data from a filled, temporary database,\n    to a main one already initialized.\n    \"\"\"\n    try:\n        CHT.__name__\n    except NameError:\n        #print 'We import cover hash table'\n        import cover_hash_table as CHT",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "documentation": {}
    },
    {
        "label": "my_vacuum",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "description": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "peekOfCode": "def my_vacuum(dbpathnew, dbpathold):\n    \"\"\"\n    My own vacuum function on cover_hash_table.\n    It works by copying and is slow!\n    My main use, transform the page size to 4096\n    Here because of its use of 'aggregate_dbs'\n    \"\"\"\n    if os.path.exists(dbpathnew):\n        print 'ERROR: %s already exists.' % dbpathnew\n        return",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "documentation": {}
    },
    {
        "label": "create_fill_one_partial_db",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "description": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "peekOfCode": "def create_fill_one_partial_db(filelist=None, outputdb=None):\n    \"\"\"\n    This is the main function called by each process\n    \"\"\"\n    # assert we have the params\n    assert (not filelist is None) and (not outputdb is None), \"internal arg passing error...!\"\n    # must be imported there... maybe... because of local num_hash_tables count\n    import cover_hash_table as CHT\n    # other imports\n    import quick_query_test as QQT # should be replaced in the future",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "documentation": {}
    },
    {
        "label": "create_fill_one_partial_db_wrapper",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "description": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "peekOfCode": "def create_fill_one_partial_db_wrapper(args):\n    \"\"\" wrapper function for multiprocessor, calls run_steps \"\"\"\n    try:\n        create_fill_one_partial_db(**args)\n    except KeyboardInterrupt:\n        raise KeyboardInterruptError()\ndef die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'compute_hashcodes_mprocess.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "description": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'compute_hashcodes_mprocess.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print ''\n    print 'Code to extract all hashcodes from a give folder of'\n    print 'data, e.g. the whole millin song dataset.'\n    print 'Creates as many db as process, then aggregate them into one'\n    print 'USAGE'\n    print '   python compute_hashcodes_mprocess.py <maindir> <output.db> <nthreads>'",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "documentation": {}
    },
    {
        "label": "LEVELS",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "description": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "peekOfCode": "LEVELS = [3]\nCOMPRESSION=2\ndef get_all_files(basedir,ext='.h5') :\n    \"\"\"\n    From a root directory, go through all subdirectories\n    and find all files with the given extension.\n    Return all absolute paths in a list.\n    \"\"\"\n    allfiles = set()\n    for root, dirs, files in os.walk(basedir):",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.compute_hashcodes_mprocess",
        "documentation": {}
    },
    {
        "label": "get_jump_code",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "def get_jump_code(tdiff, pdiff, poffset):\n    \"\"\"\n    Encode a jump based on a time difference and a pitch difference.\n    The encoding is the following:\n      pdiff = (pdiff + 12) % 12\n      code = tdiff * 12 + pdiff\n    tdiff of zero are accepted!\n    \"\"\"\n    return FH.get_jump_code(tdiff, pdiff, poffset)\ndef rotate_jump_code(jumpcode, rotation):",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "rotate_jump_code",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "def rotate_jump_code(jumpcode, rotation):\n    \"\"\"\n    From a given jumpcode, and a pitch rotation (0 -> 11)\n    return a new jumpcode\n    \"\"\"\n    assert rotation >= 0 and rotation < 12\n    if rotation == 0:\n        return jumpcode\n    original_offset = jumpcode % 12\n    new_offset = (original_offset + rotation) % 12",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "rotate_jump_code_sql",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "def rotate_jump_code_sql(jumpcode, rotation):\n    \"\"\"\n    Create a piece of SQL command that rotates the jumpcode\n    to the right value\n    \"\"\"\n    q = str(jumpcode) + \"-\" + str(jumpcode) + \"%12\"\n    q += \"+(\" + str(jumpcode) + \"%12+\" +str(rotation) + \")%12\"\n    return q\ndef init_db(conn_or_path):\n    \"\"\"",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "init_db",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "def init_db(conn_or_path):\n    \"\"\"\n    Init the hash table using an open sqlite3 connection.\n    If the argument is a string, we create/open the database.\n    In both case, we return the connection object.\n    \"\"\"\n    # open / get connection\n    is_file = type(conn_or_path).__name__ == 'str'\n    if is_file:\n        conn = sqlite3.connect(conn_or_path)",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "get_current_hash_table",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "def get_current_hash_table(conn):\n    \"\"\"\n    Get the name of the current hash_table\n    \"\"\"\n    # get the number\n    q = \"SELECT cnt FROM num_hash_tables LIMIT 1\"\n    res = conn.execute(q)\n    num_tables = res.fetchone()[0]\n    return 'hashes' + str(num_tables + 1)\ndef add_hash_table(conn, verbose=1):",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "add_hash_table",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "def add_hash_table(conn, verbose=1):\n    \"\"\"\n    Add a new hash table ebcause the previous one is full,\n    i.e. passed the number of max rows\n    RETURN NEW NAME\n    \"\"\"\n    # get actual number of tables\n    q = \"SELECT cnt FROM num_hash_tables LIMIT 1\"\n    res = conn.execute(q)\n    num_tables = res.fetchone()[0]",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "reindex",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "def reindex(conn, verbose=1):\n    \"\"\"\n    Reindex all hash tables\n    \"\"\"\n    # get number of hash tables\n    q = \"SELECT cnt FROM num_hash_tables LIMIT 1\"\n    res = conn.execute(q)\n    num_tables = res.fetchone()[0]\n    # reindex each table\n    for tablecnt in range(1, num_tables+1):",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "reindex_table",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "def reindex_table(conn, table_name, verbose=0):\n    \"\"\"\n    Called by reindex\n    Create or recreate the index on hashcodes\n    \"\"\"\n    t1 = time.time()\n    # check if index exists, delete if needed\n    try:\n        q = \"DROP INDEX \" + INDEX_NAMES + table_name + '_1'\n        conn.execute(q)",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "add_hash",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "def add_hash(conn, tid, hashcode, weight=1., tid_is_in=False):\n    \"\"\"\n    Add a given hashcode (jumpcode) in the hashtable,\n    possibly with a given weight (default=1.)\n    To makes things faster if we know the tid is already in the tids table,\n    set tid_is_in to True\n    If tid_is_in is False, we also check for the current hash_table, eventually\n    we create a new one\n    \"\"\"\n    global current_hash_table",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "add_jumpcodes",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "def add_jumpcodes(conn, tid, jumpcodes, normalize=False, commit=False):\n    \"\"\"\n    From a list of jumps (time diff / pitch diff / pitch offset), add\n    them to the hashes table\n    PARAMS\n        codeoffset    - add some offset to the code,\n                        can be useful for different sets of codes\n    \"\"\"\n    #weights = np.zeros(MAX_JUMP_CODE + 1, dtype='float')\n    weights = {}",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "retrieve_hash",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "def retrieve_hash(conn,hashcode):\n    \"\"\"\n    Given a hashcode/jumpcode, returns a list of track_id and weight\n    that fit.\n    RETURN\n       list of (tid, weight)\n    \"\"\"\n    # need reindexing?\n    #if NEEDS_REINDEXING:\n    #    reindex(conn)",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "select_matches",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "def select_matches(conn, jumps, weights, weight_margin=.1, from_tid=None, verbose=0):\n    \"\"\"\n    Select possible cover songs based on a set of jumpcodes and weights.\n    PARAMS\n       weight_margin    - we look for weights that are within that many\n                          percent of the original\n       tid_in_db        - if the tid is already in the db, we don't need\n                          jumps and weights, we take it from db\n    \"\"\"\n    # get number of hash tables",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "select_matches2",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "def select_matches2(conn, weight_margin=.1, from_tid=None, verbose=0):\n    \"\"\"\n    Select possible cover songs based on a set of jumpcodes and weights.\n    ASSUME WE HAVE ONE TABLE PER JUMPCODE\n    PARAMS\n       weight_margin    - we look for weights that are within that many\n                          percent of the original\n       tid_in_db        - if the tid is already in the db, we don't need\n                          jumps and weights, we take it from db\n    \"\"\"",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'cover_hash_table.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print '      tb2332@columbia.edu'\n    print ''\n    print 'This should be mostly used as a library, but you can'\n    print 'launch some debugging code using:'\n    print '   python cover_hash_table.py [FLAGS] <datadir> <coverlist> <tmp_db>'\n    print 'FLAGS'",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "INDEX_NAMES",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "INDEX_NAMES = 'idx_jumpcodes_'\nNEEDS_REINDEXING = True\nMAX_ROWS = 1e7   # max number rows in 'hashes' before using a second table\n# FOR DEBUGGING\nWIN=3\nDECAY=.995\nNORMALIZE=True\nWEIGHT_MARGIN=.60\nMAX_PER_FRAME=3\nLEVELS = [3]",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "NEEDS_REINDEXING",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "NEEDS_REINDEXING = True\nMAX_ROWS = 1e7   # max number rows in 'hashes' before using a second table\n# FOR DEBUGGING\nWIN=3\nDECAY=.995\nNORMALIZE=True\nWEIGHT_MARGIN=.60\nMAX_PER_FRAME=3\nLEVELS = [3]\nCOMPRESSION=2",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "MAX_ROWS",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "MAX_ROWS = 1e7   # max number rows in 'hashes' before using a second table\n# FOR DEBUGGING\nWIN=3\nDECAY=.995\nNORMALIZE=True\nWEIGHT_MARGIN=.60\nMAX_PER_FRAME=3\nLEVELS = [3]\nCOMPRESSION=2\n# SLOW (SLIGHTLY BETTER) SET OF HASHES",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "LEVELS",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "LEVELS = [3]\nCOMPRESSION=2\n# SLOW (SLIGHTLY BETTER) SET OF HASHES\n#WIN=6\n#DECAY=.96\n#NORMALIZE=True\n#WEIGHT_MARGIN=.50\n#MAX_PER_FRAME=2\n#LEVELS = [1,4]\n#COMPRESSION=1",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "#LEVELS",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "#LEVELS = [1,4]\n#COMPRESSION=1\nimport fingerprint_hash as FH\ndef get_jump_code(tdiff, pdiff, poffset):\n    \"\"\"\n    Encode a jump based on a time difference and a pitch difference.\n    The encoding is the following:\n      pdiff = (pdiff + 12) % 12\n      code = tdiff * 12 + pdiff\n    tdiff of zero are accepted!",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "current_hash_table",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "current_hash_table = 'hashes1'\ndef add_hash(conn, tid, hashcode, weight=1., tid_is_in=False):\n    \"\"\"\n    Add a given hashcode (jumpcode) in the hashtable,\n    possibly with a given weight (default=1.)\n    To makes things faster if we know the tid is already in the tids table,\n    set tid_is_in to True\n    If tid_is_in is False, we also check for the current hash_table, eventually\n    we create a new one\n    \"\"\"",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "_KNOWN_JCODES",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "_KNOWN_JCODES = set()\n_ALL_TIDS = []\ndef select_matches2(conn, weight_margin=.1, from_tid=None, verbose=0):\n    \"\"\"\n    Select possible cover songs based on a set of jumpcodes and weights.\n    ASSUME WE HAVE ONE TABLE PER JUMPCODE\n    PARAMS\n       weight_margin    - we look for weights that are within that many\n                          percent of the original\n       tid_in_db        - if the tid is already in the db, we don't need",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "_ALL_TIDS",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "description": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "peekOfCode": "_ALL_TIDS = []\ndef select_matches2(conn, weight_margin=.1, from_tid=None, verbose=0):\n    \"\"\"\n    Select possible cover songs based on a set of jumpcodes and weights.\n    ASSUME WE HAVE ONE TABLE PER JUMPCODE\n    PARAMS\n       weight_margin    - we look for weights that are within that many\n                          percent of the original\n       tid_in_db        - if the tid is already in the db, we don't need\n                          jumps and weights, we take it from db",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.cover_hash_table",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.create_jcode_tables",
        "description": "Tasks_Demos.CoverSongs.waspaa11.create_jcode_tables",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'create one table per code!'\n    print 'USAGE:'\n    print '   python create_jcode_tables.py <db>'\n    sys.exit(0)\nif __name__ == '__main__':\n    # help menu\n    if len(sys.argv) < 2:\n        die_with_usage()",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.create_jcode_tables",
        "documentation": {}
    },
    {
        "label": "landmarks_pass",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "description": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "peekOfCode": "def landmarks_pass(btchroma, decay, max_per_frame):\n    \"\"\"\n    Performs a forward pass to find landmarks\n    PARAMS\n       btchroma       - we know what this is\n       decay          - same as get_landmarks\n       max_per_frame  - how many landmarks we allow in 1 col\n    RETURN\n       binary matrix, same size as btchroma, with ones\n       for landmarks",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "documentation": {}
    },
    {
        "label": "get_landmarks",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "description": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "peekOfCode": "def get_landmarks(btchroma, decay, max_per_frame=3, verbose=0):\n    \"\"\"\n    Returns a set of landmarks extracted from btchroma.\n    \"\"\"\n    if verbose > 0:\n        t1 = time.time()\n    # forward pass\n    landmarks_fwd = landmarks_pass(btchroma, decay=decay, max_per_frame=max_per_frame)\n    # backward pass\n    landmarks_bwd = landmarks_pass(btchroma[:,-1::-1],",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "documentation": {}
    },
    {
        "label": "get_jumps",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "description": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "peekOfCode": "def get_jumps(landmarks, win=12):\n    \"\"\"\n    Receives a binary matrix containing landmarks.\n    Computes pairs over a window of size 'win'\n    ONLY ONE MAX PER COL IS ALOUD\n    ONLY JUMPS BETWEEN TWO POINTS ARE CONSIDERED\n    RETURN\n       list of [tdiff, pdiff, toffset, poffset]\n    \"\"\"\n    assert landmarks.shape[0] == 12, 'What?'",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "documentation": {}
    },
    {
        "label": "get_jumpcode_from_composed_jump",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "description": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "peekOfCode": "def get_jumpcode_from_composed_jump(jump, maxwin=15, slowdebug=False):\n    \"\"\"\n    Compute a jumpcode from a set of jumps\n    The jump here is the output of 'get_composed_jumps(...)'\n    It's a list of (t,p,t,p,t,p...), eventually with a -1 value at the end\n    \"\"\"\n    nlmks = int(len(jump) / 2)\n    assert nlmks > 1\n    maxcode = 12 * 12 * (maxwin + 1) # max code for a level 1 jump (2 lmks)\n    finalcode = 0",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "documentation": {}
    },
    {
        "label": "get_jump_code",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "description": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "peekOfCode": "def get_jump_code(tdiff, pdiff, poffset):\n    \"\"\"\n    Encode a jump based on a time difference and a pitch difference.\n    The encoding is the following:\n      pdiff = (pdiff + 12) % 12\n      code = tdiff * 12 + pdiff\n    tdiff of zero are accepted!\n    \"\"\"\n    pdiff = (pdiff + 12) % 12\n    # sanity checks, should be removed for speed",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "documentation": {}
    },
    {
        "label": "compose_jump_code",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "description": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "peekOfCode": "def compose_jump_code(code1, code2):\n    \"\"\"\n    We want to avoid collision, and rotation must still be done\n    by %12 and adding rotation\n    \"\"\"\n    code1_rot = code1 % 12\n    code1_norot = code1 - code1_rot\n    code2_norot = code2 - (code2 % 12)\n    # combine code norot\n    mult = 12 * np.power(10, max(int(np.log10(code1_norot)), int(np.log10(code2_norot))) + 1)",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "documentation": {}
    },
    {
        "label": "add_nlmk2_jumps_to_db",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "description": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "peekOfCode": "def add_nlmk2_jumps_to_db(conn, jumps, nocopy=False):\n    \"\"\"\n    Get the output of 'get_jumps' and add it to a proper\n    database that can be use to compose_jumps later\n    \"\"\"\n    # create table\n    q = \"CREATE TEMP TABLE jumps_level1 (t0 INT, p0 INT, t1 INT, p1 INT, jumpcode INT)\"\n    conn.execute(q)\n    # add jumps\n    for tdiff, pdiff, t0, p0 in jumps:",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "documentation": {}
    },
    {
        "label": "compose_jumps",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "description": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "peekOfCode": "def compose_jumps(conn, win, level=2, verbose=0):\n    \"\"\"\n    Receives a database of jumps that have been composed up to\n    the ('level'-1) level\n    level1 means jumps between 2 landmarks, so we do at least level 2 here..\n    Composed jumps have to have a maximum lenght of size 'win'\n    \"\"\"\n    assert level >= 2\n    # name of current (level under) table\n    curr_table_name = \"jumps_level\" + str(level-1)",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "documentation": {}
    },
    {
        "label": "get_composed_jumps",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "description": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "peekOfCode": "def get_composed_jumps(jumps, levels, win, verbose=0):\n    \"\"\"\n    Take the output of get_jumps (from landmarks)\n    Compose the jumps, return them as an array of array.\n    If intermediate=True, we return the jumps for intermediary levels,\n    not just the requested one.\n    We use a temporary sqlite3 connection to work.\n    \"\"\"\n    assert len(levels) > 0\n    maxlevel = max(levels)",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "description": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'fingerprint_hash.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print '      tb2332@columbia.edu'\n    print ''\n    print 'This code should mostly be used as a library'\n    print 'Creates landmarks similar to D. Ellis fingerprinting code.'\n    print 'USAGE (debugging):'\n    print '    python fingerprint_hash.py <hdf5 song file> <decay>'",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.fingerprint_hash",
        "documentation": {}
    },
    {
        "label": "KeyboardInterruptError",
        "kind": 6,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.query_for_covers_mprocess",
        "description": "Tasks_Demos.CoverSongs.waspaa11.query_for_covers_mprocess",
        "peekOfCode": "class KeyboardInterruptError(Exception):\n    pass\n# for multiprocessing\ndef query_one_thread_wrapper(args):\n    \"\"\" wrapper function for multiprocessor, calls run_steps \"\"\"\n    try:\n        query_one_thread(**args)\n    except KeyboardInterrupt:\n        raise KeyboardInterruptError()\ndef die_with_usage():",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.query_for_covers_mprocess",
        "documentation": {}
    },
    {
        "label": "read_cover_list",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.query_for_covers_mprocess",
        "description": "Tasks_Demos.CoverSongs.waspaa11.query_for_covers_mprocess",
        "peekOfCode": "def read_cover_list(filename):\n    \"\"\"\n    Read shs_dataset_train.txt or similarly formatted file.\n    Return\n      * clique -> tids dict\n      * clique -> name\n    clique are random integer numbers\n    \"\"\"\n    clique_tid = {}\n    clique_name = {}",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.query_for_covers_mprocess",
        "documentation": {}
    },
    {
        "label": "init_h5_result_file",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.query_for_covers_mprocess",
        "description": "Tasks_Demos.CoverSongs.waspaa11.query_for_covers_mprocess",
        "peekOfCode": "def init_h5_result_file(h5, expectedrows=50000):\n    \"\"\"\n    Receives a h5 file that has just been created,\n    creates the proper arrays:\n     - query\n     - target\n     - position\n     - n_results\n    \"\"\"\n    group = h5.createGroup(\"/\",'results','general, sole group')",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.query_for_covers_mprocess",
        "documentation": {}
    },
    {
        "label": "query_one_thread",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.query_for_covers_mprocess",
        "description": "Tasks_Demos.CoverSongs.waspaa11.query_for_covers_mprocess",
        "peekOfCode": "def query_one_thread(dbpath=None, clique_tid=None, outputf=None):\n    \"\"\"\n    Query the database given a set of queries, put the result\n    in a... HDF5 file?\n    \"\"\"\n    assert not dbpath is None\n    assert not clique_tid is None\n    assert not outputf is None\n    # verbose\n    n_cover_songs = sum(map(lambda l: len(l), clique_tid.values()))    ",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.query_for_covers_mprocess",
        "documentation": {}
    },
    {
        "label": "query_one_thread_wrapper",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.query_for_covers_mprocess",
        "description": "Tasks_Demos.CoverSongs.waspaa11.query_for_covers_mprocess",
        "peekOfCode": "def query_one_thread_wrapper(args):\n    \"\"\" wrapper function for multiprocessor, calls run_steps \"\"\"\n    try:\n        query_one_thread(**args)\n    except KeyboardInterrupt:\n        raise KeyboardInterruptError()\ndef die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'query_for_covers_mprocess.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.query_for_covers_mprocess",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.query_for_covers_mprocess",
        "description": "Tasks_Demos.CoverSongs.waspaa11.query_for_covers_mprocess",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'query_for_covers_mprocess.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print 'query a filled database with cliques of covers'\n    print 'i.e. the SHS train or test set'\n    print ''\n    print 'USAGE'\n    print '   python query_for_covers_mprocess.py <db> <SHS set> <output> <nthreads>'\n    sys.exit(0)",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.query_for_covers_mprocess",
        "documentation": {}
    },
    {
        "label": "get_cpressed_btchroma",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "def get_cpressed_btchroma(path, compression=1):\n    \"\"\" to easily play with the btchromas we get \"\"\"\n    # MAIN FUNCTION WITH COMPRESSION / STRETCH\n    add_loudness = False\n    h5 = BAF.GETTERS.open_h5_file_read(path)\n    chromas = BAF.GETTERS.get_segments_pitches(h5)\n    segstarts = BAF.GETTERS.get_segments_start(h5)\n    if compression >= 1:\n        btstarts = BAF.GETTERS.get_beats_start(h5)[::compression]\n    elif compression == .5:",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "renorm_chroma",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "def renorm_chroma(chroma):\n    \"\"\"\n    Weird method to put all chroma values between 0 and 1 where\n    the max of each column gets 1, the second 1-1/11, ... the last 0\n    \"\"\"\n    t1 = time.time()\n    assert chroma.shape[0] == 12\n    s = np.argsort(chroma, axis=0)\n    #chroma = 1. - 1./s  # we create inf, we could do something faster\n    #chroma[np.isinf(chroma)] = 1.",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "path_from_tid",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "def path_from_tid(maindir, tid):\n    \"\"\"\n    Returns a full path based on a main directory and a track id\n    \"\"\"\n    p = os.path.join(maindir, tid[2])\n    p = os.path.join(p, tid[3])\n    p = os.path.join(p, tid[4])\n    p = os.path.join(p, tid.upper() + '.h5')\n    return p\ndef read_cover_list(filename):",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "read_cover_list",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "def read_cover_list(filename):\n    \"\"\"\n    Read shs_dataset_train.txt or similarly formatted file.\n    Return\n      * clique -> tids dict\n      * clique -> name\n    clique are random integer numbers\n    \"\"\"\n    clique_tid = {}\n    clique_name = {}",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "get_jumps",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "def get_jumps(btchroma, verbose=0):\n    \"\"\"\n    Use fingerprinting hash\n    \"\"\"\n    landmarks = FH.get_landmarks(btchroma, decay=DECAY, max_per_frame=MAX_PER_FRAME, verbose=verbose)\n    jumps = FH.get_jumps(landmarks, win=WIN)\n    return jumps\ndef one_exp(maindir, clique_tid, verbose=0):\n    \"\"\"\n    performs one experiment:",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "one_exp",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "def one_exp(maindir, clique_tid, verbose=0):\n    \"\"\"\n    performs one experiment:\n      - select two covers\n      - select random song\n      - computes hashes / jumps\n      - return 1 if we return cover correctly, 0 otherwise\n    \"\"\"\n    # select cliques\n    cliques = sorted(clique_tid.keys())",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'Performs a few quick experiments to easily compare hashing'\n    print 'methods without a full experiment.'\n    print 'USAGE:'\n    print '     python quick_query_test.py <maindir> <coverlist> <OPT: comment>'\n    sys.exit(0)\nif __name__ == '__main__':\n    # help menu\n    if len(sys.argv) < 3:",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "RANDOMSEED",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "RANDOMSEED = hash('caitlin')\n# params\nNUM_EXPS = 524\nDECAY = .995\nWIN = 3\nNORMALIZE = True\nWEIGHT_MARGIN = .60\nMAX_PER_FRAME = 3\nCOMPRESSION = [2] #(1,2,3,4,8)\nLEVELS = [3] # composed jumps, 1 means first order jumps",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "NUM_EXPS",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "NUM_EXPS = 524\nDECAY = .995\nWIN = 3\nNORMALIZE = True\nWEIGHT_MARGIN = .60\nMAX_PER_FRAME = 3\nCOMPRESSION = [2] #(1,2,3,4,8)\nLEVELS = [3] # composed jumps, 1 means first order jumps\n# BEST (SLOW!) 500 -> .798\n#DECAY = 0.96",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "DECAY",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "DECAY = .995\nWIN = 3\nNORMALIZE = True\nWEIGHT_MARGIN = .60\nMAX_PER_FRAME = 3\nCOMPRESSION = [2] #(1,2,3,4,8)\nLEVELS = [3] # composed jumps, 1 means first order jumps\n# BEST (SLOW!) 500 -> .798\n#DECAY = 0.96\n#WIN = 6",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "WIN",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "WIN = 3\nNORMALIZE = True\nWEIGHT_MARGIN = .60\nMAX_PER_FRAME = 3\nCOMPRESSION = [2] #(1,2,3,4,8)\nLEVELS = [3] # composed jumps, 1 means first order jumps\n# BEST (SLOW!) 500 -> .798\n#DECAY = 0.96\n#WIN = 6\n#NORMALIZE = 1",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "NORMALIZE",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "NORMALIZE = True\nWEIGHT_MARGIN = .60\nMAX_PER_FRAME = 3\nCOMPRESSION = [2] #(1,2,3,4,8)\nLEVELS = [3] # composed jumps, 1 means first order jumps\n# BEST (SLOW!) 500 -> .798\n#DECAY = 0.96\n#WIN = 6\n#NORMALIZE = 1\n#WEIGHT_MARGIN = 0.50",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "WEIGHT_MARGIN",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "WEIGHT_MARGIN = .60\nMAX_PER_FRAME = 3\nCOMPRESSION = [2] #(1,2,3,4,8)\nLEVELS = [3] # composed jumps, 1 means first order jumps\n# BEST (SLOW!) 500 -> .798\n#DECAY = 0.96\n#WIN = 6\n#NORMALIZE = 1\n#WEIGHT_MARGIN = 0.50\n#MAX_PER_FRAME = 2",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "MAX_PER_FRAME",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "MAX_PER_FRAME = 3\nCOMPRESSION = [2] #(1,2,3,4,8)\nLEVELS = [3] # composed jumps, 1 means first order jumps\n# BEST (SLOW!) 500 -> .798\n#DECAY = 0.96\n#WIN = 6\n#NORMALIZE = 1\n#WEIGHT_MARGIN = 0.50\n#MAX_PER_FRAME = 2\n#COMPRESSION = [1]",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "COMPRESSION",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "COMPRESSION = [2] #(1,2,3,4,8)\nLEVELS = [3] # composed jumps, 1 means first order jumps\n# BEST (SLOW!) 500 -> .798\n#DECAY = 0.96\n#WIN = 6\n#NORMALIZE = 1\n#WEIGHT_MARGIN = 0.50\n#MAX_PER_FRAME = 2\n#COMPRESSION = [1]\n#LEVELS = [1, 4]",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "LEVELS",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "LEVELS = [3] # composed jumps, 1 means first order jumps\n# BEST (SLOW!) 500 -> .798\n#DECAY = 0.96\n#WIN = 6\n#NORMALIZE = 1\n#WEIGHT_MARGIN = 0.50\n#MAX_PER_FRAME = 2\n#COMPRESSION = [1]\n#LEVELS = [1, 4]\ndef get_cpressed_btchroma(path, compression=1):",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "#DECAY",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "#DECAY = 0.96\n#WIN = 6\n#NORMALIZE = 1\n#WEIGHT_MARGIN = 0.50\n#MAX_PER_FRAME = 2\n#COMPRESSION = [1]\n#LEVELS = [1, 4]\ndef get_cpressed_btchroma(path, compression=1):\n    \"\"\" to easily play with the btchromas we get \"\"\"\n    # MAIN FUNCTION WITH COMPRESSION / STRETCH",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "#WIN",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "#WIN = 6\n#NORMALIZE = 1\n#WEIGHT_MARGIN = 0.50\n#MAX_PER_FRAME = 2\n#COMPRESSION = [1]\n#LEVELS = [1, 4]\ndef get_cpressed_btchroma(path, compression=1):\n    \"\"\" to easily play with the btchromas we get \"\"\"\n    # MAIN FUNCTION WITH COMPRESSION / STRETCH\n    add_loudness = False",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "#NORMALIZE",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "#NORMALIZE = 1\n#WEIGHT_MARGIN = 0.50\n#MAX_PER_FRAME = 2\n#COMPRESSION = [1]\n#LEVELS = [1, 4]\ndef get_cpressed_btchroma(path, compression=1):\n    \"\"\" to easily play with the btchromas we get \"\"\"\n    # MAIN FUNCTION WITH COMPRESSION / STRETCH\n    add_loudness = False\n    h5 = BAF.GETTERS.open_h5_file_read(path)",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "#WEIGHT_MARGIN",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "#WEIGHT_MARGIN = 0.50\n#MAX_PER_FRAME = 2\n#COMPRESSION = [1]\n#LEVELS = [1, 4]\ndef get_cpressed_btchroma(path, compression=1):\n    \"\"\" to easily play with the btchromas we get \"\"\"\n    # MAIN FUNCTION WITH COMPRESSION / STRETCH\n    add_loudness = False\n    h5 = BAF.GETTERS.open_h5_file_read(path)\n    chromas = BAF.GETTERS.get_segments_pitches(h5)",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "#MAX_PER_FRAME",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "#MAX_PER_FRAME = 2\n#COMPRESSION = [1]\n#LEVELS = [1, 4]\ndef get_cpressed_btchroma(path, compression=1):\n    \"\"\" to easily play with the btchromas we get \"\"\"\n    # MAIN FUNCTION WITH COMPRESSION / STRETCH\n    add_loudness = False\n    h5 = BAF.GETTERS.open_h5_file_read(path)\n    chromas = BAF.GETTERS.get_segments_pitches(h5)\n    segstarts = BAF.GETTERS.get_segments_start(h5)",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "#COMPRESSION",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "#COMPRESSION = [1]\n#LEVELS = [1, 4]\ndef get_cpressed_btchroma(path, compression=1):\n    \"\"\" to easily play with the btchromas we get \"\"\"\n    # MAIN FUNCTION WITH COMPRESSION / STRETCH\n    add_loudness = False\n    h5 = BAF.GETTERS.open_h5_file_read(path)\n    chromas = BAF.GETTERS.get_segments_pitches(h5)\n    segstarts = BAF.GETTERS.get_segments_start(h5)\n    if compression >= 1:",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "#LEVELS",
        "kind": 5,
        "importPath": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "description": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "peekOfCode": "#LEVELS = [1, 4]\ndef get_cpressed_btchroma(path, compression=1):\n    \"\"\" to easily play with the btchromas we get \"\"\"\n    # MAIN FUNCTION WITH COMPRESSION / STRETCH\n    add_loudness = False\n    h5 = BAF.GETTERS.open_h5_file_read(path)\n    chromas = BAF.GETTERS.get_segments_pitches(h5)\n    segstarts = BAF.GETTERS.get_segments_start(h5)\n    if compression >= 1:\n        btstarts = BAF.GETTERS.get_beats_start(h5)[::compression]",
        "detail": "Tasks_Demos.CoverSongs.waspaa11.quick_query_test",
        "documentation": {}
    },
    {
        "label": "create_cliques",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.finding_duplicates",
        "description": "Tasks_Demos.CoverSongs.finding_duplicates",
        "peekOfCode": "def create_cliques(data):\n    \"\"\"\n    Always receive an array of arrays, each sub array has 3 elements:\n        - artist (id or name)\n        - song (id or title)\n        - track id\n    From that, create cluster (or clique) that have the first two\n    elements in common. We asusme they are duplicates.\n    We rely on the python hash() function, we assume no collisions.\n    Returns a dictionary: int -> [tracks], int are arbitrary",
        "detail": "Tasks_Demos.CoverSongs.finding_duplicates",
        "documentation": {}
    },
    {
        "label": "merge_cliques",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.finding_duplicates",
        "description": "Tasks_Demos.CoverSongs.finding_duplicates",
        "peekOfCode": "def merge_cliques(clique1,clique2):\n    \"\"\"\n    Merge two sets of cliques, return a new dictionary\n    of clique. As always, dict keys are random.\n    \"\"\"\n    # copy clique1 into new one\n    new_clique = {}\n    new_clique.update( zip( range(1,len(clique1)+1), clique1.values() ) )\n    clique_id = len(clique1)\n    # reverse index",
        "detail": "Tasks_Demos.CoverSongs.finding_duplicates",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.CoverSongs.finding_duplicates",
        "description": "Tasks_Demos.CoverSongs.finding_duplicates",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'finding_duplicates.py'\n    print '    by T. Bertin-Mahieux (2011) Columbia University'\n    print '       tb2332@columbia.edu'\n    print ''\n    print 'This code identify a list of duplicate songs in the dataset.'\n    print 'These duplicates are easy to find, in the sense that they have'\n    print 'the same artist and title. WE DO NOT FIND ALL DUPLICATES!'\n    print ''",
        "detail": "Tasks_Demos.CoverSongs.finding_duplicates",
        "documentation": {}
    },
    {
        "label": "lyrics_to_bow",
        "kind": 2,
        "importPath": "Tasks_Demos.Lyrics.lyrics_to_bow",
        "description": "Tasks_Demos.Lyrics.lyrics_to_bow",
        "peekOfCode": "def lyrics_to_bow(lyrics):\n    \"\"\"\n    Main function to stem and create bag of words.\n    It is what we used for the musiXmatch dataset.\n    It is heavily oriented towards English lyrics, we apologize for that.\n    INPUT\n        lyrics as a string\n    RETURN\n        dictionary word -> count\n        or None if something was wrong (e.g. not enough words)",
        "detail": "Tasks_Demos.Lyrics.lyrics_to_bow",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.Lyrics.lyrics_to_bow",
        "description": "Tasks_Demos.Lyrics.lyrics_to_bow",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'lyrics_to_bow.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print '      tb2332@columbia.edu'\n    print 'This code shows how we transformed lyrics into bag-of-words.'\n    print 'It is mostly intended to be used as a library, but you can pass'\n    print 'in lyrics and we print the resulting dictionary.'\n    print ''\n    print 'USAGE:'",
        "detail": "Tasks_Demos.Lyrics.lyrics_to_bow",
        "documentation": {}
    },
    {
        "label": "encode_string",
        "kind": 2,
        "importPath": "Tasks_Demos.Lyrics.mxm_dataset_to_db",
        "description": "Tasks_Demos.Lyrics.mxm_dataset_to_db",
        "peekOfCode": "def encode_string(s):\n    \"\"\"\n    Simple utility function to make sure a string is proper\n    to be used in a SQLite query\n    (different than posgtresql, no N to specify unicode)\n    EXAMPLE:\n      That's my boy! -> 'That''s my boy!'\n    \"\"\"\n    return \"'\" + s.replace(\"'\", \"''\") + \"'\"\ndef die_with_usage():",
        "detail": "Tasks_Demos.Lyrics.mxm_dataset_to_db",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.Lyrics.mxm_dataset_to_db",
        "description": "Tasks_Demos.Lyrics.mxm_dataset_to_db",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'mxm_dataset_to_db.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print '      tb2332@columbia.edu'\n    print 'This code puts the musiXmatch dataset into an SQLite database.'\n    print ''\n    print 'USAGE:'\n    print '  ./mxm_dataset_to_db.py <train> <test> <output.db>'\n    print 'PARAMS:'",
        "detail": "Tasks_Demos.Lyrics.mxm_dataset_to_db",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.Lyrics.split_mxm_dataset",
        "description": "Tasks_Demos.Lyrics.split_mxm_dataset",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'split_mxm_dataset.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print '      tb2332@columbia.edu'\n    print 'This code splits the full musiXmatch dataset based on the'\n    print 'artist split used for automatic music tagging.'\n    print 'This code is provided more as a demo than anything else,'\n    print 'you should have received the musiXmatch dataset alredy split.'\n    print ''",
        "detail": "Tasks_Demos.Lyrics.split_mxm_dataset",
        "documentation": {}
    },
    {
        "label": "get_artistid_trackid_artistname",
        "kind": 2,
        "importPath": "Tasks_Demos.NamesAnalysis.list_all_artists",
        "description": "Tasks_Demos.NamesAnalysis.list_all_artists",
        "peekOfCode": "def get_artistid_trackid_artistname(trackfile):\n    \"\"\"\n    Utility function, opens a h5 file, gets the 4 following fields:\n     - artist Echo Nest ID\n     - artist Musicbrainz ID\n     - track Echo Nest ID\n     - artist name\n    It is returns as a triple (,,)\n    Assumes one song per file only!\n    \"\"\"",
        "detail": "Tasks_Demos.NamesAnalysis.list_all_artists",
        "documentation": {}
    },
    {
        "label": "list_all",
        "kind": 2,
        "importPath": "Tasks_Demos.NamesAnalysis.list_all_artists",
        "description": "Tasks_Demos.NamesAnalysis.list_all_artists",
        "peekOfCode": "def list_all(maindir):\n    \"\"\"\n    Goes through all subdirectories, open every song file,\n    and list all artists it finds.\n    It returns a dictionary of string -> tuples:\n       artistID -> (musicbrainz ID, trackID, artist_name)\n    The track ID is random, i.e. the first one we find for that\n    artist. The artist information should be the same in all track\n    files from that artist.\n    We assume one song per file, if not, must be modified to take",
        "detail": "Tasks_Demos.NamesAnalysis.list_all_artists",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.NamesAnalysis.list_all_artists",
        "description": "Tasks_Demos.NamesAnalysis.list_all_artists",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'list_all_artists.py'\n    print '   by T. Bertin-Mahieux (2010) Columbia University'\n    print ''\n    print 'usage:'\n    print '  python list_all_artists.py <DATASET DIR> output.txt'\n    print ''\n    print 'This code lets you list all artists contained in all'\n    print 'subdirectories of a given directory.'",
        "detail": "Tasks_Demos.NamesAnalysis.list_all_artists",
        "documentation": {}
    },
    {
        "label": "url_call",
        "kind": 2,
        "importPath": "Tasks_Demos.Preview7digital.get_preview_url",
        "description": "Tasks_Demos.Preview7digital.get_preview_url",
        "peekOfCode": "def url_call(url):\n    \"\"\"\n    Do a simple request to the 7digital API\n    We assume we don't do intense querying, this function is not\n    robust\n    Return the answer as na xml document\n    \"\"\"\n    stream = urllib2.urlopen(url)\n    xmldoc = minidom.parse(stream).documentElement\n    stream.close()",
        "detail": "Tasks_Demos.Preview7digital.get_preview_url",
        "documentation": {}
    },
    {
        "label": "levenshtein",
        "kind": 2,
        "importPath": "Tasks_Demos.Preview7digital.get_preview_url",
        "description": "Tasks_Demos.Preview7digital.get_preview_url",
        "peekOfCode": "def levenshtein(s1, s2):\n    \"\"\"\n    Levenstein distance, or edit distance, taken from Wikibooks:\n    http://en.wikibooks.org/wiki/Algorithm_implementation/Strings/Levenshtein_distance#Python\n    \"\"\"\n    if len(s1) < len(s2):\n        return levenshtein(s2, s1)\n    if not s1:\n        return len(s2)\n    previous_row = xrange(len(s2) + 1)",
        "detail": "Tasks_Demos.Preview7digital.get_preview_url",
        "documentation": {}
    },
    {
        "label": "get_closest_track",
        "kind": 2,
        "importPath": "Tasks_Demos.Preview7digital.get_preview_url",
        "description": "Tasks_Demos.Preview7digital.get_preview_url",
        "peekOfCode": "def get_closest_track(tracklist,target):\n    \"\"\"\n    Find the closest track based on edit distance\n    Might not be an exact match, you should check!\n    \"\"\"\n    dists = map(lambda x: levenshtein(x,target),tracklist)\n    best = argmin(dists)\n    return tracklist[best]\ndef get_trackid_from_text_search(title,artistname=''):\n    \"\"\"",
        "detail": "Tasks_Demos.Preview7digital.get_preview_url",
        "documentation": {}
    },
    {
        "label": "get_trackid_from_text_search",
        "kind": 2,
        "importPath": "Tasks_Demos.Preview7digital.get_preview_url",
        "description": "Tasks_Demos.Preview7digital.get_preview_url",
        "peekOfCode": "def get_trackid_from_text_search(title,artistname=''):\n    \"\"\"\n    Search for an artist + title using 7digital search API\n    Return None if there is a problem, or tuple (title,trackid)\n    \"\"\"\n    url = 'http://api.7digital.com/1.2/track/search?'\n    url += 'oauth_consumer_key='+DIGITAL7_API_KEY\n    query = title\n    if artistname != '':\n        query = artistname + ' ' + query",
        "detail": "Tasks_Demos.Preview7digital.get_preview_url",
        "documentation": {}
    },
    {
        "label": "get_tracks_from_artistid",
        "kind": 2,
        "importPath": "Tasks_Demos.Preview7digital.get_preview_url",
        "description": "Tasks_Demos.Preview7digital.get_preview_url",
        "peekOfCode": "def get_tracks_from_artistid(artistid):\n    \"\"\"\n    We get a list of release from artists.\n    For each of these, get release.\n    After calling the API with a given release ID, we receive a list of tracks.\n    We return a map of <track name> -> <track id>\n    or None if there is a problem\n    \"\"\"\n    url = 'http://api.7digital.com/1.2/artist/releases?'\n    url += '&artistid='+str(artistid)",
        "detail": "Tasks_Demos.Preview7digital.get_preview_url",
        "documentation": {}
    },
    {
        "label": "get_tracks_from_releaseid",
        "kind": 2,
        "importPath": "Tasks_Demos.Preview7digital.get_preview_url",
        "description": "Tasks_Demos.Preview7digital.get_preview_url",
        "peekOfCode": "def get_tracks_from_releaseid(releaseid):\n    \"\"\"\n    After calling the API with a given release ID, we receive a list of tracks.\n    We return a map of <track name> -> <track id>\n    or None if there is a problem\n    \"\"\"\n    url = 'http://api.7digital.com/1.2/release/tracks?'\n    url += 'releaseid='+str(releaseid)\n    url += '&oauth_consumer_key='+DIGITAL7_API_KEY\n    xmldoc = url_call(url)",
        "detail": "Tasks_Demos.Preview7digital.get_preview_url",
        "documentation": {}
    },
    {
        "label": "get_preview_from_trackid",
        "kind": 2,
        "importPath": "Tasks_Demos.Preview7digital.get_preview_url",
        "description": "Tasks_Demos.Preview7digital.get_preview_url",
        "peekOfCode": "def get_preview_from_trackid(trackid):\n    \"\"\"\n    Ask for the preview to a particular track, get the XML answer\n    After calling the API with a given track id,\n    we get an XML response that looks like:\n    <response status=\"ok\" version=\"1.2\" xsi:noNamespaceSchemaLocation=\"http://api.7digital.com/1.2/static/7digitalAPI.xsd\">\n      <url>\n        http://previews.7digital.com/clips/34/6804688.clip.mp3\n      </url>\n    </response>",
        "detail": "Tasks_Demos.Preview7digital.get_preview_url",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.Preview7digital.get_preview_url",
        "description": "Tasks_Demos.Preview7digital.get_preview_url",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'get_preview_url.py'\n    print '    by T. Bertin-Mahieux (2010) Columbia University'\n    print 'HELP MENU'\n    print 'usage:'\n    print '    python get_preview_url.py [FLAG] <SONGFILE>'\n    print 'PARAMS:'\n    print '  <SONGFILE>  - a Million Song Dataset file TRABC...123.h5'\n    print 'FLAGS:'",
        "detail": "Tasks_Demos.Preview7digital.get_preview_url",
        "documentation": {}
    },
    {
        "label": "pythonsrc",
        "kind": 5,
        "importPath": "Tasks_Demos.Preview7digital.get_preview_url",
        "description": "Tasks_Demos.Preview7digital.get_preview_url",
        "peekOfCode": "pythonsrc = os.path.abspath('__file__')\npythonsrc = os.path.join(pythonsrc,'../../../PythonSrc')\npythonsrc = os.path.abspath( pythonsrc )\nsys.path.append( pythonsrc )\nimport hdf5_utils\nimport hdf5_getters as GETTERS\n# try to get 7digital API key\nglobal DIGITAL7_API_KEY\ntry:\n    DIGITAL7_API_KEY = os.environ['DIGITAL7_API_KEY']",
        "detail": "Tasks_Demos.Preview7digital.get_preview_url",
        "documentation": {}
    },
    {
        "label": "pythonsrc",
        "kind": 5,
        "importPath": "Tasks_Demos.Preview7digital.get_preview_url",
        "description": "Tasks_Demos.Preview7digital.get_preview_url",
        "peekOfCode": "pythonsrc = os.path.join(pythonsrc,'../../../PythonSrc')\npythonsrc = os.path.abspath( pythonsrc )\nsys.path.append( pythonsrc )\nimport hdf5_utils\nimport hdf5_getters as GETTERS\n# try to get 7digital API key\nglobal DIGITAL7_API_KEY\ntry:\n    DIGITAL7_API_KEY = os.environ['DIGITAL7_API_KEY']\nexcept KeyError:",
        "detail": "Tasks_Demos.Preview7digital.get_preview_url",
        "documentation": {}
    },
    {
        "label": "pythonsrc",
        "kind": 5,
        "importPath": "Tasks_Demos.Preview7digital.get_preview_url",
        "description": "Tasks_Demos.Preview7digital.get_preview_url",
        "peekOfCode": "pythonsrc = os.path.abspath( pythonsrc )\nsys.path.append( pythonsrc )\nimport hdf5_utils\nimport hdf5_getters as GETTERS\n# try to get 7digital API key\nglobal DIGITAL7_API_KEY\ntry:\n    DIGITAL7_API_KEY = os.environ['DIGITAL7_API_KEY']\nexcept KeyError:\n    DIGITAL7_API_KEY = None",
        "detail": "Tasks_Demos.Preview7digital.get_preview_url",
        "documentation": {}
    },
    {
        "label": "PlayerApp",
        "kind": 6,
        "importPath": "Tasks_Demos.Preview7digital.player_7digital",
        "description": "Tasks_Demos.Preview7digital.player_7digital",
        "peekOfCode": "class PlayerApp(Frame):\n    \"\"\"\n    MAIN CLASS, contains the Tkinter app\n    \"\"\"\n    def __init__(self, master=None, tmdb=None, url=''):\n        \"\"\"\n        Contstructor\n        INPUTS\n           tmdb  - path to track_metadata.db (containing track_7digitalid)\n           url   - more for debugging, starts with a loaded url",
        "detail": "Tasks_Demos.Preview7digital.player_7digital",
        "documentation": {}
    },
    {
        "label": "encode_string",
        "kind": 2,
        "importPath": "Tasks_Demos.Preview7digital.player_7digital",
        "description": "Tasks_Demos.Preview7digital.player_7digital",
        "peekOfCode": "def encode_string(s):\n    \"\"\"\n    Simple utility function to make sure a string is proper\n    to be used in a SQLite query\n    (different than posgtresql, no N to specify unicode)\n    EXAMPLE:\n      That's my boy! -> 'That''s my boy!'\n    \"\"\"\n    return \"'\"+s.replace(\"'\",\"''\")+\"'\"\nclass PlayerApp(Frame):",
        "detail": "Tasks_Demos.Preview7digital.player_7digital",
        "documentation": {}
    },
    {
        "label": "launch_applet",
        "kind": 2,
        "importPath": "Tasks_Demos.Preview7digital.player_7digital",
        "description": "Tasks_Demos.Preview7digital.player_7digital",
        "peekOfCode": "def launch_applet(tmdb=None,url=''):\n    \"\"\"\n    Should be the main function to launch the interface\n    \"\"\"\n    app = PlayerApp(tmdb=tmdb,url=url)\n    app.master.title(\"7digital Player for the Million Song Dataset\")\n    app.mainloop()\ndef die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'player_7digital.py'",
        "detail": "Tasks_Demos.Preview7digital.player_7digital",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.Preview7digital.player_7digital",
        "description": "Tasks_Demos.Preview7digital.player_7digital",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'player_7digital.py'\n    print '    by T. Bertin-Mahieux (2011) Columbia University'\n    print '       tb2332@columbia.edu'\n    print 'Small interface to the 7digital service.'\n    print 'INPUT'\n    print '   python player_7digital.py track_metadata.db'\n    print 'REQUIREMENTS'\n    print '  * 7digital key in your environment as: DIGITAL7_API_KEY'",
        "detail": "Tasks_Demos.Preview7digital.player_7digital",
        "documentation": {}
    },
    {
        "label": "path_from_trackid",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.create_artist_similarity_db",
        "description": "Tasks_Demos.SQLite.create_artist_similarity_db",
        "peekOfCode": "def path_from_trackid(trackid):\n    \"\"\"\n    Creates the path from a given trackid\n    \"\"\"\n    s = os.path.join(trackid[2],trackid[3])\n    s = os.path.join(s,trackid[4])\n    s = os.path.join(s,trackid)\n    s = s.upper() + '.h5'\n    return s\ndef encode_string(s):",
        "detail": "Tasks_Demos.SQLite.create_artist_similarity_db",
        "documentation": {}
    },
    {
        "label": "encode_string",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.create_artist_similarity_db",
        "description": "Tasks_Demos.SQLite.create_artist_similarity_db",
        "peekOfCode": "def encode_string(s):\n    \"\"\"\n    Simple utility function to make sure a string is proper\n    to be used in a SQLite query\n    (different than posgtresql, no N to specify unicode)\n    EXAMPLE:\n      That's my boy! -> 'That''s my boy!'\n    \"\"\"\n    return \"'\"+s.replace(\"'\",\"''\")+\"'\"\ndef create_db(filename,artistlist):",
        "detail": "Tasks_Demos.SQLite.create_artist_similarity_db",
        "documentation": {}
    },
    {
        "label": "create_db",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.create_artist_similarity_db",
        "description": "Tasks_Demos.SQLite.create_artist_similarity_db",
        "peekOfCode": "def create_db(filename,artistlist):\n    \"\"\"\n    Create a SQLite database with 2 tables\n    table1: artists\n            contains one column, artist_id\n    table2: similarity\n            contains two columns, target and similar\n            both containing Echo Nest artist ID\n            it means that 'similars' are similar to the\n            target. It is not necessarily symmetric!",
        "detail": "Tasks_Demos.SQLite.create_artist_similarity_db",
        "documentation": {}
    },
    {
        "label": "fill_from_h5",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.create_artist_similarity_db",
        "description": "Tasks_Demos.SQLite.create_artist_similarity_db",
        "peekOfCode": "def fill_from_h5(conn,h5path):\n    \"\"\"\n    Fill 'similarity' table from the information regarding the\n    artist in that file, i.e. we get his similar artists, check\n    if they are in the dataset, add them.\n    Doesn't commit, doesn't close conn at the end!\n    This h5 file must be for a new artist, we can't have twice the\n    same artist entered in the database!\n    The info is added to tables: similarity\n    as many row as existing similar artists are added",
        "detail": "Tasks_Demos.SQLite.create_artist_similarity_db",
        "documentation": {}
    },
    {
        "label": "add_indices_to_db",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.create_artist_similarity_db",
        "description": "Tasks_Demos.SQLite.create_artist_similarity_db",
        "peekOfCode": "def add_indices_to_db(conn,verbose=0):\n    \"\"\"\n    Since the db is considered final, we can add all sorts of indecies\n    to make sure the retrieval time is as fast as possible.\n    Indecies take up a little space, but they hurt performance only when\n    we modify the data (which should not happen)\n    This function commits its changes at the end\n    Note: tutorial on MySQL (close enough to SQLite):\n    http://www.databasejournal.com/features/mysql/article.php/10897_1382791_1/Optimizing-MySQL-Queries-and-Indexes.htm\n    \"\"\"",
        "detail": "Tasks_Demos.SQLite.create_artist_similarity_db",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.create_artist_similarity_db",
        "description": "Tasks_Demos.SQLite.create_artist_similarity_db",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'Command to create the artist_terms SQLite database'\n    print 'to launch (it might take a while!):'\n    print '   python create_artist_terms_db.py <MillionSong dir> <artistlist> <artist_similarity.db>'\n    print 'PARAMS'\n    print '  MillionSong dir        - directory containing .h5 song files in sub dirs'\n    print '  artist list            - list in form: artistid<SEP>artist_mbid<SEP>track_id<SEP>...'\n    print '  artist_similarity.db   - filename for the database'\n    print ''",
        "detail": "Tasks_Demos.SQLite.create_artist_similarity_db",
        "documentation": {}
    },
    {
        "label": "path_from_trackid",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.create_artist_terms_db",
        "description": "Tasks_Demos.SQLite.create_artist_terms_db",
        "peekOfCode": "def path_from_trackid(trackid):\n    \"\"\"\n    Creates the path from a given trackid\n    \"\"\"\n    s = os.path.join(trackid[2],trackid[3])\n    s = os.path.join(s,trackid[4])\n    s = os.path.join(s,trackid)\n    s = s.upper() + '.h5'\n    return s\ndef encode_string(s):",
        "detail": "Tasks_Demos.SQLite.create_artist_terms_db",
        "documentation": {}
    },
    {
        "label": "encode_string",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.create_artist_terms_db",
        "description": "Tasks_Demos.SQLite.create_artist_terms_db",
        "peekOfCode": "def encode_string(s):\n    \"\"\"\n    Simple utility function to make sure a string is proper\n    to be used in a SQLite query\n    (different than posgtresql, no N to specify unicode)\n    EXAMPLE:\n      That's my boy! -> 'That''s my boy!'\n    \"\"\"\n    return \"'\"+s.replace(\"'\",\"''\")+\"'\"\ndef create_db(filename,artistlist,termlist,mbtaglist):",
        "detail": "Tasks_Demos.SQLite.create_artist_terms_db",
        "documentation": {}
    },
    {
        "label": "create_db",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.create_artist_terms_db",
        "description": "Tasks_Demos.SQLite.create_artist_terms_db",
        "peekOfCode": "def create_db(filename,artistlist,termlist,mbtaglist):\n    \"\"\"\n    Create a SQLite database with 5 tables\n    table1: artists\n            contains one column, artist_id\n    table2: terms\n            contains one column, term (tags)\n    table3: artist_term\n            contains two columns, artist_id and term\n    table4: mbtags",
        "detail": "Tasks_Demos.SQLite.create_artist_terms_db",
        "documentation": {}
    },
    {
        "label": "fill_from_h5",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.create_artist_terms_db",
        "description": "Tasks_Demos.SQLite.create_artist_terms_db",
        "peekOfCode": "def fill_from_h5(conn,h5path):\n    \"\"\"\n    Add information rgarding the artist in that one h5 song file.\n    Doesn't commit, doesn't close conn at the end!\n    This h5 file must be for a new artist, we can't have twice the\n    same artist entered in the database!\n    The info is added to tables: artist_term, artist_mbtag\n    as many row as term/mbtag are added\n    \"\"\"\n    # get info from h5 file",
        "detail": "Tasks_Demos.SQLite.create_artist_terms_db",
        "documentation": {}
    },
    {
        "label": "add_indices_to_db",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.create_artist_terms_db",
        "description": "Tasks_Demos.SQLite.create_artist_terms_db",
        "peekOfCode": "def add_indices_to_db(conn,verbose=0):\n    \"\"\"\n    Since the db is considered final, we can add all sorts of indecies\n    to make sure the retrieval time is as fast as possible.\n    Indecies take up a little space, but they hurt performance only when\n    we modify the data (which should not happen)\n    This function commits its changes at the end\n    Note: tutorial on MySQL (close enough to SQLite):\n    http://www.databasejournal.com/features/mysql/article.php/10897_1382791_1/Optimizing-MySQL-Queries-and-Indexes.htm\n    \"\"\"",
        "detail": "Tasks_Demos.SQLite.create_artist_terms_db",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.create_artist_terms_db",
        "description": "Tasks_Demos.SQLite.create_artist_terms_db",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'Command to create the artist_terms SQLite database'\n    print 'to launch (it might take a while!):'\n    print '   python create_artist_terms_db.py <MillionSong dir> <termlist> <mbtaglist> <artistlist> <artist_term.db>'\n    print 'PARAMS'\n    print '  MillionSong dir   - directory containing .h5 song files in sub dirs'\n    print '  termlist          - list of all possible terms (Echo Nest tags)'\n    print '  mbtaglist         - list of all possible musicbrainz tags'\n    print '  artist list       - list in form: artistid<SEP>artist_mbid<SEP>track_id<SEP>...'",
        "detail": "Tasks_Demos.SQLite.create_artist_terms_db",
        "documentation": {}
    },
    {
        "label": "encode_string",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.create_track_metadata_db",
        "description": "Tasks_Demos.SQLite.create_track_metadata_db",
        "peekOfCode": "def encode_string(s):\n    \"\"\"\n    Simple utility function to make sure a string is proper\n    to be used in a SQLite query\n    (different than posgtresql, no N to specify unicode)\n    EXAMPLE:\n      That's my boy! -> 'That''s my boy!'\n    \"\"\"\n    return \"'\" + s.replace(\"'\", \"''\") + \"'\"\ndef create_db(filename):",
        "detail": "Tasks_Demos.SQLite.create_track_metadata_db",
        "documentation": {}
    },
    {
        "label": "create_db",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.create_track_metadata_db",
        "description": "Tasks_Demos.SQLite.create_track_metadata_db",
        "peekOfCode": "def create_db(filename):\n    \"\"\"\n    Creates the file and an empty table.\n    \"\"\"\n    # creates file\n    conn = sqlite3.connect(filename)\n    # add stuff\n    c = conn.cursor()\n    q = 'CREATE TABLE songs (track_id text PRIMARY KEY, '\n    q += 'title text, song_id text, '",
        "detail": "Tasks_Demos.SQLite.create_track_metadata_db",
        "documentation": {}
    },
    {
        "label": "fill_from_h5",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.create_track_metadata_db",
        "description": "Tasks_Demos.SQLite.create_track_metadata_db",
        "peekOfCode": "def fill_from_h5(conn, h5path, verbose=0):\n    \"\"\"\n    Add a row with he information from this .h5 file\n    Doesn't commit, doesn't close conn at the end!\n    \"\"\"\n    h5 = open_h5_file_read(h5path)\n    c = conn.cursor()\n    # build query\n    q = 'INSERT INTO songs VALUES ('\n    track_id = get_track_id(h5)",
        "detail": "Tasks_Demos.SQLite.create_track_metadata_db",
        "documentation": {}
    },
    {
        "label": "add_indices_to_db",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.create_track_metadata_db",
        "description": "Tasks_Demos.SQLite.create_track_metadata_db",
        "peekOfCode": "def add_indices_to_db(conn, verbose=0):\n    \"\"\"\n    Since the db is considered final, we can add all sorts of indecies\n    to make sure the retrieval time is as fast as possible.\n    Indecies take up a little space, but they hurt performance only when\n    we modify the data (which should not happen)\n    This function commits its changes at the end\n    You might want to add your own indices if you do weird query, e.g. on title\n    and artist musicbrainz ID.\n    Indices should be on the columns of the WHERE of your search, the goal",
        "detail": "Tasks_Demos.SQLite.create_track_metadata_db",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.create_track_metadata_db",
        "description": "Tasks_Demos.SQLite.create_track_metadata_db",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'Command to create the track_metadata SQLite database'\n    print 'to launch (it might take a while!):'\n    print '   python create_track_metadata_db.py [FLAGS] <MSD dir> <tmdb>'\n    print 'PARAMS'\n    print '   MSD dir   - directory containing .h5 song files in sub dirs'\n    print '        tmdb - filename for the database (track_metadata.db)'\n    print 'FLAGS'\n    print '  -shsdata f  - file containing the SHS dataset'",
        "detail": "Tasks_Demos.SQLite.create_track_metadata_db",
        "documentation": {}
    },
    {
        "label": "encode_string",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.demo_artist_similarity",
        "description": "Tasks_Demos.SQLite.demo_artist_similarity",
        "peekOfCode": "def encode_string(s):\n    \"\"\"\n    Simple utility function to make sure a string is proper\n    to be used in a SQLite query\n    (different than posgtresql, no N to specify unicode)\n    EXAMPLE:\n      That's my boy! -> 'That''s my boy!'\n    \"\"\"\n    return \"'\"+s.replace(\"'\",\"''\")+\"'\"\ndef die_with_usage():",
        "detail": "Tasks_Demos.SQLite.demo_artist_similarity",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.demo_artist_similarity",
        "description": "Tasks_Demos.SQLite.demo_artist_similarity",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'demo_artist_similarity.py'\n    print '  by T. Bertin-Mahieux (2011) Columbia University'\n    print '     tb2332@columbia.edu'\n    print 'This codes gives examples on how to query the database artist_similarity.db'\n    print 'To first create this database, see: create_artist_similarity_db.py'\n    print 'Note that you should first check: demo_track_metadata.py if you are not'\n    print 'familiar with SQLite.'\n    print 'usage:'",
        "detail": "Tasks_Demos.SQLite.demo_artist_similarity",
        "documentation": {}
    },
    {
        "label": "encode_string",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.demo_artist_term",
        "description": "Tasks_Demos.SQLite.demo_artist_term",
        "peekOfCode": "def encode_string(s):\n    \"\"\"\n    Simple utility function to make sure a string is proper\n    to be used in a SQLite query\n    (different than posgtresql, no N to specify unicode)\n    EXAMPLE:\n      That's my boy! -> 'That''s my boy!'\n    \"\"\"\n    return \"'\"+s.replace(\"'\",\"''\")+\"'\"\ndef die_with_usage():",
        "detail": "Tasks_Demos.SQLite.demo_artist_term",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.demo_artist_term",
        "description": "Tasks_Demos.SQLite.demo_artist_term",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'demo_artist_term.py'\n    print '  by T. Bertin-Mahieux (2010) Columbia University'\n    print '     tb2332@columbia.edu'\n    print 'This codes gives examples on how to query the database artist_term.db'\n    print 'To first create this database, see: create_artist_term_db.py'\n    print 'Note that you should first check: demo_track_metadata.py if you are not'\n    print 'familiar with SQLite.'\n    print 'usage:'",
        "detail": "Tasks_Demos.SQLite.demo_artist_term",
        "documentation": {}
    },
    {
        "label": "encode_string",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.demo_track_metadata",
        "description": "Tasks_Demos.SQLite.demo_track_metadata",
        "peekOfCode": "def encode_string(s):\n    \"\"\"\n    Simple utility function to make sure a string is proper\n    to be used in a SQLite query\n    (different than posgtresql, no N to specify unicode)\n    EXAMPLE:\n      That's my boy! -> 'That''s my boy!'\n    \"\"\"\n    return \"'\"+s.replace(\"'\",\"''\")+\"'\"\ndef die_with_usage():",
        "detail": "Tasks_Demos.SQLite.demo_track_metadata",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.demo_track_metadata",
        "description": "Tasks_Demos.SQLite.demo_track_metadata",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'demo_track_metadata.py'\n    print '  by T. Bertin-Mahieux (2010) Columbia University'\n    print '     tb2332@columbia.edu'\n    print 'This codes gives examples on how to query the database track_metadata.db'\n    print 'To first create this database, see: create_track_metadata_db.py'\n    print 'usage:'\n    print '   python demo_track_metadata.py <database path>'\n    sys.exit(0)",
        "detail": "Tasks_Demos.SQLite.demo_track_metadata",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.list_all_artists_from_db",
        "description": "Tasks_Demos.SQLite.list_all_artists_from_db",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'list_all_artists_from_db.py'\n    print '   by T. Bertin-Mahieux (2010) Columbia University'\n    print 'mimic the program /Tasks_Demo/NamesAnalysis/list_all_artist.py'\n    print 'but assumes the sqlite db track_metadata.py is available'\n    print 'i.e. it takes a few second instead of a few hours!'\n    print 'to download track_metadata.db, see Million Song website'\n    print 'to recreate it, see create_track_metadata.py'\n    print ''",
        "detail": "Tasks_Demos.SQLite.list_all_artists_from_db",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.SQLite.list_all_tracks_from_db",
        "description": "Tasks_Demos.SQLite.list_all_tracks_from_db",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'list_all_tracks_from_db.py'\n    print '   by T. Bertin-Mahieux (2010) Columbia University'\n    print 'Code to create a list of all tracks in the dataset as'\n    print 'a text file. Assumes track_metadata.db already exists.'\n    print \"Format is (IDs are EchoNest's):\"\n    print 'trackID<SEP>songID<SEP>artist name<SEP>song title'\n    print ' '\n    print 'usage:'",
        "detail": "Tasks_Demos.SQLite.list_all_tracks_from_db",
        "documentation": {}
    },
    {
        "label": "encode_string",
        "kind": 2,
        "importPath": "Tasks_Demos.Tagging.analyze_test_set",
        "description": "Tasks_Demos.Tagging.analyze_test_set",
        "peekOfCode": "def encode_string(s):\n    \"\"\"\n    Simple utility function to make sure a string is proper\n    to be used in a SQLite query\n    (different than posgtresql, no N to specify unicode)\n    EXAMPLE:\n      That's my boy! -> 'That''s my boy!'\n    \"\"\"\n    return \"'\"+s.replace(\"'\",\"''\")+\"'\"\ndef die_with_usage():",
        "detail": "Tasks_Demos.Tagging.analyze_test_set",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.Tagging.analyze_test_set",
        "description": "Tasks_Demos.Tagging.analyze_test_set",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'analyze_test_set.py'\n    print '    by T. Bertin-Mahieux (2011) Columbia University'\n    print '       tb2332@columbia.edu'\n    print ''\n    print 'Code to analyze the test set, and give a benchmark result for'\n    print 'automatic tagging based on tag stats (no audio analysis)'\n    print ''\n    print 'USAGE'",
        "detail": "Tasks_Demos.Tagging.analyze_test_set",
        "documentation": {}
    },
    {
        "label": "path_from_trackid",
        "kind": 2,
        "importPath": "Tasks_Demos.Tagging.get_unique_terms",
        "description": "Tasks_Demos.Tagging.get_unique_terms",
        "peekOfCode": "def path_from_trackid(trackid):\n    \"\"\"\n    Creates the path from a given trackid\n    \"\"\"\n    s = os.path.join(trackid[2],trackid[3])\n    s = os.path.join(s,trackid[4])\n    s = os.path.join(s,trackid)\n    s = s.upper() + '.h5'\n    return s\ndef put_term_in_hash_table(hash_table,term):",
        "detail": "Tasks_Demos.Tagging.get_unique_terms",
        "documentation": {}
    },
    {
        "label": "put_term_in_hash_table",
        "kind": 2,
        "importPath": "Tasks_Demos.Tagging.get_unique_terms",
        "description": "Tasks_Demos.Tagging.get_unique_terms",
        "peekOfCode": "def put_term_in_hash_table(hash_table,term):\n    \"\"\"\n    Function to get the hash code of a term and put it in the\n    given table\n    \"\"\"\n    np.random.seed(hash(term))\n    bucket_idx = np.random.randint(NUMBUCKETS)\n    hash_table[bucket_idx].add(term)\ndef die_with_usage():\n    \"\"\" HELP MENU \"\"\"",
        "detail": "Tasks_Demos.Tagging.get_unique_terms",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.Tagging.get_unique_terms",
        "description": "Tasks_Demos.Tagging.get_unique_terms",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'get_unique_terms.py'\n    print '  by T. Bertin-Mahieux (2010) Colubia University'\n    print 'GOAL'\n    print '  creates a list of unique terms and unique musicbrainz tags as fast as possible'\n    print 'USAGE'\n    print '  python get_unique_terms.py <MillionSong dir> <output_terms.txt> <output_mbtags.txt> (OPTIONAL <artist list>)'\n    print 'PARAM'\n    print '   MillionSong dir   - MillionSongDataset root directory'",
        "detail": "Tasks_Demos.Tagging.get_unique_terms",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.Tagging.get_unique_terms_from_db",
        "description": "Tasks_Demos.Tagging.get_unique_terms_from_db",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'get_unique_terms_from_db.py'\n    print '  by T. Bertin-Mahieux (2010) Columbia University'\n    print 'GOAL'\n    print '  creates a list of unique tags as fast as possible'\n    print '  actually, this code just extracts it from a SQLite db'\n    print 'USAGE'\n    print '  python get_unique_terms_from_db.py <artist_term.db> <unique_terms.txt> <unique_mbtags.txt>'\n    print 'PARAM'",
        "detail": "Tasks_Demos.Tagging.get_unique_terms_from_db",
        "documentation": {}
    },
    {
        "label": "encode_string",
        "kind": 2,
        "importPath": "Tasks_Demos.Tagging.split_train_test",
        "description": "Tasks_Demos.Tagging.split_train_test",
        "peekOfCode": "def encode_string(s):\n    \"\"\"\n    Simple utility function to make sure a string is proper\n    to be used in a SQLite query\n    (different than posgtresql, no N to specify unicode)\n    EXAMPLE:\n      That's my boy! -> 'That''s my boy!'\n    \"\"\"\n    return \"'\"+s.replace(\"'\",\"''\")+\"'\"\ndef check_constraint(term_freq,top_terms,top_terms_test_freq):",
        "detail": "Tasks_Demos.Tagging.split_train_test",
        "documentation": {}
    },
    {
        "label": "check_constraint",
        "kind": 2,
        "importPath": "Tasks_Demos.Tagging.split_train_test",
        "description": "Tasks_Demos.Tagging.split_train_test",
        "peekOfCode": "def check_constraint(term_freq,top_terms,top_terms_test_freq):\n    \"\"\"\n    Check the constraint 12%-30% for the test set\n    term_freq is the dictionnary of all term frequencies\n    top_terms is the list of terms we care about (first 300?)\n    top_terms_freq is an array of frequency of top terms in test set.\n    RETURN\n      True if constraint satisfied, False otherwise\n    \"\"\"\n    return check_constraint_12pc(term_freq,top_terms,top_terms_test_freq) and check_constraint_30pc(term_freq,top_terms,top_terms_test_freq)",
        "detail": "Tasks_Demos.Tagging.split_train_test",
        "documentation": {}
    },
    {
        "label": "check_constraint_12pc",
        "kind": 2,
        "importPath": "Tasks_Demos.Tagging.split_train_test",
        "description": "Tasks_Demos.Tagging.split_train_test",
        "peekOfCode": "def check_constraint_12pc(term_freq,top_terms,top_terms_test_freq):\n    \"\"\"\n    Check the constraint >12% for the test set\n    term_freq is the dictionnary of all term frequencies\n    top_terms is the list of terms we care about (first 300?)\n    top_terms_freq is an array of frequency of top terms in test set.\n    RETURN\n      True if constraint satisfied, False otherwise\n    \"\"\"\n    for tidx,t in enumerate(top_terms):",
        "detail": "Tasks_Demos.Tagging.split_train_test",
        "documentation": {}
    },
    {
        "label": "check_constraint_30pc",
        "kind": 2,
        "importPath": "Tasks_Demos.Tagging.split_train_test",
        "description": "Tasks_Demos.Tagging.split_train_test",
        "peekOfCode": "def check_constraint_30pc(term_freq,top_terms,top_terms_test_freq):\n    \"\"\"\n    Check the constraint <30% for the test set\n    term_freq is the dictionnary of all term frequencies\n    top_terms is the list of terms we care about (first 300?)\n    top_terms_freq is an array of frequency of top terms in test set.\n    RETURN\n      True if constraint satisfied, False otherwise\n    \"\"\"\n    for tidx,t in enumerate(top_terms):",
        "detail": "Tasks_Demos.Tagging.split_train_test",
        "documentation": {}
    },
    {
        "label": "get_terms_for_artist",
        "kind": 2,
        "importPath": "Tasks_Demos.Tagging.split_train_test",
        "description": "Tasks_Demos.Tagging.split_train_test",
        "peekOfCode": "def get_terms_for_artist(conn,artistid):\n    \"\"\"\n    Returns the list of terms for a given artist ID\n    \"\"\"\n    q = \"SELECT term FROM artist_term WHERE artist_id='\"+artistid+\"'\"\n    res = conn.execute(q)\n    return map(lambda x: x[0],res.fetchall())\ndef get_random_artist_for_term(conn,term,avoid_artists=None):\n    \"\"\"\n    Get a random artist that is tagged by a given term.",
        "detail": "Tasks_Demos.Tagging.split_train_test",
        "documentation": {}
    },
    {
        "label": "get_random_artist_for_term",
        "kind": 2,
        "importPath": "Tasks_Demos.Tagging.split_train_test",
        "description": "Tasks_Demos.Tagging.split_train_test",
        "peekOfCode": "def get_random_artist_for_term(conn,term,avoid_artists=None):\n    \"\"\"\n    Get a random artist that is tagged by a given term.\n    If avoid_artists is a list, we exclude these artists.\n    \"\"\"\n    q = \"SELECT artist_id FROM artist_term WHERE term=\"+encode_string(term)\n    res = conn.execute(q)\n    all_artists = sorted( map(lambda x: x[0], res.fetchall()) )\n    np.random.shuffle(all_artists)\n    if avoid_artists is None:",
        "detail": "Tasks_Demos.Tagging.split_train_test",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.Tagging.split_train_test",
        "description": "Tasks_Demos.Tagging.split_train_test",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'split_train_test.py'\n    print '  by T. Bertin-Mahieux (2010) Columbia University'\n    print 'GOAL'\n    print '  split the list of artists into train and test based on terms (Echo Nest tags).'\n    print 'USAGE'\n    print '  python split_train_test.py <artist_term.db> <train.txt> <test.txt> <top_terms.txt> <subset_tmdb>'\n    print 'PARAMS'\n    print '  artist_term.db    - SQLite database containing terms per artist'",
        "detail": "Tasks_Demos.Tagging.split_train_test",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.YahooRatings.count_ratings_known_artists",
        "description": "Tasks_Demos.YahooRatings.count_ratings_known_artists",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'count_ratings_known_artists.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print '      tb2332@columbia.edu'\n    print ''\n    print 'Checks how many ratings from the Yahoo Dataset R1 we cover'\n    print 'with known Echo Nest artist IDs'\n    print ''\n    print 'USAGE:'",
        "detail": "Tasks_Demos.YahooRatings.count_ratings_known_artists",
        "documentation": {}
    },
    {
        "label": "encode_string",
        "kind": 2,
        "importPath": "Tasks_Demos.YahooRatings.match_artist_names",
        "description": "Tasks_Demos.YahooRatings.match_artist_names",
        "peekOfCode": "def encode_string(s):\n    \"\"\"\n    Simple utility function to make sure a string is proper\n    to be used in a SQLite query\n    (different than posgtresql, no N to specify unicode)\n    EXAMPLE:\n      That's my boy! -> 'That''s my boy!'\n    \"\"\"\n    return \"'\"+s.replace(\"'\",\"''\")+\"'\"\ndef purge_yahoo_artists(yartists_dict,done_artists):",
        "detail": "Tasks_Demos.YahooRatings.match_artist_names",
        "documentation": {}
    },
    {
        "label": "purge_yahoo_artists",
        "kind": 2,
        "importPath": "Tasks_Demos.YahooRatings.match_artist_names",
        "description": "Tasks_Demos.YahooRatings.match_artist_names",
        "peekOfCode": "def purge_yahoo_artists(yartists_dict,done_artists):\n    \"\"\"\n    Takes a dictionnary 'modified name' -> 'original Yahoo name'\n    and removes the ones that are done\n    \"\"\"\n    for k in yartists_dict.keys():\n        if yartists_dict[k] in done_artists:\n            del yartists_dict[k]\ndef remove_small_chars(s):\n    \"\"\"",
        "detail": "Tasks_Demos.YahooRatings.match_artist_names",
        "documentation": {}
    },
    {
        "label": "remove_small_chars",
        "kind": 2,
        "importPath": "Tasks_Demos.YahooRatings.match_artist_names",
        "description": "Tasks_Demos.YahooRatings.match_artist_names",
        "peekOfCode": "def remove_small_chars(s):\n    \"\"\"\n    Remove , . and similar things, includind spaces\n    \"\"\"\n    s = s.replace(' ','')\n    s = s.replace('.','')\n    s = s.replace(',','')\n    s = s.replace('\"','')\n    s = s.replace(\"'\",'')\n    s = s.replace('\\\\','')",
        "detail": "Tasks_Demos.YahooRatings.match_artist_names",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.YahooRatings.match_artist_names",
        "description": "Tasks_Demos.YahooRatings.match_artist_names",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'match_artist_names.py'\n    print '  by T. Bertin-Mahieux (2011) Columbia University'\n    print '     tb2332@columbia.edu'\n    print ''\n    print 'This code try to find matches between artists in Yahoo Ratings'\n    print 'and artists in the Million Song Dataset'\n    print ''\n    print 'USAGE:'",
        "detail": "Tasks_Demos.YahooRatings.match_artist_names",
        "documentation": {}
    },
    {
        "label": "build_commands",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.auto_vw",
        "description": "Tasks_Demos.YearPrediction.ismir11.auto_vw",
        "peekOfCode": "def build_commands():\n    \"\"\"\n    Build commands based on arguments to try\n    \"\"\"\n    # fixed info\n    cmd = PREFIX + ' -c --cache_file ' + CACHE + ' ' + TRAIN\n    cmd += ' -f ' + MODEL\n    if QUIET:\n        cmd += ' --quiet'\n    cmd += ' ' # hack, so we can split and keep the interesting part",
        "detail": "Tasks_Demos.YearPrediction.ismir11.auto_vw",
        "documentation": {}
    },
    {
        "label": "build_test_cmd",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.auto_vw",
        "description": "Tasks_Demos.YearPrediction.ismir11.auto_vw",
        "peekOfCode": "def build_test_cmd():\n    \"\"\" CREATE TEST COMMAND \"\"\"\n    cmd = PREFIX + \" \" + TEST + ' --quiet'\n    cmd += \" -i \" + MODEL + \" -p \" + PREDS\n    # done\n    return cmd\ndef print_best_results(results,ntoprint=5):\n    \"\"\" function to print best results so far \"\"\"\n    print '*******************************************'\n    print 'BEST RESULTS:'",
        "detail": "Tasks_Demos.YearPrediction.ismir11.auto_vw",
        "documentation": {}
    },
    {
        "label": "print_best_results",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.auto_vw",
        "description": "Tasks_Demos.YearPrediction.ismir11.auto_vw",
        "peekOfCode": "def print_best_results(results,ntoprint=5):\n    \"\"\" function to print best results so far \"\"\"\n    print '*******************************************'\n    print 'BEST RESULTS:'\n    results = sorted(results,key=itemgetter(0))\n    for res in results[:ntoprint]:\n        print '*',res[0],'->',res[1].split('  ')[1],format('(%.3f %.3f %.3f %.3f)' % res[2])\n    print '*******************************************'\ndef results_to_file(results):\n    \"\"\" output results to file \"\"\"",
        "detail": "Tasks_Demos.YearPrediction.ismir11.auto_vw",
        "documentation": {}
    },
    {
        "label": "results_to_file",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.auto_vw",
        "description": "Tasks_Demos.YearPrediction.ismir11.auto_vw",
        "peekOfCode": "def results_to_file(results):\n    \"\"\" output results to file \"\"\"\n    results = sorted(results,key=itemgetter(0))\n    f = open(AUTOTRAINOUTPUT,'w')\n    for res in results:\n        f.write('* ['+str(res[0])+'] '+res[1].split('  ')[1]+' -> '+str(res[2]))\n        f.write('\\n')\n    f.close()\ndef launch_vw_wrapper(cmd=None,threadid=0,outputf=''):\n    \"\"\"",
        "detail": "Tasks_Demos.YearPrediction.ismir11.auto_vw",
        "documentation": {}
    },
    {
        "label": "launch_vw_wrapper",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.auto_vw",
        "description": "Tasks_Demos.YearPrediction.ismir11.auto_vw",
        "peekOfCode": "def launch_vw_wrapper(cmd=None,threadid=0,outputf=''):\n    \"\"\"\n    Wrapper to use our automatic training with multiple processes\n    IN DEVELOPMENT\n    \"\"\"\n    try:\n        assert not cmd is None\n        assert not outputf is ''\n        # replace stuff\n        model = MODEL + str(int(threadid))",
        "detail": "Tasks_Demos.YearPrediction.ismir11.auto_vw",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.auto_vw",
        "description": "Tasks_Demos.YearPrediction.ismir11.auto_vw",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'auto_vw.py'\n    print 'test a ton of vw parameters, shows the results'\n    print ''\n    sys.exit(0)\nif __name__ == '__main__':\n    if len(sys.argv) < 2:\n        die_with_usage()\n    # create commands",
        "detail": "Tasks_Demos.YearPrediction.ismir11.auto_vw",
        "documentation": {}
    },
    {
        "label": "corr_and_compress",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.compress_feat",
        "description": "Tasks_Demos.YearPrediction.ismir11.compress_feat",
        "peekOfCode": "def corr_and_compress(feats,finaldim,seed=3232343,randproj=None):\n    \"\"\"\n    From a features matrix 12x.... (beat-aligned or not)\n    Compute the correlation matrix (12x12) and projects it (except diagonal)\n    to the given final dim\n    RETURN\n       vector 1xfinaldim   or 0xfinaldim is problem\n    \"\"\"\n    # features length\n    ftlen = feats.shape[1]",
        "detail": "Tasks_Demos.YearPrediction.ismir11.compress_feat",
        "documentation": {}
    },
    {
        "label": "cov_and_compress",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.compress_feat",
        "description": "Tasks_Demos.YearPrediction.ismir11.compress_feat",
        "peekOfCode": "def cov_and_compress(feats,finaldim,seed=3232343,randproj=None):\n    \"\"\"\n    From a features matrix 12x.... (beat-aligned or not)\n    Compute the correlation matrix (12x12) and projects it (except diagonal)\n    to the given final dim\n    RETURN\n       vector 1xfinaldim   or 0xfinaldim is problem\n    \"\"\"\n    # features length\n    ftlen = feats.shape[1]",
        "detail": "Tasks_Demos.YearPrediction.ismir11.compress_feat",
        "documentation": {}
    },
    {
        "label": "avgcov_and_compress",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.compress_feat",
        "description": "Tasks_Demos.YearPrediction.ismir11.compress_feat",
        "peekOfCode": "def avgcov_and_compress(feats,finaldim,seed=3232343,randproj=None):\n    \"\"\"\n    From a features matrix 12x.... (beat-aligned or not)\n    Compute the correlation matrix (12x12) and projects it (except diagonal)\n    to the given final dim\n    RETURN\n       vector 1xfinaldim   or 0xfinaldim is problem\n    \"\"\"\n    # features length\n    ftlen = feats.shape[1]",
        "detail": "Tasks_Demos.YearPrediction.ismir11.compress_feat",
        "documentation": {}
    },
    {
        "label": "extract_and_compress",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.compress_feat",
        "description": "Tasks_Demos.YearPrediction.ismir11.compress_feat",
        "peekOfCode": "def extract_and_compress(btfeat,npicks,winsize,finaldim,seed=3232343,randproj=None):\n    \"\"\"\n    From a btfeat matrix, usually 12xLENGTH\n    Extracts 'npicks' windows of size 'winsize' equally spaced\n    Flatten these picks, pass them through a random projection, final\n    size is 'finaldim'\n    Returns matrix npicks x finaldim, or 0 x finaldim if problem\n    (btfeats not long enough for instance)\n    We could return less than npicks if not long enough!\n    For speed, we can compute the random projection once and pass it as an",
        "detail": "Tasks_Demos.YearPrediction.ismir11.compress_feat",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.compress_feat",
        "description": "Tasks_Demos.YearPrediction.ismir11.compress_feat",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'compress_feat.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print '      tb2332@columbia.edu'\n    print ''\n    print 'This code extracts and compress samples from beat-aligned features.'\n    print 'Should be used as a library, no main'\n    sys.exit(0)\nif __name__ == '__main__':",
        "detail": "Tasks_Demos.YearPrediction.ismir11.compress_feat",
        "documentation": {}
    },
    {
        "label": "convert_year",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.create_vw_dataset",
        "description": "Tasks_Demos.YearPrediction.ismir11.create_vw_dataset",
        "peekOfCode": "def convert_year(y):\n    \"\"\"\n    brings back the year between 0 and 1\n    returns a float\n    \"\"\"\n    res = (y - 1922.) / (2011.-1922.)\n    assert res>=0 and res<=1,'problem in year conversion, '+str(y)+'->'+str(res)\n    return res\ndef fullpath_from_trackid(maindir,trackid):\n    \"\"\" Creates proper file paths for song files \"\"\"",
        "detail": "Tasks_Demos.YearPrediction.ismir11.create_vw_dataset",
        "documentation": {}
    },
    {
        "label": "fullpath_from_trackid",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.create_vw_dataset",
        "description": "Tasks_Demos.YearPrediction.ismir11.create_vw_dataset",
        "peekOfCode": "def fullpath_from_trackid(maindir,trackid):\n    \"\"\" Creates proper file paths for song files \"\"\"\n    p = os.path.join(maindir,trackid[2])\n    p = os.path.join(p,trackid[3])\n    p = os.path.join(p,trackid[4])\n    p = os.path.join(p,trackid+'.h5')\n    return str(p)\ndef get_train_test_songs(msd_dir,testartists,tmdb):\n    \"\"\"\n    Creates two list of songs, one for training and one for testing.",
        "detail": "Tasks_Demos.YearPrediction.ismir11.create_vw_dataset",
        "documentation": {}
    },
    {
        "label": "get_train_test_songs",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.create_vw_dataset",
        "description": "Tasks_Demos.YearPrediction.ismir11.create_vw_dataset",
        "peekOfCode": "def get_train_test_songs(msd_dir,testartists,tmdb):\n    \"\"\"\n    Creates two list of songs, one for training and one for testing.\n    INPUT\n           msd_dir   - main MSD dir, <?>/MillionSong/data\n       testartists   - file containing test artist IDs\n              tmdb   - SQLite database track_metadata.db\n    RETURN\n       2 lists: trainsongs, testsongs\n    \"\"\"",
        "detail": "Tasks_Demos.YearPrediction.ismir11.create_vw_dataset",
        "documentation": {}
    },
    {
        "label": "extract_features",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.create_vw_dataset",
        "description": "Tasks_Demos.YearPrediction.ismir11.create_vw_dataset",
        "peekOfCode": "def extract_features(songlist,outputf):\n    \"\"\"\n    Extract features from a list of songs, save them in a give filename\n    in MLcomp ready format\n    INPUT\n        songlist   - arrays of path to HDF5 song files\n         outputf   - filename (text file)\n    \"\"\"\n    # sanity check\n    if os.path.isfile(outputf):",
        "detail": "Tasks_Demos.YearPrediction.ismir11.create_vw_dataset",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.create_vw_dataset",
        "description": "Tasks_Demos.YearPrediction.ismir11.create_vw_dataset",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'create_vw_dataset.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print '      tb2332@columbia.edu'\n    print '      copyright (c) TBM, 2011, All Rights Reserved'\n    print ''\n    print 'Code to create a dataset on year prediction for MLcomp.'\n    print 'Features are mean and covariance of timbre features.'\n    print 'Target is \"year\".'",
        "detail": "Tasks_Demos.YearPrediction.ismir11.create_vw_dataset",
        "documentation": {}
    },
    {
        "label": "convert_back_to_year",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.measure_vw_res",
        "description": "Tasks_Demos.YearPrediction.ismir11.measure_vw_res",
        "peekOfCode": "def convert_back_to_year(val):\n    \"\"\"\n    get something between 0 and 1, return a year between 1922 and 2011\n    \"\"\"\n    assert val >= 0 and val <= 1\n    return 1922. + val * ( 2011. - 1922. )\ndef measure(testf,vwout,verbose=1):\n    \"\"\"\n    measure the result from the test file and vw output\n    \"\"\"",
        "detail": "Tasks_Demos.YearPrediction.ismir11.measure_vw_res",
        "documentation": {}
    },
    {
        "label": "measure",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.measure_vw_res",
        "description": "Tasks_Demos.YearPrediction.ismir11.measure_vw_res",
        "peekOfCode": "def measure(testf,vwout,verbose=1):\n    \"\"\"\n    measure the result from the test file and vw output\n    \"\"\"\n    years_real = []\n    years_pred = []\n    f_real = open(testf,'r')\n    f_pred = open(vwout,'r')\n    for line_real in f_real.xreadlines():\n        line_pred = f_pred.readline().strip()",
        "detail": "Tasks_Demos.YearPrediction.ismir11.measure_vw_res",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.measure_vw_res",
        "description": "Tasks_Demos.YearPrediction.ismir11.measure_vw_res",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'python measure_vw_res.py testfile vwoutput'\n    sys.exit(0)\nif __name__ == '__main__':\n    if len(sys.argv) < 3:\n        die_with_usage()\n    testf = sys.argv[1]\n    vwout = sys.argv[2]\n    measure(testf,vwout)",
        "detail": "Tasks_Demos.YearPrediction.ismir11.measure_vw_res",
        "documentation": {}
    },
    {
        "label": "KeyboardInterruptError",
        "kind": 6,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "description": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "peekOfCode": "class KeyboardInterruptError(Exception):pass\ndef fullpath_from_trackid(maindir,trackid):\n    p = os.path.join(maindir,trackid[2])\n    p = os.path.join(p,trackid[3])\n    p = os.path.join(p,trackid[4])\n    p = os.path.join(p,trackid+'.h5')\n    return str(p)\ndef do_prediction(processed_feats,kd,h5model,K=1):\n    \"\"\"\n    Receive processed features from test set, apply KNN,",
        "detail": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "documentation": {}
    },
    {
        "label": "fullpath_from_trackid",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "description": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "peekOfCode": "def fullpath_from_trackid(maindir,trackid):\n    p = os.path.join(maindir,trackid[2])\n    p = os.path.join(p,trackid[3])\n    p = os.path.join(p,trackid[4])\n    p = os.path.join(p,trackid+'.h5')\n    return str(p)\ndef do_prediction(processed_feats,kd,h5model,K=1):\n    \"\"\"\n    Receive processed features from test set, apply KNN,\n    return an actual predicted year (float)",
        "detail": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "documentation": {}
    },
    {
        "label": "do_prediction",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "description": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "peekOfCode": "def do_prediction(processed_feats,kd,h5model,K=1):\n    \"\"\"\n    Receive processed features from test set, apply KNN,\n    return an actual predicted year (float)\n    INPUT\n       processed_feats - extracted from a test song\n                    kd - ANN kdtree on top of model\n               h5model - open h5 file with data.feats and data.year\n                     K - K-nn parameter\n    \"\"\"",
        "detail": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "documentation": {}
    },
    {
        "label": "process_filelist_test",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "description": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "peekOfCode": "def process_filelist_test(filelist=None,model=None,tmpfilename=None,\n                           npicks=None,winsize=None,finaldim=None,K=1,\n                          typecompress='picks'):\n    \"\"\"\n    Main function, process all files in the list (as long as their artist\n    is in testartist)\n    INPUT\n       filelist     - a list of song files\n       model        - h5 file containing feats and year for all train songs\n       tmpfilename  - where to save our processed features",
        "detail": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "documentation": {}
    },
    {
        "label": "process_filelist_test_wrapper",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "description": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "peekOfCode": "def process_filelist_test_wrapper(args):\n    \"\"\" wrapper function for multiprocessor, calls process_filelist_test \"\"\"\n    try:\n        process_filelist_test(**args)\n    except KeyboardInterrupt:\n        raise KeyboardInterruptError()\ndef process_filelist_test_main_pass(nthreads,model,testsongs,\n                                    npicks,winsize,finaldim,K,typecompress):\n    \"\"\"\n    Do the main walk through the data, deals with the threads,",
        "detail": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "documentation": {}
    },
    {
        "label": "process_filelist_test_main_pass",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "description": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "peekOfCode": "def process_filelist_test_main_pass(nthreads,model,testsongs,\n                                    npicks,winsize,finaldim,K,typecompress):\n    \"\"\"\n    Do the main walk through the data, deals with the threads,\n    creates the tmpfiles.\n    INPUT\n      - nthreads     - number of threads to use\n      - model        - h5 files containing feats and year for all train songs\n      - testsongs    - list of songs in the test set\n      - npicks       - number of samples to pick per song",
        "detail": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "description": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "peekOfCode": "def test(nthreads,model,testsongs,npicks,winsize,finaldim,K,typecompress):\n    \"\"\"\n    Main function to do the testing\n    Do the main pass with the number of given threads.\n    Then, reads the tmp files, computes the score, delete the tmpfiles.\n    INPUT\n      - nthreads     - number of threads to use\n      - model        - h5 files containing feats and year for all train songs\n      - testsongs    - songs to test on\n      - npicks       - number of samples to pick per song",
        "detail": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "description": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'process_test_set.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print '      tb2332@columbia.edu'\n    print 'Code to perform year prediction on the Million Song Dataset.'\n    print 'This performs the testing based of a KNN model.'\n    print 'USAGE:'\n    print '  python process_test_set.py [FLAGS] <MSD_DIR> <model> <testartists> <tmdb>'\n    print 'PARAMS:'",
        "detail": "Tasks_Demos.YearPrediction.ismir11.process_test_set",
        "documentation": {}
    },
    {
        "label": "KeyboardInterruptError",
        "kind": 6,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "description": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "peekOfCode": "class KeyboardInterruptError(Exception):pass\ndef fullpath_from_trackid(maindir,trackid):\n    \"\"\" Creates proper file paths for song files \"\"\"\n    p = os.path.join(maindir,trackid[2])\n    p = os.path.join(p,trackid[3])\n    p = os.path.join(p,trackid[4])\n    p = os.path.join(p,trackid+'.h5')\n    return str(p)\ndef get_all_files(basedir,ext='.h5'):\n    \"\"\"",
        "detail": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "documentation": {}
    },
    {
        "label": "fullpath_from_trackid",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "description": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "peekOfCode": "def fullpath_from_trackid(maindir,trackid):\n    \"\"\" Creates proper file paths for song files \"\"\"\n    p = os.path.join(maindir,trackid[2])\n    p = os.path.join(p,trackid[3])\n    p = os.path.join(p,trackid[4])\n    p = os.path.join(p,trackid+'.h5')\n    return str(p)\ndef get_all_files(basedir,ext='.h5'):\n    \"\"\"\n    From a root directory, go through all subdirectories",
        "detail": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "documentation": {}
    },
    {
        "label": "get_all_files",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "description": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "peekOfCode": "def get_all_files(basedir,ext='.h5'):\n    \"\"\"\n    From a root directory, go through all subdirectories\n    and find all files with the given extension.\n    Return all absolute paths in a list.\n    \"\"\"\n    allfiles = []\n    apply_to_all_files(basedir,func=lambda x: allfiles.append(x),ext=ext)\n    return allfiles\ndef apply_to_all_files(basedir,func=lambda x: x,ext='.h5'):",
        "detail": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "documentation": {}
    },
    {
        "label": "apply_to_all_files",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "description": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "peekOfCode": "def apply_to_all_files(basedir,func=lambda x: x,ext='.h5'):\n    \"\"\"\n    From a root directory, go through all subdirectories\n    and find all files with the given extension.\n    Apply the given function func\n    If no function passed, does nothing and counts file\n    Return number of files\n    \"\"\"\n    cnt = 0\n    for root, dirs, files in os.walk(basedir):",
        "detail": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "documentation": {}
    },
    {
        "label": "process_filelist_train",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "description": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "peekOfCode": "def process_filelist_train(filelist=None,testartists=None,tmpfilename=None,\n                           npicks=None,winsize=None,finaldim=None,typecompress='picks'):\n    \"\"\"\n    Main function, process all files in the list (as long as their artist\n    is not in testartist)\n    INPUT\n       filelist     - a list of song files\n       testartists  - set of artist ID that we should not use\n       tmpfilename  - where to save our processed features\n       npicks       - number of segments to pick per song",
        "detail": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "documentation": {}
    },
    {
        "label": "process_filelist_train_wrapper",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "description": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "peekOfCode": "def process_filelist_train_wrapper(args):\n    \"\"\" wrapper function for multiprocessor, calls process_filelist_train \"\"\"\n    try:\n        process_filelist_train(**args)\n    except KeyboardInterrupt:\n        raise KeyboardInterruptError()\ndef process_filelist_train_main_pass(nthreads,maindir,testartists,\n                                     npicks,winsize,finaldim,trainsongs=None,\n                                     typecompress='picks'):\n    \"\"\"",
        "detail": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "documentation": {}
    },
    {
        "label": "process_filelist_train_main_pass",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "description": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "peekOfCode": "def process_filelist_train_main_pass(nthreads,maindir,testartists,\n                                     npicks,winsize,finaldim,trainsongs=None,\n                                     typecompress='picks'):\n    \"\"\"\n    Do the main walk through the data, deals with the threads,\n    creates the tmpfiles.\n    INPUT\n      - nthreads     - number of threads to use\n      - maindir      - dir of the MSD, wehre to find song files\n      - testartists  - set of artists to ignore",
        "detail": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "description": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "peekOfCode": "def train(nthreads,maindir,output,testartists,npicks,winsize,finaldim,trainsongs=None,typecompress='picks'):\n    \"\"\"\n    Main function to do the training\n    Do the main pass with the number of given threads.\n    Then, reads the tmp files, creates the main output, delete the tmpfiles.\n    INPUT\n      - nthreads     - number of threads to use\n      - maindir      - dir of the MSD, wehre to find song files\n      - output       - main model, contains everything to perform KNN\n      - testartists  - set of artists to ignore",
        "detail": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "description": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'process_train_set.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print '      tb2332@columbia.edu'\n    print 'Code to perform year prediction on the Million Song Dataset.'\n    print 'This performs the training of the KNN model.'\n    print 'USAGE:'\n    print '  python process_train_set.py [FLAGS] <MSD_DIR> <output>'\n    print 'PARAMS:'",
        "detail": "Tasks_Demos.YearPrediction.ismir11.process_train_set",
        "documentation": {}
    },
    {
        "label": "proj_point5",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.randproj",
        "description": "Tasks_Demos.YearPrediction.ismir11.randproj",
        "peekOfCode": "def proj_point5(dimFrom,dimTo,seed=3232343):\n    \"\"\"\n    Creates a matrix dimFrom x dimTo where each element is\n    .5 or -.5 with probability 1/2 each\n    For theoretical results using this projection see:\n      D. Achlioptas. Database-friendly random projections.\n      In Symposium on Principles of Database Systems\n      (PODS), pages 274-281, 2001.\n      http://portal.acm.org/citation.cfm?doid=375551.375608\n    \"\"\"",
        "detail": "Tasks_Demos.YearPrediction.ismir11.randproj",
        "documentation": {}
    },
    {
        "label": "proj_sqrt3",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.randproj",
        "description": "Tasks_Demos.YearPrediction.ismir11.randproj",
        "peekOfCode": "def proj_sqrt3(dimFrom, dimTo,seed=3232343):\n    \"\"\"\n    Creates a matrix dimFrom x dimTo where each element is\n    sqrt(3) or -sqrt(3) with probability 1/6 each\n    or 0 otherwise\n    Slower than proj_point5 to create, and lots of zeros.\n    For theoretical results using this projection see:\n      D. Achlioptas. Database-friendly random projections.\n      In Symposium on Principles of Database Systems\n      (PODS), pages 274-281, 2001.",
        "detail": "Tasks_Demos.YearPrediction.ismir11.randproj",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.randproj",
        "description": "Tasks_Demos.YearPrediction.ismir11.randproj",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'randproj.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print '      tb2332@columbia.edu'\n    print ''\n    print 'This code generates matrices for random projections.'\n    print 'Should be used as a library, no main'\n    sys.exit(0)\nif __name__ == '__main__':",
        "detail": "Tasks_Demos.YearPrediction.ismir11.randproj",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.year_pred_benchmark",
        "description": "Tasks_Demos.YearPrediction.ismir11.year_pred_benchmark",
        "peekOfCode": "def evaluate(years_real,years_pred,verbose=0):\n    \"\"\"\n    Evaluate the result of a year prediction algorithm\n    RETURN\n      avg diff\n      std diff\n      avg square diff\n      std square diff\n    \"\"\"\n    years_real = np.array(years_real).flatten()",
        "detail": "Tasks_Demos.YearPrediction.ismir11.year_pred_benchmark",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.ismir11.year_pred_benchmark",
        "description": "Tasks_Demos.YearPrediction.ismir11.year_pred_benchmark",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'year_pred_benchmark.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print '      tb2332@columbia.edu'\n    print ''\n    print 'Script to get a benchmark on year prediction without'\n    print 'using features. Also, contains functions to measure'\n    print 'our predictions'\n    print ''",
        "detail": "Tasks_Demos.YearPrediction.ismir11.year_pred_benchmark",
        "documentation": {}
    },
    {
        "label": "get_btchromas",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "description": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "peekOfCode": "def get_btchromas(h5):\n    \"\"\"\n    Get beat-aligned chroma from a song file of the Million Song Dataset\n    INPUT:\n       h5          - filename or open h5 file\n    RETURN:\n       btchromas   - beat-aligned chromas, one beat per column\n                     or None if something went wrong (e.g. no beats)\n    \"\"\"\n    # if string, open and get chromas, if h5, get chromas",
        "detail": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "documentation": {}
    },
    {
        "label": "get_btchromas_loudness",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "description": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "peekOfCode": "def get_btchromas_loudness(h5):\n    \"\"\"\n    Similar to btchroma, but adds the loudness back.\n    We use the segments_loudness_max\n    There is no max value constraint, simply no negative values.\n    \"\"\"\n    # if string, open and get chromas, if h5, get chromas\n    if type(h5).__name__ == 'str':\n        h5 = GETTERS.open_h5_file_read(h5)\n        chromas = GETTERS.get_segments_pitches(h5)",
        "detail": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "documentation": {}
    },
    {
        "label": "get_bttimbre",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "description": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "peekOfCode": "def get_bttimbre(h5):\n    \"\"\"\n    Get beat-aligned timbre from a song file of the Million Song Dataset\n    INPUT:\n       h5          - filename or open h5 file\n    RETURN:\n       bttimbre    - beat-aligned timbre, one beat per column\n                     or None if something went wrong (e.g. no beats)\n    \"\"\"\n    # if string, open and get timbre, if h5, get timbre",
        "detail": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "documentation": {}
    },
    {
        "label": "get_btloudnessmax",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "description": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "peekOfCode": "def get_btloudnessmax(h5):\n    \"\"\"\n    Get beat-aligned loudness max from a song file of the Million Song Dataset\n    INPUT:\n       h5             - filename or open h5 file\n    RETURN:\n       btloudnessmax  - beat-aligned loudness max, one beat per column\n                        or None if something went wrong (e.g. no beats)\n    \"\"\"\n    # if string, open and get max loudness, if h5, get max loudness",
        "detail": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "documentation": {}
    },
    {
        "label": "align_feats",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "description": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "peekOfCode": "def align_feats(feats, segstarts, btstarts, duration):\n    \"\"\"\n    MAIN FUNCTION: aligned whatever matrix of features is passed,\n    one column per segment, and interpolate them to get features\n    per beat.\n    Note that btstarts could be anything, e.g. bar starts\n    INPUT\n       feats      - matrix of features, one column per segment\n       segstarts  - segments starts in seconds,\n                    dim must match feats # cols (flatten ndarray)",
        "detail": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "documentation": {}
    },
    {
        "label": "get_time_warp_matrix",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "description": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "peekOfCode": "def get_time_warp_matrix(segstart, btstart, duration):\n    \"\"\"\n    Used by create_beat_synchro_chromagram\n    Returns a matrix (#beats,#segs)\n    #segs should be larger than #beats, i.e. many events or segs\n    happen in one beat.\n    THIS FUNCTION WAS ORIGINALLY CREATED BY RON J. WEISS (Columbia/NYU/Google)\n    \"\"\"\n    # length of beats and segments in seconds\n    # result for track: 'TR0002Q11C3FA8332D'",
        "detail": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "documentation": {}
    },
    {
        "label": "idB",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "description": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "peekOfCode": "def idB(loudness_array):\n    \"\"\"\n    Reverse the Echo Nest loudness dB features.\n    'loudness_array' can be pretty any numpy object:\n    one value or an array\n    Inspired by D. Ellis MATLAB code\n    \"\"\"\n    return np.power(10., loudness_array / 20.)\ndef dB(inv_loudness_array):\n    \"\"\"",
        "detail": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "documentation": {}
    },
    {
        "label": "dB",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "description": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "peekOfCode": "def dB(inv_loudness_array):\n    \"\"\"\n    Put loudness back in dB\n    \"\"\"\n    return np.log10(inv_loudness_array) * 20.\ndef die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'beat_aligned_feats.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print '      tb2332@columbia.edu'",
        "detail": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "description": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'beat_aligned_feats.py'\n    print '   by T. Bertin-Mahieux (2011) Columbia University'\n    print '      tb2332@columbia.edu'\n    print ''\n    print 'This code is intended to be used as a library.'\n    print 'For debugging purposes, you can launch:'\n    print '   python beat_aligned_feats.py <SONG FILENAME>'\n    sys.exit(0)",
        "detail": "Tasks_Demos.YearPrediction.beat_aligned_feats",
        "documentation": {}
    },
    {
        "label": "die_with_usage",
        "kind": 2,
        "importPath": "Tasks_Demos.YearPrediction.split_train_test",
        "description": "Tasks_Demos.YearPrediction.split_train_test",
        "peekOfCode": "def die_with_usage():\n    \"\"\" HELP MENU \"\"\"\n    print 'split_train_test.py'\n    print '  by T. Bertin-Mahieux (2010) Columbia University'\n    print '     tb2332@columbia.edu'\n    print 'GOAL'\n    print '  Split the list of artists into train and test based on track years.'\n    print '  We do not split individual tracks because of the producer effect,'\n    print '  e.g. we want to predict years, not to recognize artists.'\n    print 'USAGE'",
        "detail": "Tasks_Demos.YearPrediction.split_train_test",
        "documentation": {}
    }
]